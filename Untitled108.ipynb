{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyp_LPRK8eL1",
        "outputId": "90de62f6-027e-471d-c82d-587ff79f9df7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded snapshots: 22350\n",
            "Saved: flattened_snapshots.csv\n",
            "Using Call columns: Previous_Call_ltp Current_Call_ltp Next_Call_ltp\n",
            "Using Put  columns: Previous_Put_ltp Current_Put_ltp Next_Put_ltp\n",
            "Using Strike columns: Previous_Strikeprice Current_Strikeprice Next_Strikeprice\n",
            "Detected call strikes: 7\n",
            "Detected put strikes : 7\n",
            "✔ Pipeline complete. Files generated:\n",
            "- MERGED_CE_PE_FORECAST.csv\n",
            "- TOP_BUY_PUTS.csv\n",
            "- TOP_BUY_CALLS.csv\n",
            "- ALL_ACTIONS.csv\n",
            "- TOP_CE_ONLY.csv\n",
            "- TOP_PE_ONLY.csv\n",
            "- TOP_HOLD_BREAKOUTS.csv\n",
            "- REVERSALS.csv\n",
            "- IV_CRUSH.csv\n",
            "- SPOT_VS_MOMENTUM.png\n",
            "- PREMIUM_HEATMAP.png\n",
            "- AUTO_TRADE_SIGNALS.json\n",
            "\n",
            "Merged CSV columns (sample):\n",
            "['strike', 'Unnamed: 0', 'n_obs', 'first_premium', 'last_premium', 'peak_premium', 'trough_premium', 'abs_change', 'pct_change', '5min_low', '5min_high', '10min_low', '10min_high', 'p5_expected_lo', 'p5_expected_hi', 'p10_expected_lo', 'p10_expected_hi', 'n_obs_call', 'n_obs_put', 'first_call_ltp', 'last_call_ltp', 'peak_call_ltp', 'trough_call_ltp', 'abs_change_call', 'pct_change_call', 'first_put_ltp', 'last_put_ltp', 'peak_put_ltp', 'trough_put_ltp', 'abs_change_put', 'pct_change_put', 'tags', 'call_moneyflow', 'put_moneyflow', 'reasons', 'recommended_action', 'highconv_total', 'highconv_success', 'highconv_hit_rate', 'Current_Strikeprice', 'strength']\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# full_ce_pe_pipeline_use_ltp_enriched_final_restored.py\n",
        "\"\"\"\n",
        "Consolidated CE+PE pipeline using LTP where available.\n",
        "Produces a merged enriched CSV with all requested columns,\n",
        "plus auxiliary outputs: top calls/puts, alerts, reversals, IV-crush,\n",
        "heatmap, auto-trade export, CE-only / PE-only lists, hold-breakouts.\n",
        "\n",
        "Drop this file next to your flattened_snapshots.csv and run:\n",
        "    python full_ce_pe_pipeline_use_ltp_enriched_final_restored.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from collections import defaultdict, Counter\n",
        "from datetime import timedelta\n",
        "import math\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "from pandas import json_normalize\n",
        "\n",
        "INPUT_FILE = \"19112025_BANK_PNL.txt\"     # your pasted file\n",
        "OUTPUT_CSV = \"flattened_snapshots.csv\"\n",
        "\n",
        "\n",
        "def parse_snapshot(obj):\n",
        "    \"\"\"\n",
        "    Flatten one snapshot including nested Current, Previous, Next JSON blocks.\n",
        "    \"\"\"\n",
        "    flat = {}\n",
        "\n",
        "    # 1. Copy all top-level fields except 'Current'\n",
        "    for k, v in obj.items():\n",
        "        if k != \"Current\":\n",
        "            flat[k] = v\n",
        "\n",
        "    # 2. Parse nested Current JSON string safely\n",
        "    if \"Current\" in obj:\n",
        "        try:\n",
        "            curr = json.loads(obj[\"Current\"])\n",
        "        except Exception:\n",
        "            curr = {}\n",
        "\n",
        "        # Process Previous, Current, Next inside\n",
        "        for section_name in [\"Previous\", \"Current\", \"Next\"]:\n",
        "            if section_name in curr and isinstance(curr[section_name], dict):\n",
        "                for key, val in curr[section_name].items():\n",
        "                    flat[f\"{section_name}_{key}\"] = val\n",
        "            else:\n",
        "                # Add empty if missing\n",
        "                flat[f\"{section_name}\"] = None\n",
        "\n",
        "    return flat\n",
        "\n",
        "\n",
        "def load_snapshots(path):\n",
        "    \"\"\"\n",
        "    Load file that contains:\n",
        "    - either JSON objects separated by newlines\n",
        "    - or multiple JSON blobs one after another\n",
        "    \"\"\"\n",
        "    snapshots = []\n",
        "    with open(path, \"r\") as f:\n",
        "        raw = f.read().strip()\n",
        "\n",
        "    # Try line-by-line JSON parsing\n",
        "    for line in raw.splitlines():\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        try:\n",
        "            snapshots.append(json.loads(line))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # If nothing loaded, try parsing entire file as list\n",
        "    if len(snapshots) == 0:\n",
        "        try:\n",
        "            snapshots = json.loads(raw)\n",
        "        except:\n",
        "            raise ValueError(\"Cannot parse file: Format looks invalid\")\n",
        "\n",
        "    return snapshots\n",
        "\n",
        "\n",
        "def main():\n",
        "    snapshots = load_snapshots(INPUT_FILE)\n",
        "    print(f\"Loaded snapshots: {len(snapshots)}\")\n",
        "\n",
        "    # Flatten all snapshots\n",
        "    flattened = [parse_snapshot(snap) for snap in snapshots]\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(flattened)\n",
        "\n",
        "    # Normalize types\n",
        "    for col in df.columns:\n",
        "        # Clean array strings \"[]\"\n",
        "        df[col] = df[col].apply(lambda x: None if x == \"[]\" else x)\n",
        "\n",
        "    # Convert timestamp\n",
        "    if \"LTT\" in df.columns:\n",
        "        df[\"LTT\"] = pd.to_datetime(df[\"LTT\"], errors=\"coerce\")\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv(OUTPUT_CSV, index=False)\n",
        "    print(f\"Saved: {OUTPUT_CSV}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------- CONFIG ---------------------- #\n",
        "INPUT_CSV = \"flattened_snapshots.csv\"\n",
        "\n",
        "OUT_MERGED = \"MERGED_CE_PE_FORECAST.csv\"\n",
        "OUT_TOP_PUTS = \"TOP_BUY_PUTS.csv\"\n",
        "OUT_TOP_CALLS = \"TOP_BUY_CALLS.csv\"\n",
        "OUT_ALL_ACTIONS = \"ALL_ACTIONS.csv\"\n",
        "\n",
        "OUT_CE_ONLY = \"TOP_CE_ONLY.csv\"\n",
        "OUT_PE_ONLY = \"TOP_PE_ONLY.csv\"\n",
        "OUT_HOLD_BREAKOUTS = \"TOP_HOLD_BREAKOUTS.csv\"\n",
        "\n",
        "OUT_REVERSALS = \"REVERSALS.csv\"\n",
        "OUT_IV_CRUSH = \"IV_CRUSH.csv\"\n",
        "\n",
        "OUT_PLOT = \"SPOT_VS_MOMENTUM.png\"\n",
        "OUT_HEATMAP = \"PREMIUM_HEATMAP.png\"\n",
        "OUT_AUTOTRADE = \"AUTO_TRADE_SIGNALS.json\"\n",
        "\n",
        "REVERSAL_WINDOW_MIN = 5\n",
        "REVERSAL_DROP_PCT = 12        # % drop from peak to last to mark reversal\n",
        "IV_CRUSH_DROP = 15            # % drop in IV to mark IV crush\n",
        "\n",
        "# ---------------------- LOAD ---------------------- #\n",
        "if not os.path.exists(INPUT_CSV):\n",
        "    print(f\"ERROR: input file not found: {INPUT_CSV}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Parse LTT as datetime if present; if not, we'll proceed without times.\n",
        "df = pd.read_csv(INPUT_CSV, low_memory=False)\n",
        "if \"LTT\" in df.columns:\n",
        "    try:\n",
        "        df[\"LTT\"] = pd.to_datetime(df[\"LTT\"])\n",
        "    except Exception:\n",
        "        # keep raw if parse fails\n",
        "        pass\n",
        "\n",
        "# ---------------------- COLUMN SELECTION (LTP preferred) ---------------------- #\n",
        "# Preferred LTP column names\n",
        "PREV_CALL_LTP = \"Previous_Call_ltp\"\n",
        "CURR_CALL_LTP = \"Current_Call_ltp\"\n",
        "NEXT_CALL_LTP = \"Next_Call_ltp\"\n",
        "\n",
        "PREV_PUT_LTP = \"Previous_Put_ltp\"\n",
        "CURR_PUT_LTP = \"Current_Put_ltp\"\n",
        "NEXT_PUT_LTP = \"Next_Put_ltp\"\n",
        "\n",
        "# Strike columns\n",
        "PREV_STR = \"Previous_Strikeprice\"\n",
        "CURR_STR = \"Current_Strikeprice\"\n",
        "NEXT_STR = \"Next_Strikeprice\"\n",
        "\n",
        "# fallback prefer _Premium if ltp missing\n",
        "def pick(col_ltp, col_prem):\n",
        "    if col_ltp in df.columns:\n",
        "        return col_ltp\n",
        "    if col_prem in df.columns:\n",
        "        return col_prem\n",
        "    return None\n",
        "\n",
        "prev_call_col = pick(PREV_CALL_LTP, \"Previous_Call_Premium\")\n",
        "curr_call_col = pick(CURR_CALL_LTP, \"Current_Call_Premium\")\n",
        "next_call_col = pick(NEXT_CALL_LTP, \"Next_Call_Premium\")\n",
        "\n",
        "prev_put_col = pick(PREV_PUT_LTP, \"Previous_Put_Premium\")\n",
        "curr_put_col = pick(CURR_PUT_LTP, \"Current_Put_Premium\")\n",
        "next_put_col = pick(NEXT_PUT_LTP, \"Next_Put_Premium\")\n",
        "\n",
        "print(\"Using Call columns:\", prev_call_col, curr_call_col, next_call_col)\n",
        "print(\"Using Put  columns:\", prev_put_col, curr_put_col, next_put_col)\n",
        "print(\"Using Strike columns:\", PREV_STR, CURR_STR, NEXT_STR)\n",
        "\n",
        "# ---------------------- BUILD TIMESERIES PER STRIKE ---------------------- #\n",
        "def build_series(str_col_prev, str_col_curr, str_col_next, val_prev, val_curr, val_next):\n",
        "    series = defaultdict(list)\n",
        "    for idx, row in df.iterrows():\n",
        "        t = row.get(\"LTT\", None)\n",
        "        triples = [\n",
        "            (str_col_prev, val_prev),\n",
        "            (str_col_curr, val_curr),\n",
        "            (str_col_next, val_next)\n",
        "        ]\n",
        "        for sc, vc in triples:\n",
        "            if sc is None or vc is None:\n",
        "                continue\n",
        "            if sc not in df.columns or vc not in df.columns:\n",
        "                continue\n",
        "            scv = row.get(sc)\n",
        "            vcv = row.get(vc)\n",
        "            if pd.isna(scv) or pd.isna(vcv):\n",
        "                continue\n",
        "            try:\n",
        "                s = int(scv)\n",
        "                p = float(vcv)\n",
        "            except Exception:\n",
        "                continue\n",
        "            # keep timestamp if present, else use index-based monotonic increasing fallback\n",
        "            if t is None or pd.isna(t):\n",
        "                # use index as proxy timestamp\n",
        "                t_use = pd.Timestamp(idx)\n",
        "            else:\n",
        "                t_use = t\n",
        "            series[s].append((t_use, p))\n",
        "    return series\n",
        "\n",
        "call_series = build_series(PREV_STR, CURR_STR, NEXT_STR, prev_call_col, curr_call_col, next_call_col)\n",
        "put_series  = build_series(PREV_STR, CURR_STR, NEXT_STR, prev_put_col, curr_put_col, next_put_col)\n",
        "\n",
        "print(\"Detected call strikes:\", len(call_series))\n",
        "print(\"Detected put strikes :\", len(put_series))\n",
        "\n",
        "# ---------------------- SUMMARY & FORECAST HELPERS ---------------------- #\n",
        "def summarize(pairs):\n",
        "    pairs_sorted = sorted(pairs, key=lambda x: x[0])\n",
        "    prices = [p for _, p in pairs_sorted]\n",
        "    if not prices:\n",
        "        return None\n",
        "    first = prices[0]\n",
        "    last = prices[-1]\n",
        "    peak = max(prices)\n",
        "    trough = min(prices)\n",
        "    abs_change = last - first\n",
        "    pct_change = (abs_change / first * 100) if first != 0 else np.nan\n",
        "    return {\n",
        "        \"first\": first,\n",
        "        \"last\": last,\n",
        "        \"peak\": peak,\n",
        "        \"trough\": trough,\n",
        "        \"abs_change\": abs_change,\n",
        "        \"pct_change\": pct_change,\n",
        "        \"n_obs\": len(prices),\n",
        "        \"series_sorted\": pairs_sorted\n",
        "    }\n",
        "\n",
        "def forecast_from_pct(last, pct):\n",
        "    \"\"\"Return ((5min_low,5min_high),(10min_low,10min_high))\"\"\"\n",
        "    if pd.isna(pct):\n",
        "        pct = 0.0\n",
        "    if pct >= 25:\n",
        "        return (last+15, last+35), (last+25, last+60)\n",
        "    if pct >= 8:\n",
        "        return (last+6, last+18), (last+12, last+30)\n",
        "    if pct > 0:\n",
        "        return (last+2, last+8), (last+5, last+15)\n",
        "    # neutral/slight negative\n",
        "    return (last-5, last+2), (last-8, last+5)\n",
        "\n",
        "# Build per-side summaries\n",
        "call_summary = {}\n",
        "for s, pairs in call_series.items():\n",
        "    st = summarize(pairs)\n",
        "    if st:\n",
        "        call_summary[s] = st\n",
        "\n",
        "put_summary = {}\n",
        "for s, pairs in put_series.items():\n",
        "    st = summarize(pairs)\n",
        "    if st:\n",
        "        put_summary[s] = st\n",
        "\n",
        "# All unique strikes from both sides\n",
        "all_strikes = sorted(set(list(call_summary.keys()) + list(put_summary.keys())))\n",
        "\n",
        "# ---------------------- BUILD MERGED ENRICHED ROWS ---------------------- #\n",
        "rows = []\n",
        "for s in all_strikes:\n",
        "    cs = call_summary.get(s)\n",
        "    ps = put_summary.get(s)\n",
        "\n",
        "    # n_obs unified\n",
        "    n_obs_call = int(cs[\"n_obs\"]) if cs else 0\n",
        "    n_obs_put  = int(ps[\"n_obs\"]) if ps else 0\n",
        "    n_obs = n_obs_call + n_obs_put\n",
        "\n",
        "    # choose first/last/peak/trough: prefer side with more observations (call over put if tie)\n",
        "    def prefer(key_call, key_put):\n",
        "        if cs and key_call in cs:\n",
        "            return cs[key_call]\n",
        "        if ps and key_put in ps:\n",
        "            return ps[key_put]\n",
        "        return np.nan\n",
        "\n",
        "    # However to be explicit use logic:\n",
        "    if n_obs_call >= n_obs_put and cs:\n",
        "        first_premium = cs[\"first\"]\n",
        "        last_premium  = cs[\"last\"]\n",
        "        peak_premium  = cs[\"peak\"]\n",
        "        trough_premium= cs[\"trough\"]\n",
        "        abs_change    = cs[\"abs_change\"]\n",
        "        pct_change    = cs[\"pct_change\"]\n",
        "        f5, f10 = forecast_from_pct(last_premium, pct_change)\n",
        "    elif ps:\n",
        "        first_premium = ps[\"first\"]\n",
        "        last_premium  = ps[\"last\"]\n",
        "        peak_premium  = ps[\"peak\"]\n",
        "        trough_premium= ps[\"trough\"]\n",
        "        abs_change    = ps[\"abs_change\"]\n",
        "        pct_change    = ps[\"pct_change\"]\n",
        "        f5, f10 = forecast_from_pct(last_premium, pct_change)\n",
        "    else:\n",
        "        first_premium = last_premium = peak_premium = trough_premium = abs_change = pct_change = np.nan\n",
        "        f5 = (np.nan, np.nan); f10 = (np.nan, np.nan)\n",
        "\n",
        "    # p5/p10 expected deltas\n",
        "    p5_expected_lo = (f5[0] - last_premium) if (not pd.isna(f5[0]) and not pd.isna(last_premium)) else np.nan\n",
        "    p5_expected_hi = (f5[1] - last_premium) if (not pd.isna(f5[1]) and not pd.isna(last_premium)) else np.nan\n",
        "    p10_expected_lo = (f10[0] - last_premium) if (not pd.isna(f10[0]) and not pd.isna(last_premium)) else np.nan\n",
        "    p10_expected_hi = (f10[1] - last_premium) if (not pd.isna(f10[1]) and not pd.isna(last_premium)) else np.nan\n",
        "\n",
        "    rows.append({\n",
        "        \"strike\": s,\n",
        "        \"Unnamed: 0\": (df[\"Unnamed: 0\"].iloc[0] if \"Unnamed: 0\" in df.columns else np.nan),\n",
        "        \"n_obs\": n_obs,\n",
        "        \"first_premium\": first_premium,\n",
        "        \"last_premium\": last_premium,\n",
        "        \"peak_premium\": peak_premium,\n",
        "        \"trough_premium\": trough_premium,\n",
        "        \"abs_change\": abs_change,\n",
        "        \"pct_change\": pct_change,\n",
        "        \"5min_low\": f5[0],\n",
        "        \"5min_high\": f5[1],\n",
        "        \"10min_low\": f10[0],\n",
        "        \"10min_high\": f10[1],\n",
        "        \"p5_expected_lo\": p5_expected_lo,\n",
        "        \"p5_expected_hi\": p5_expected_hi,\n",
        "        \"p10_expected_lo\": p10_expected_lo,\n",
        "        \"p10_expected_hi\": p10_expected_hi,\n",
        "        # placeholders for call/put details - attach below\n",
        "        \"n_obs_call\": n_obs_call,\n",
        "        \"n_obs_put\": n_obs_put,\n",
        "        \"first_call_ltp\": cs[\"first\"] if cs else np.nan,\n",
        "        \"last_call_ltp\": cs[\"last\"] if cs else np.nan,\n",
        "        \"peak_call_ltp\": cs[\"peak\"] if cs else np.nan,\n",
        "        \"trough_call_ltp\": cs[\"trough\"] if cs else np.nan,\n",
        "        \"abs_change_call\": cs[\"abs_change\"] if cs else np.nan,\n",
        "        \"pct_change_call\": cs[\"pct_change\"] if cs else np.nan,\n",
        "        \"first_put_ltp\": ps[\"first\"] if ps else np.nan,\n",
        "        \"last_put_ltp\": ps[\"last\"] if ps else np.nan,\n",
        "        \"peak_put_ltp\": ps[\"peak\"] if ps else np.nan,\n",
        "        \"trough_put_ltp\": ps[\"trough\"] if ps else np.nan,\n",
        "        \"abs_change_put\": ps[\"abs_change\"] if ps else np.nan,\n",
        "        \"pct_change_put\": ps[\"pct_change\"] if ps else np.nan,\n",
        "    })\n",
        "\n",
        "merged_df = pd.DataFrame(rows)\n",
        "\n",
        "# ---------------------- EXTRACT TAGS & MONEYFLOW FOR EACH STRIKE ---------------------- #\n",
        "tag_cols = [\"Previous_StrategyTag\",\"Current_StrategyTag\",\"Next_StrategyTag\"]\n",
        "call_money_cols = [\"Previous_CallMoneyFlow\",\"Current_CallMoneyFlow\",\"Next_CallMoneyFlow\",\n",
        "                   \"Previous_TotalcallMoneyFlow\",\"Current_TotalcallMoneyFlow\",\"Next_TotalcallMoneyFlow\"]\n",
        "put_money_cols  = [\"Previous_PutMoneyFlow\",\"Current_PutMoneyFlow\",\"Next_PutMoneyFlow\",\n",
        "                   \"Previous_TotalputMoneyFlow\",\"Current_TotalputMoneyFlow\",\"Next_TotalputMoneyFlow\"]\n",
        "\n",
        "# initialize\n",
        "strike_info = {int(s): {\"tags\":Counter(), \"call_moneyflow\":0.0, \"put_moneyflow\":0.0} for s in merged_df[\"strike\"].astype(int)}\n",
        "\n",
        "# iterate source df once and aggregate\n",
        "for idx, row in df.iterrows():\n",
        "    for sc in (PREV_STR, CURR_STR, NEXT_STR):\n",
        "        if sc not in df.columns:\n",
        "            continue\n",
        "        scval = row.get(sc)\n",
        "        if pd.isna(scval):\n",
        "            continue\n",
        "        try:\n",
        "            s = int(scval)\n",
        "        except Exception:\n",
        "            continue\n",
        "        if s not in strike_info:\n",
        "            continue\n",
        "        # tags\n",
        "        for col in tag_cols:\n",
        "            if col in df.columns:\n",
        "                v = row.get(col)\n",
        "                if isinstance(v, str) and v.strip():\n",
        "                    tokens = [t.strip() for t in v.replace(\"|\",\";\").split(\";\") if t.strip()]\n",
        "                    for t in tokens:\n",
        "                        strike_info[s][\"tags\"][t] += 1\n",
        "        # call moneyflow\n",
        "        for col in call_money_cols:\n",
        "            if col in df.columns:\n",
        "                try:\n",
        "                    v = row.get(col)\n",
        "                    if not pd.isna(v):\n",
        "                        strike_info[s][\"call_moneyflow\"] += float(v)\n",
        "                except Exception:\n",
        "                    pass\n",
        "        # put moneyflow\n",
        "        for col in put_money_cols:\n",
        "            if col in df.columns:\n",
        "                try:\n",
        "                    v = row.get(col)\n",
        "                    if not pd.isna(v):\n",
        "                        strike_info[s][\"put_moneyflow\"] += float(v)\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "def tags_to_text(s):\n",
        "    t = strike_info.get(int(s), {}).get(\"tags\", {})\n",
        "    if not t:\n",
        "        return \"\"\n",
        "    return \";\".join([f\"{k}:{v}\" for k,v in t.items()])\n",
        "\n",
        "merged_df[\"tags\"] = merged_df[\"strike\"].apply(lambda s: tags_to_text(s))\n",
        "merged_df[\"call_moneyflow\"] = merged_df[\"strike\"].apply(lambda s: strike_info.get(int(s), {}).get(\"call_moneyflow\", 0.0))\n",
        "merged_df[\"put_moneyflow\"] = merged_df[\"strike\"].apply(lambda s: strike_info.get(int(s), {}).get(\"put_moneyflow\", 0.0))\n",
        "\n",
        "# ---------------------- REASONS (derived) ---------------------- #\n",
        "def build_reasons(row):\n",
        "    reasons = []\n",
        "    tags_text = str(row.get(\"tags\",\"\"))\n",
        "    # tag based\n",
        "    if any(k.lower() in tags_text.lower() for k in [\"rsimacd\",\"rsi\",\"macd\"]):\n",
        "        reasons.append(\"RSI/MACD momentum\")\n",
        "    if \"VWAP\" in tags_text or \"vwap\" in tags_text:\n",
        "        reasons.append(\"VWAP divergence\")\n",
        "    if \"OI\" in tags_text or \"oi\" in tags_text:\n",
        "        reasons.append(\"OI support/resistance\")\n",
        "    # moneyflow\n",
        "    if row.get(\"call_moneyflow\",0) > 0:\n",
        "        reasons.append(\"Call net buying\")\n",
        "    if row.get(\"put_moneyflow\",0) > 0:\n",
        "        reasons.append(\"Put net buying\")\n",
        "    # pct-based\n",
        "    try:\n",
        "        pct = float(row.get(\"pct_change\", 0) or 0)\n",
        "        if pct > 10:\n",
        "            reasons.append(\"Strong premium move\")\n",
        "        elif pct > 3:\n",
        "            reasons.append(\"Moderate premium move\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    if not reasons:\n",
        "        reasons = [\"No strong signals\"]\n",
        "    # dedupe\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for r in reasons:\n",
        "        if r not in seen:\n",
        "            out.append(r)\n",
        "            seen.add(r)\n",
        "    return \"; \".join(out)\n",
        "\n",
        "merged_df[\"reasons\"] = merged_df.apply(build_reasons, axis=1)\n",
        "\n",
        "# ---------------------- RECOMMENDED ACTION (dual-side) ---------------------- #\n",
        "def decide_action(row):\n",
        "    call_pct = row.get(\"pct_change_call\")\n",
        "    put_pct = row.get(\"pct_change_put\")\n",
        "    call_pct = 0 if pd.isna(call_pct) else float(call_pct)\n",
        "    put_pct = 0 if pd.isna(put_pct) else float(put_pct)\n",
        "    tags = (row.get(\"tags\") or \"\").lower()\n",
        "\n",
        "    bull_boost = (\"call buying\" in tags) or (\"oi_support_call\" in tags) or (\"bull\" in tags)\n",
        "    bear_boost = (\"put buying\" in tags) or (\"call writing\" in tags) or (\"bear\" in tags)\n",
        "\n",
        "    # priority rules\n",
        "    if (put_pct > 8) or (put_pct > 5 and bear_boost):\n",
        "        return \"BUY_PUT\"\n",
        "    if (call_pct > 8) or (call_pct > 5 and bull_boost):\n",
        "        return \"BUY_CALL\"\n",
        "    return \"HOLD\"\n",
        "\n",
        "merged_df[\"recommended_action\"] = merged_df.apply(decide_action, axis=1)\n",
        "\n",
        "# ---------------------- HIGH-CONVICTION STATISTICS ---------------------- #\n",
        "hc_col = \"Current_IsHighConvictionSignal\"\n",
        "\n",
        "def compute_highconv_stats(strike, action):\n",
        "    total = 0\n",
        "    success = 0\n",
        "    if hc_col not in df.columns:\n",
        "        return 0, 0, None\n",
        "    # select rows where current strike matches and hc flag true\n",
        "    cond = (df.get(\"Current_Strikeprice\") == strike) & (df.get(hc_col) == True)\n",
        "    hc_rows = df[cond]\n",
        "    total = int(hc_rows.shape[0])\n",
        "    for idx, hr in hc_rows.iterrows():\n",
        "        t0 = hr.get(\"LTT\")\n",
        "        if pd.isna(t0):\n",
        "            continue\n",
        "        # pick premium column based on action preference\n",
        "        if action == \"BUY_PUT\" and curr_put_col in df.columns:\n",
        "            base_col = curr_put_col\n",
        "        elif action == \"BUY_CALL\" and curr_call_col in df.columns:\n",
        "            base_col = curr_call_col\n",
        "        else:\n",
        "            base_col = curr_call_col if curr_call_col in df.columns else curr_put_col if curr_put_col in df.columns else None\n",
        "        if base_col is None:\n",
        "            continue\n",
        "        p0 = hr.get(base_col)\n",
        "        if pd.isna(p0):\n",
        "            continue\n",
        "        # window 3 minutes forward\n",
        "        window = df[(df[\"LTT\"] >= t0) & (df[\"LTT\"] <= (t0 + timedelta(minutes=3)))]\n",
        "        if window.empty:\n",
        "            continue\n",
        "        try:\n",
        "            if window[base_col].max() > p0:\n",
        "                success += 1\n",
        "        except Exception:\n",
        "            pass\n",
        "    rate = (success/total) if total > 0 else None\n",
        "    return total, success, rate\n",
        "\n",
        "hc_totals = []\n",
        "hc_successes = []\n",
        "hc_rates = []\n",
        "for idx, r in merged_df.iterrows():\n",
        "    s = int(r[\"strike\"])\n",
        "    action = r.get(\"recommended_action\",\"HOLD\")\n",
        "    tot, succ, rate = compute_highconv_stats(s, action)\n",
        "    hc_totals.append(tot)\n",
        "    hc_successes.append(succ)\n",
        "    hc_rates.append(rate)\n",
        "\n",
        "merged_df[\"highconv_total\"] = hc_totals\n",
        "merged_df[\"highconv_success\"] = hc_successes\n",
        "merged_df[\"highconv_hit_rate\"] = hc_rates\n",
        "\n",
        "# add Current_Strikeprice column (explicit)\n",
        "merged_df[\"Current_Strikeprice\"] = merged_df[\"strike\"]\n",
        "\n",
        "# ---------------------- REVERSAL DETECTION ---------------------- #\n",
        "def detect_reversals_in_map(series_map, side_label):\n",
        "    out = []\n",
        "    for s, pairs in series_map.items():\n",
        "        sr = sorted(pairs, key=lambda x: x[0])\n",
        "        if len(sr) < 3:\n",
        "            continue\n",
        "        t_last = sr[-1][0]\n",
        "        # window lookback\n",
        "        window = [(t,p) for t,p in sr if t >= (t_last - timedelta(minutes=REVERSAL_WINDOW_MIN))]\n",
        "        if not window:\n",
        "            continue\n",
        "        prices = [p for _,p in window]\n",
        "        peak = max(prices)\n",
        "        last_price = prices[-1]\n",
        "        drop_pct = (peak - last_price) / peak * 100 if peak > 0 else 0\n",
        "        if drop_pct >= REVERSAL_DROP_PCT:\n",
        "            out.append({\n",
        "                \"strike\": s,\n",
        "                \"side\": side_label,\n",
        "                \"peak\": peak,\n",
        "                \"last\": last_price,\n",
        "                \"drop_pct\": drop_pct\n",
        "            })\n",
        "    return out\n",
        "\n",
        "rev_list = detect_reversals_in_map(call_series, \"CALL\") + detect_reversals_in_map(put_series, \"PUT\")\n",
        "rev_df = pd.DataFrame(rev_list)\n",
        "if not rev_df.empty:\n",
        "    rev_df.to_csv(OUT_REVERSALS, index=False)\n",
        "else:\n",
        "    # create empty file\n",
        "    pd.DataFrame(columns=[\"strike\",\"side\",\"peak\",\"last\",\"drop_pct\"]).to_csv(OUT_REVERSALS, index=False)\n",
        "\n",
        "# ---------------------- IV CRUSH DETECTION ---------------------- #\n",
        "iv_crush_events = []\n",
        "# iterate rows and check prev vs curr IV fields (existence optional)\n",
        "for idx, row in df.iterrows():\n",
        "    for prev_iv_col, curr_iv_col, side in [\n",
        "        (\"Previous_Call_IV\",\"Current_Call_IV\",\"CALL\"),\n",
        "        (\"Previous_Put_IV\",\"Current_Put_IV\",\"PUT\")\n",
        "    ]:\n",
        "        if prev_iv_col in df.columns and curr_iv_col in df.columns:\n",
        "            prev_iv = row.get(prev_iv_col)\n",
        "            curr_iv = row.get(curr_iv_col)\n",
        "            if pd.notna(prev_iv) and pd.notna(curr_iv) and prev_iv > 0:\n",
        "                drop_pct = (prev_iv - curr_iv) / prev_iv * 100\n",
        "                if drop_pct >= IV_CRUSH_DROP:\n",
        "                    iv_crush_events.append({\n",
        "                        \"LTT\": row.get(\"LTT\"),\n",
        "                        \"strike\": row.get(\"Current_Strikeprice\"),\n",
        "                        \"side\": side,\n",
        "                        \"prev_iv\": prev_iv,\n",
        "                        \"curr_iv\": curr_iv,\n",
        "                        \"drop_pct\": drop_pct\n",
        "                    })\n",
        "\n",
        "iv_crush_df = pd.DataFrame(iv_crush_events)\n",
        "if not iv_crush_df.empty:\n",
        "    iv_crush_df.to_csv(OUT_IV_CRUSH, index=False)\n",
        "else:\n",
        "    pd.DataFrame(columns=[\"LTT\",\"strike\",\"side\",\"prev_iv\",\"curr_iv\",\"drop_pct\"]).to_csv(OUT_IV_CRUSH, index=False)\n",
        "\n",
        "# ---------------------- HEATMAP (CALL/PUT pct change) ---------------------- #\n",
        "# Prepare simple heatmap matrix: rows = strikes, cols = [pct_change_call, pct_change_put]\n",
        "heatmap_df = merged_df[[\"pct_change_call\",\"pct_change_put\"]].fillna(0)\n",
        "if not heatmap_df.empty:\n",
        "    plt.figure(figsize=(8, max(3, len(heatmap_df)/4)))\n",
        "    plt.imshow(heatmap_df.values, aspect='auto', interpolation='nearest')\n",
        "    plt.colorbar(label=\"Premium % Change\")\n",
        "    plt.title(\"CALL/PUT Premium % Change Heatmap (rows=strikes)\")\n",
        "    plt.ylabel(\"strike index (not price)\")\n",
        "    plt.xlabel(\"0=CE_pct_change, 1=PE_pct_change\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUT_HEATMAP, dpi=150)\n",
        "    plt.close()\n",
        "else:\n",
        "    # produce an empty placeholder\n",
        "    plt.figure(figsize=(6,2)); plt.text(0.5,0.5,\"No data\"); plt.axis('off'); plt.savefig(OUT_HEATMAP); plt.close()\n",
        "\n",
        "# ---------------------- SPOT VS MOMENTUM PLOT ---------------------- #\n",
        "last_spot = df[\"SpotPrice\"].dropna().iloc[-1] if \"SpotPrice\" in df.columns and not df[\"SpotPrice\"].dropna().empty else np.nan\n",
        "avg_ce_pct = merged_df[\"pct_change_call\"].replace([np.inf,-np.inf],np.nan).dropna().mean() if \"pct_change_call\" in merged_df.columns else np.nan\n",
        "avg_pe_pct = merged_df[\"pct_change_put\"].replace([np.inf,-np.inf],np.nan).dropna().mean() if \"pct_change_put\" in merged_df.columns else np.nan\n",
        "\n",
        "vals = [last_spot if not pd.isna(last_spot) else 0, avg_ce_pct if not pd.isna(avg_ce_pct) else 0, avg_pe_pct if not pd.isna(avg_pe_pct) else 0]\n",
        "labels = [\"Spot Last\",\"Avg CE %\",\"Avg PE %\"]\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.title(\"Spot (last) vs Avg CE/PE %change\")\n",
        "plt.bar(labels, vals)\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUT_PLOT, dpi=150)\n",
        "plt.close()\n",
        "\n",
        "# ---------------------- AUTO-TRADE EXPORT ---------------------- #\n",
        "auto_signals = []\n",
        "for idx, r in merged_df.iterrows():\n",
        "    action = r.get(\"recommended_action\",\"HOLD\")\n",
        "    if action in (\"BUY_CALL\",\"BUY_PUT\"):\n",
        "        strength = float(r.get(\"pct_change_call\",0) or r.get(\"pct_change_put\",0) or 0)\n",
        "        auto_signals.append({\n",
        "            \"strike\": int(r[\"strike\"]),\n",
        "            \"action\": action,\n",
        "            \"strength\": strength,\n",
        "            \"reason\": r.get(\"reasons\",\"\"),\n",
        "            \"tags\": r.get(\"tags\",\"\")\n",
        "        })\n",
        "with open(OUT_AUTOTRADE, \"w\") as fh:\n",
        "    json.dump(auto_signals, fh, indent=2, default=str)\n",
        "\n",
        "# ---------------------- WRITE OUTPUTS: main merged + slices ---------------------- #\n",
        "# Save merged enriched table with all requested columns\n",
        "# Ensure column order to match user's expectation\n",
        "cols_order = [\n",
        "    \"strike\",\"Unnamed: 0\",\"n_obs\",\"n_obs_call\",\"n_obs_put\",\n",
        "    \"first_premium\",\"last_premium\",\"peak_premium\",\"trough_premium\",\n",
        "    \"abs_change\",\"pct_change\",\n",
        "    \"5min_low\",\"5min_high\",\"10min_low\",\"10min_high\",\n",
        "    \"p5_expected_lo\",\"p5_expected_hi\",\"p10_expected_lo\",\"p10_expected_hi\",\n",
        "    \"call_moneyflow\",\"put_moneyflow\",\n",
        "    \"tags\",\"reasons\",\"recommended_action\",\n",
        "    \"highconv_total\",\"highconv_success\",\"highconv_hit_rate\",\n",
        "    \"Current_Strikeprice\",\n",
        "    # add call/put detailed columns too\n",
        "    \"first_call_ltp\",\"last_call_ltp\",\"peak_call_ltp\",\"trough_call_ltp\",\"abs_change_call\",\"pct_change_call\",\n",
        "    \"first_put_ltp\",\"last_put_ltp\",\"peak_put_ltp\",\"trough_put_ltp\",\"abs_change_put\",\"pct_change_put\"\n",
        "]\n",
        "\n",
        "# keep only existing columns from the order (some may be missing)\n",
        "cols_existing = [c for c in cols_order if c in merged_df.columns]\n",
        "# append any other columns to preserve details\n",
        "other_cols = [c for c in merged_df.columns if c not in cols_existing]\n",
        "final_cols = cols_existing + other_cols\n",
        "\n",
        "merged_df.to_csv(OUT_MERGED, index=False, columns=final_cols)\n",
        "\n",
        "# Top BUY_PUTS (by pct_change_put)\n",
        "top_puts = merged_df[merged_df[\"recommended_action\"]==\"BUY_PUT\"].copy()\n",
        "if not top_puts.empty and \"pct_change_put\" in top_puts.columns:\n",
        "    top_puts = top_puts.sort_values(\"pct_change_put\", ascending=False).head(200)\n",
        "top_puts.to_csv(OUT_TOP_PUTS, index=False)\n",
        "\n",
        "# Top BUY_CALLS\n",
        "top_calls = merged_df[merged_df[\"recommended_action\"]==\"BUY_CALL\"].copy()\n",
        "if not top_calls.empty and \"pct_change_call\" in top_calls.columns:\n",
        "    top_calls = top_calls.sort_values(\"pct_change_call\", ascending=False).head(200)\n",
        "top_calls.to_csv(OUT_TOP_CALLS, index=False)\n",
        "\n",
        "# All actions sorted by strength (max of CE/PE pct)\n",
        "def compute_strength(r):\n",
        "    try:\n",
        "        a = r.get(\"pct_change_call\", 0) or 0\n",
        "        b = r.get(\"pct_change_put\", 0) or 0\n",
        "        return max(a, b)\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "merged_df[\"strength\"] = merged_df.apply(compute_strength, axis=1)\n",
        "merged_df.sort_values(\"strength\", ascending=False).to_csv(OUT_ALL_ACTIONS, index=False)\n",
        "\n",
        "# CE-only / PE-only tables\n",
        "if \"pct_change_call\" in merged_df.columns:\n",
        "    merged_df[merged_df[\"pct_change_call\"].notna()].to_csv(OUT_CE_ONLY, index=False)\n",
        "else:\n",
        "    pd.DataFrame().to_csv(OUT_CE_ONLY, index=False)\n",
        "if \"pct_change_put\" in merged_df.columns:\n",
        "    merged_df[merged_df[\"pct_change_put\"].notna()].to_csv(OUT_PE_ONLY, index=False)\n",
        "else:\n",
        "    pd.DataFrame().to_csv(OUT_PE_ONLY, index=False)\n",
        "\n",
        "# HOLD breakout candidates\n",
        "hold_df = merged_df[merged_df[\"recommended_action\"]==\"HOLD\"].copy()\n",
        "# define breakout_score as sum of positive pct changes\n",
        "hold_df[\"breakout_score\"] = (hold_df.get(\"pct_change_call\",0).clip(lower=0).fillna(0)\n",
        "                             + hold_df.get(\"pct_change_put\",0).clip(lower=0).fillna(0))\n",
        "hold_df.sort_values(\"breakout_score\", ascending=False).to_csv(OUT_HOLD_BREAKOUTS, index=False)\n",
        "\n",
        "print(\"✔ Pipeline complete. Files generated:\")\n",
        "for f in [OUT_MERGED, OUT_TOP_PUTS, OUT_TOP_CALLS, OUT_ALL_ACTIONS,\n",
        "          OUT_CE_ONLY, OUT_PE_ONLY, OUT_HOLD_BREAKOUTS,\n",
        "          OUT_REVERSALS, OUT_IV_CRUSH, OUT_PLOT, OUT_HEATMAP, OUT_AUTOTRADE]:\n",
        "    print(\"-\", f)\n",
        "\n",
        "# Show columns included in the merged output for immediate verification\n",
        "print(\"\\nMerged CSV columns (sample):\")\n",
        "print(list(merged_df.columns[:50]))\n"
      ]
    }
  ]
}