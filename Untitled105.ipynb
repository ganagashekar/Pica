{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ccqm4Uh6mJN",
        "outputId": "0bb8fc82-c996-4e2a-c5d9-ad498849d75e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated FULL_STRIKE_FORECAST_OUTPUT.csv successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "from datetime import timedelta\n",
        "\n",
        "# ============================================================\n",
        "# LOAD CSV\n",
        "# ============================================================\n",
        "df = pd.read_csv(\"flattened_snapshots.csv\", low_memory=False, parse_dates=['LTT'])\n",
        "\n",
        "# Detect which premium column exists\n",
        "prev_p = \"Previous_Call_Premium\" if \"Previous_Call_Premium\" in df else \"Previous_Call_ltp\"\n",
        "curr_p = \"Current_Call_Premium\"  if \"Current_Call_Premium\"  in df else \"Current_Call_ltp\"\n",
        "next_p = \"Next_Call_Premium\"     if \"Next_Call_Premium\"     in df else \"Next_Call_ltp\"\n",
        "\n",
        "prev_str, curr_str, next_str = \"Previous_Strikeprice\",\"Current_Strikeprice\",\"Next_Strikeprice\"\n",
        "\n",
        "# ============================================================\n",
        "# BUILD STRIKE → (timestamp, premium) TIMESERIES\n",
        "# ============================================================\n",
        "strike_series = defaultdict(list)\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    t = row[\"LTT\"]\n",
        "    for sc, pc in [(prev_str, prev_p), (curr_str, curr_p), (next_str, next_p)]:\n",
        "        if sc in row and pc in row and not pd.isna(row[sc]) and not pd.isna(row[pc]):\n",
        "            try:\n",
        "                s = int(row[sc])\n",
        "                p = float(row[pc])\n",
        "                strike_series[s].append((t, p))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "# ============================================================\n",
        "# SUMMARY + FORECAST FUNCTIONS\n",
        "# ============================================================\n",
        "def summarize(series):\n",
        "    sr = sorted(series, key=lambda x: x[0])\n",
        "    ps = [p for _, p in sr]\n",
        "\n",
        "    if not ps:\n",
        "        return None\n",
        "\n",
        "    first, last = ps[0], ps[-1]\n",
        "    peak, trough = max(ps), min(ps)\n",
        "    pct = ((last-first)/first*100) if first != 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"first_premium\": first,\n",
        "        \"last_premium\": last,\n",
        "        \"peak_premium\": peak,\n",
        "        \"trough_premium\": trough,\n",
        "        \"abs_change\": last - first,\n",
        "        \"pct_change\": pct,\n",
        "        \"n_obs\": len(ps)\n",
        "    }\n",
        "\n",
        "def forecast(stats):\n",
        "    pct = stats[\"pct_change\"]\n",
        "    last = stats[\"last_premium\"]\n",
        "\n",
        "    if pct >= 25:\n",
        "        return (last+15, last+35), (last+25, last+60)\n",
        "    elif pct >= 8:\n",
        "        return (last+6, last+18), (last+12, last+30)\n",
        "    elif pct > 0:\n",
        "        return (last+2, last+8), (last+5, last+15)\n",
        "    else:\n",
        "        return (last-5, last+2), (last-8, last+5)\n",
        "\n",
        "# ============================================================\n",
        "# BUILD SUMMARY FOR ALL STRIKES\n",
        "# ============================================================\n",
        "records = []\n",
        "\n",
        "for s, ts in strike_series.items():\n",
        "    if len(ts) < 3:\n",
        "        continue\n",
        "\n",
        "    st = summarize(ts)\n",
        "    if st is None:\n",
        "        continue\n",
        "\n",
        "    f5, f10 = forecast(st)\n",
        "\n",
        "    rec = {\n",
        "        \"strike\": s,\n",
        "        **st,\n",
        "        \"5min_low\":  f5[0],\n",
        "        \"5min_high\": f5[1],\n",
        "        \"10min_low\": f10[0],\n",
        "        \"10min_high\":f10[1],\n",
        "        \"p5_expected_lo\": f5[0] - st[\"last_premium\"],\n",
        "        \"p5_expected_hi\": f5[1] - st[\"last_premium\"],\n",
        "        \"p10_expected_lo\": f10[0] - st[\"last_premium\"],\n",
        "        \"p10_expected_hi\": f10[1] - st[\"last_premium\"]\n",
        "    }\n",
        "\n",
        "    records.append(rec)\n",
        "\n",
        "summary = pd.DataFrame(records)\n",
        "\n",
        "# ============================================================\n",
        "# STRATEGY TAGS + MONEYFLOW EXTRACTION\n",
        "# ============================================================\n",
        "tag_keywords = [\n",
        "    \"RSI\", \"MACD\", \"VWAP\", \"RSI_MACD\", \"VWAP_Divergence\", \"OI_Support_Call\",\n",
        "    \"Put Buying\", \"Call Writing\", \"PnL\", \"Momentum\", \"Breakout\"\n",
        "]\n",
        "\n",
        "strike_info = {\n",
        "    int(s): {\n",
        "        \"tags\": Counter(),\n",
        "        \"call_moneyflow\": 0.0,\n",
        "        \"put_moneyflow\": 0.0\n",
        "    }\n",
        "    for s in summary[\"strike\"]\n",
        "}\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    for sc in [prev_str, curr_str, next_str]:\n",
        "        if sc in row and not pd.isna(row[sc]):\n",
        "            try:\n",
        "                s = int(row[sc])\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            if s not in strike_info:\n",
        "                continue\n",
        "\n",
        "            # Extract tags\n",
        "            for col in [\"Previous_StrategyTag\", \"Current_StrategyTag\", \"Next_StrategyTag\"]:\n",
        "                if col in df and isinstance(row.get(col), str):\n",
        "                    for kw in tag_keywords:\n",
        "                        if kw.lower() in row[col].lower():\n",
        "                            strike_info[s][\"tags\"][kw] += 1\n",
        "\n",
        "            # Moneyflow\n",
        "            for col in [\n",
        "                \"Previous_CallMoneyFlow\",\"Current_CallMoneyFlow\",\"Next_CallMoneyFlow\",\n",
        "                \"Previous_PutMoneyFlow\",\"Current_PutMoneyFlow\",\"Next_PutMoneyFlow\"\n",
        "            ]:\n",
        "                if col in df and not pd.isna(row.get(col)):\n",
        "                    val = float(row[col])\n",
        "                    if \"Call\" in col:\n",
        "                        strike_info[s][\"call_moneyflow\"] += val\n",
        "                    else:\n",
        "                        strike_info[s][\"put_moneyflow\"] += val\n",
        "\n",
        "# ============================================================\n",
        "# HIGH CONVICTION SIGNAL HIT RATE\n",
        "# ============================================================\n",
        "highconv_col = \"Current_IsHighConvictionSignal\"\n",
        "\n",
        "# ============================================================\n",
        "# BUILD FINAL OUTPUT\n",
        "# ============================================================\n",
        "final_rows = []\n",
        "\n",
        "for _, r in summary.iterrows():\n",
        "    s = int(r[\"strike\"])\n",
        "    info = strike_info[s]\n",
        "\n",
        "    # Build reasoning text\n",
        "    tags = info[\"tags\"]\n",
        "    reasons = []\n",
        "\n",
        "    if tags.get(\"RSI\",0) or tags.get(\"RSI_MACD\",0):\n",
        "        reasons.append(\"RSI/MACD bullish pattern\")\n",
        "    if tags.get(\"VWAP_Divergence\",0):\n",
        "        reasons.append(\"VWAP divergence support\")\n",
        "    if tags.get(\"OI_Support_Call\",0):\n",
        "        reasons.append(\"OI call support detected\")\n",
        "    if tags.get(\"Put Buying\",0):\n",
        "        reasons.append(\"Put side hedging activity\")\n",
        "\n",
        "    if not reasons:\n",
        "        reasons.append(\"No strong signals\")\n",
        "\n",
        "    # Recommended Action\n",
        "    if r[\"pct_change\"] > 5:\n",
        "        act = \"BUY_CALL\"\n",
        "    elif r[\"pct_change\"] < -5:\n",
        "        act = \"BUY_PUT\"\n",
        "    else:\n",
        "        act = \"HOLD\"\n",
        "\n",
        "    # High-conviction stats\n",
        "    hc_rows = df[(df[curr_str]==s) & (df.get(highconv_col)==True)]\n",
        "    hc_total = hc_rows.shape[0]\n",
        "    hc_success = 0\n",
        "\n",
        "    for _, row2 in hc_rows.iterrows():\n",
        "        t0 = row2[\"LTT\"]\n",
        "        p0 = row2[curr_p]\n",
        "\n",
        "        window = df[\n",
        "            (df[\"LTT\"] >= t0) &\n",
        "            (df[\"LTT\"] <= t0 + timedelta(minutes=3)) &\n",
        "            (df[curr_str] == s)\n",
        "        ]\n",
        "\n",
        "        if not window.empty:\n",
        "            if window[curr_p].max() > p0:\n",
        "                hc_success += 1\n",
        "\n",
        "    final_rows.append({\n",
        "        **r.to_dict(),\n",
        "        \"call_moneyflow\": info[\"call_moneyflow\"],\n",
        "        \"put_moneyflow\":  info[\"put_moneyflow\"],\n",
        "        \"tags\": \";\".join([f\"{k}:{v}\" for k,v in tags.items()]),\n",
        "        \"reasons\": \"; \".join(reasons),\n",
        "        \"recommended_action\": act,\n",
        "        \"highconv_total\": hc_total,\n",
        "        \"highconv_success\": hc_success,\n",
        "        \"highconv_hit_rate\":\n",
        "            (hc_success / hc_total) if hc_total > 0 else None,\n",
        "\n",
        "        # REQUIRED BY YOU\n",
        "        \"Current_Strikeprice\": s\n",
        "    })\n",
        "\n",
        "final_df = pd.DataFrame(final_rows)\n",
        "\n",
        "# SAVE OUTPUT\n",
        "final_df.to_csv(\"FULL_STRIKE_FORECAST_OUTPUT.csv\", index=False)\n",
        "print(\"Generated FULL_STRIKE_FORECAST_OUTPUT.csv successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "from datetime import timedelta\n",
        "\n",
        "# ============================================================\n",
        "# LOAD CSV\n",
        "# ============================================================\n",
        "df = pd.read_csv(\"flattened_snapshots.csv\", low_memory=False, parse_dates=['LTT'])\n",
        "\n",
        "# Detect which premium column exists\n",
        "prev_p = \"Previous_Call_Premium\" if \"Previous_Call_Premium\" in df else \"Previous_Call_ltp\"\n",
        "curr_p = \"Current_Call_Premium\"  if \"Current_Call_Premium\"  in df else \"Current_Call_ltp\"\n",
        "next_p = \"Next_Call_Premium\"     if \"Next_Call_Premium\"     in df else \"Next_Call_ltp\"\n",
        "\n",
        "prev_str, curr_str, next_str = \"Previous_Strikeprice\",\"Current_Strikeprice\",\"Next_Strikeprice\"\n",
        "\n",
        "# ============================================================\n",
        "# BUILD STRIKE → (timestamp, premium) TIMESERIES\n",
        "# ============================================================\n",
        "strike_series = defaultdict(list)\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    t = row[\"LTT\"]\n",
        "    for sc, pc in [(prev_str, prev_p), (curr_str, curr_p), (next_str, next_p)]:\n",
        "        if sc in row and pc in row and not pd.isna(row[sc]) and not pd.isna(row[pc]):\n",
        "            try:\n",
        "                s = int(row[sc])\n",
        "                p = float(row[pc])\n",
        "                strike_series[s].append((t, p))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "# ============================================================\n",
        "# SUMMARY + FORECAST FUNCTIONS\n",
        "# ============================================================\n",
        "def summarize(series):\n",
        "    sr = sorted(series, key=lambda x: x[0])\n",
        "    ps = [p for _, p in sr]\n",
        "\n",
        "    if not ps:\n",
        "        return None\n",
        "\n",
        "    first, last = ps[0], ps[-1]\n",
        "    peak, trough = max(ps), min(ps)\n",
        "    pct = ((last-first)/first*100) if first != 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"first_premium\": first,\n",
        "        \"last_premium\": last,\n",
        "        \"peak_premium\": peak,\n",
        "        \"trough_premium\": trough,\n",
        "        \"abs_change\": last - first,\n",
        "        \"pct_change\": pct,\n",
        "        \"n_obs\": len(ps)\n",
        "    }\n",
        "\n",
        "def forecast(stats):\n",
        "    pct = stats[\"pct_change\"]\n",
        "    last = stats[\"last_premium\"]\n",
        "\n",
        "    if pct >= 25:\n",
        "        return (last+15, last+35), (last+25, last+60)\n",
        "    elif pct >= 8:\n",
        "        return (last+6, last+18), (last+12, last+30)\n",
        "    elif pct > 0:\n",
        "        return (last+2, last+8), (last+5, last+15)\n",
        "    else:\n",
        "        return (last-5, last+2), (last-8, last+5)\n",
        "\n",
        "# ============================================================\n",
        "# BUILD SUMMARY FOR ALL STRIKES\n",
        "# ============================================================\n",
        "records = []\n",
        "\n",
        "for s, ts in strike_series.items():\n",
        "    if len(ts) < 3:\n",
        "        continue\n",
        "\n",
        "    st = summarize(ts)\n",
        "    if st is None:\n",
        "        continue\n",
        "\n",
        "    f5, f10 = forecast(st)\n",
        "\n",
        "    rec = {\n",
        "        \"strike\": s,\n",
        "        **st,\n",
        "        \"5min_low\":  f5[0],\n",
        "        \"5min_high\": f5[1],\n",
        "        \"10min_low\": f10[0],\n",
        "        \"10min_high\": f10[1],\n",
        "        \"p5_expected_lo\": f5[0] - st[\"last_premium\"],\n",
        "        \"p5_expected_hi\": f5[1] - st[\"last_premium\"],\n",
        "        \"p10_expected_lo\": f10[0] - st[\"last_premium\"],\n",
        "        \"p10_expected_hi\": f10[1] - st[\"last_premium\"]\n",
        "    }\n",
        "\n",
        "    records.append(rec)\n",
        "\n",
        "summary = pd.DataFrame(records)\n",
        "\n",
        "# ============================================================\n",
        "# STRATEGY TAGS + MONEYFLOW EXTRACTION\n",
        "# ============================================================\n",
        "tag_keywords = [\n",
        "    \"RSI\", \"MACD\", \"VWAP\", \"RSI_MACD\", \"VWAP_Divergence\", \"OI_Support_Call\",\n",
        "    \"Put Buying\", \"Call Writing\", \"PnL\", \"Momentum\", \"Breakout\"\n",
        "]\n",
        "\n",
        "strike_info = {\n",
        "    int(s): {\n",
        "        \"tags\": Counter(),\n",
        "        \"call_moneyflow\": 0.0,\n",
        "        \"put_moneyflow\": 0.0\n",
        "    }\n",
        "    for s in summary[\"strike\"]\n",
        "}\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    for sc in [prev_str, curr_str, next_str]:\n",
        "        if sc in row and not pd.isna(row[sc]):\n",
        "            try:\n",
        "                s = int(row[sc])\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            if s not in strike_info:\n",
        "                continue\n",
        "\n",
        "            # Extract tags\n",
        "            for col in [\"Previous_StrategyTag\", \"Current_StrategyTag\", \"Next_StrategyTag\"]:\n",
        "                if col in df and isinstance(row.get(col), str):\n",
        "                    for kw in tag_keywords:\n",
        "                        if kw.lower() in row[col].lower():\n",
        "                            strike_info[s][\"tags\"][kw] += 1\n",
        "\n",
        "            # Moneyflow\n",
        "            for col in [\n",
        "                \"Previous_CallMoneyFlow\",\"Current_CallMoneyFlow\",\"Next_CallMoneyFlow\",\n",
        "                \"Previous_PutMoneyFlow\",\"Current_PutMoneyFlow\",\"Next_PutMoneyFlow\"\n",
        "            ]:\n",
        "                if col in df and not pd.isna(row.get(col)):\n",
        "                    val = float(row[col])\n",
        "                    if \"Call\" in col:\n",
        "                        strike_info[s][\"call_moneyflow\"] += val\n",
        "                    else:\n",
        "                        strike_info[s][\"put_moneyflow\"] += val\n",
        "\n",
        "# ============================================================\n",
        "# HIGH CONVICTION SIGNAL HIT RATE\n",
        "# ============================================================\n",
        "highconv_col = \"Current_IsHighConvictionSignal\"\n",
        "\n",
        "# ============================================================\n",
        "# BUILD FINAL OUTPUT\n",
        "# ============================================================\n",
        "final_rows = []\n",
        "\n",
        "for _, r in summary.iterrows():\n",
        "    s = int(r[\"strike\"])\n",
        "    info = strike_info[s]\n",
        "\n",
        "    # Build reasoning text\n",
        "    tags = info[\"tags\"]\n",
        "    reasons = []\n",
        "\n",
        "    if tags.get(\"RSI\",0) or tags.get(\"RSI_MACD\",0):\n",
        "        reasons.append(\"RSI/MACD bullish pattern\")\n",
        "    if tags.get(\"VWAP_Divergence\",0):\n",
        "        reasons.append(\"VWAP divergence support\")\n",
        "    if tags.get(\"OI_Support_Call\",0):\n",
        "        reasons.append(\"OI call support detected\")\n",
        "    if tags.get(\"Put Buying\",0):\n",
        "        reasons.append(\"Put side hedging activity\")\n",
        "\n",
        "    if not reasons:\n",
        "        reasons.append(\"No strong signals\")\n",
        "\n",
        "    # Recommended Action\n",
        "    if r[\"pct_change\"] > 5:\n",
        "        act = \"BUY_CALL\"\n",
        "    elif r[\"pct_change\"] < -5:\n",
        "        act = \"BUY_PUT\"\n",
        "    else:\n",
        "        act = \"HOLD\"\n",
        "\n",
        "    # High-conviction stats\n",
        "    hc_rows = df[(df[curr_str]==s) & (df.get(highconv_col)==True)]\n",
        "    hc_total = hc_rows.shape[0]\n",
        "    hc_success = 0\n",
        "\n",
        "    for _, row2 in hc_rows.iterrows():\n",
        "        t0 = row2[\"LTT\"]\n",
        "        p0 = row2[curr_p]\n",
        "\n",
        "        window = df[\n",
        "            (df[\"LTT\"] >= t0) &\n",
        "            (df[\"LTT\"] <= t0 + timedelta(minutes=3)) &\n",
        "            (df[curr_str] == s)\n",
        "        ]\n",
        "\n",
        "        if not window.empty:\n",
        "            if window[curr_p].max() > p0:\n",
        "                hc_success += 1\n",
        "\n",
        "    final_rows.append({\n",
        "        **r.to_dict(),\n",
        "        \"call_moneyflow\": info[\"call_moneyflow\"],\n",
        "        \"put_moneyflow\": info[\"put_moneyflow\"],\n",
        "        \"tags\": \";\".join([f\"{k}:{v}\" for k,v in tags.items()]),\n",
        "        \"reasons\": \"; \".join(reasons),\n",
        "        \"recommended_action\": act,\n",
        "        \"highconv_total\": hc_total,\n",
        "        \"highconv_success\": hc_success,\n",
        "        \"highconv_hit_rate\": (hc_success / hc_total) if hc_total > 0 else None,\n",
        "        \"Current_Strikeprice\": s\n",
        "    })\n",
        "\n",
        "final_df = pd.DataFrame(final_rows)\n",
        "\n",
        "# SAVE OUTPUT\n",
        "final_df.to_csv(\"FULL_STRIKE_FORECAST_OUTPUT.csv\", index=False)\n",
        "print(\"Generated FULL_STRIKE_FORECAST_OUTPUT.csv successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpXnFlov7eGu",
        "outputId": "df8e59b2-a8d1-4459-ca8a-e495cc141ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated FULL_STRIKE_FORECAST_OUTPUT.csv successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# full_ce_pe_pipeline.py\n",
        "\"\"\"\n",
        "Full CE + PE forecast pipeline:\n",
        "- Input: flattened_snapshots.csv (or path provided)\n",
        "- Outputs:\n",
        "    - MERGED_CE_PE_FORECAST.csv\n",
        "    - TOP_BUY_PUTS.csv\n",
        "    - SPOT_VS_MOMENTUM.png\n",
        "    - Prints alerts and saves alerts.csv (if any)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "from collections import defaultdict, Counter\n",
        "from datetime import timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -------------------------\n",
        "# Config / input file\n",
        "# -------------------------\n",
        "INPUT_CSV = \"flattened_snapshots.csv\"  # change if needed\n",
        "OUT_MERGED = \"MERGED_CE_PE_FORECAST.csv\"\n",
        "OUT_TOP_PUTS = \"TOP_BUY_PUTS.csv\"\n",
        "OUT_PLOT = \"SPOT_VS_MOMENTUM.png\"\n",
        "OUT_ALERTS = \"PUT_REVERSAL_ALERTS.csv\"\n",
        "\n",
        "# -------------------------\n",
        "# Safety check: file exists\n",
        "# -------------------------\n",
        "if not os.path.exists(INPUT_CSV):\n",
        "    print(f\"ERROR: input file not found: {INPUT_CSV}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# -------------------------\n",
        "# Load data\n",
        "# -------------------------\n",
        "df = pd.read_csv(INPUT_CSV, low_memory=False, parse_dates=['LTT'], infer_datetime_format=True)\n",
        "\n",
        "# -------------------------\n",
        "# Auto-detect premium column names (calls and puts)\n",
        "# -------------------------\n",
        "def pick_col(candidates):\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "prev_call_p = pick_col([\"Previous_Call_Premium\",\"Previous_Call_ltp\",\"Prev_Call_LTP\",\"Prev_Call_Premium\"])\n",
        "curr_call_p = pick_col([\"Current_Call_Premium\",\"Current_Call_ltp\",\"Current_Call_LTP\",\"Curr_Call_Premium\"])\n",
        "next_call_p = pick_col([\"Next_Call_Premium\",\"Next_Call_ltp\",\"Next_Call_LTP\",\"Next_Call_Premium\"])\n",
        "\n",
        "prev_put_p = pick_col([\"Previous_Put_Premium\",\"Previous_Put_ltp\",\"Prev_Put_LTP\",\"Prev_Put_Premium\"])\n",
        "curr_put_p = pick_col([\"Current_Put_Premium\",\"Current_Put_ltp\",\"Current_Put_LTP\",\"Curr_Put_Premium\"])\n",
        "next_put_p = pick_col([\"Next_Put_Premium\",\"Next_Put_ltp\",\"Next_Put_LTP\",\"Next_Put_Premium\"])\n",
        "\n",
        "# Strike columns\n",
        "prev_str = \"Previous_Strikeprice\" if \"Previous_Strikeprice\" in df.columns else \"Previous_Strike\"\n",
        "curr_str = \"Current_Strikeprice\" if \"Current_Strikeprice\" in df.columns else \"Current_Strike\"\n",
        "next_str = \"Next_Strikeprice\" if \"Next_Strikeprice\" in df.columns else \"Next_Strike\"\n",
        "\n",
        "print(\"Detected columns:\")\n",
        "print(f\" Call premiums: {prev_call_p}, {curr_call_p}, {next_call_p}\")\n",
        "print(f\" Put premiums : {prev_put_p}, {curr_put_p}, {next_put_p}\")\n",
        "print(f\" Strike cols  : {prev_str}, {curr_str}, {next_str}\")\n",
        "\n",
        "# -------------------------\n",
        "# Helpers: build series, summarize, forecast\n",
        "# -------------------------\n",
        "def build_series(pcols):\n",
        "    \"\"\"Build strike -> list of (timestamp, premium) pairs using triple columns mapping.\"\"\"\n",
        "    series = defaultdict(list)\n",
        "    for _, row in df.iterrows():\n",
        "        t = row.get('LTT')\n",
        "        if pd.isna(t):\n",
        "            continue\n",
        "        triples = [(prev_str, pcols[0]), (curr_str, pcols[1]), (next_str, pcols[2])]\n",
        "        for sc, pc in triples:\n",
        "            if sc in row and pc and pc in df.columns and not pd.isna(row[sc]) and not pd.isna(row[pc]):\n",
        "                try:\n",
        "                    s = int(row[sc])\n",
        "                    p = float(row[pc])\n",
        "                    series[s].append((t, p))\n",
        "                except Exception:\n",
        "                    # ignore parsing issues\n",
        "                    continue\n",
        "    return series\n",
        "\n",
        "def summarize(series_list):\n",
        "    \"\"\"Given a list of (t,p), return first/last/peak/trough/abs/pct/n.\"\"\"\n",
        "    sr = sorted(series_list, key=lambda x: x[0])\n",
        "    ps = [p for _, p in sr]\n",
        "    if len(ps) == 0:\n",
        "        return None\n",
        "    first, last = ps[0], ps[-1]\n",
        "    peak, trough = max(ps), min(ps)\n",
        "    abs_change = last - first\n",
        "    pct_change = (abs_change / first * 100) if first != 0 else float('nan')\n",
        "    return {\n",
        "        \"first\": first,\n",
        "        \"last\": last,\n",
        "        \"peak\": peak,\n",
        "        \"trough\": trough,\n",
        "        \"abs_change\": abs_change,\n",
        "        \"pct_change\": pct_change,\n",
        "        \"n\": len(ps)\n",
        "    }\n",
        "\n",
        "def forecast_from_pct(last, pct):\n",
        "    \"\"\"Return (5min_low,5min_high), (10min_low,10min_high) using your rules.\"\"\"\n",
        "    if pct >= 25:\n",
        "        return (last+15, last+35), (last+25, last+60)\n",
        "    elif pct >= 8:\n",
        "        return (last+6, last+18), (last+12, last+30)\n",
        "    elif pct > 0:\n",
        "        return (last+2, last+8), (last+5, last+15)\n",
        "    else:\n",
        "        return (last-5, last+2), (last-8, last+5)\n",
        "\n",
        "# -------------------------\n",
        "# Build call & put series\n",
        "# -------------------------\n",
        "call_series = build_series((prev_call_p, curr_call_p, next_call_p))\n",
        "put_series  = build_series((prev_put_p, curr_put_p, next_put_p))\n",
        "\n",
        "# -------------------------\n",
        "# Summarize all strikes for calls\n",
        "# -------------------------\n",
        "call_records = []\n",
        "for strike, ts in call_series.items():\n",
        "    if len(ts) < 3:\n",
        "        continue\n",
        "    st = summarize(ts)\n",
        "    if st is None:\n",
        "        continue\n",
        "    f5, f10 = forecast_from_pct(st['last'], st['pct_change'])\n",
        "    call_records.append({\n",
        "        \"strike\": strike,\n",
        "        \"n_obs_call\": st['n'],\n",
        "        \"first_call_premium\": st['first'],\n",
        "        \"last_call_premium\": st['last'],\n",
        "        \"peak_call_premium\": st['peak'],\n",
        "        \"trough_call_premium\": st['trough'],\n",
        "        \"abs_change_call\": st['abs_change'],\n",
        "        \"pct_change_call\": st['pct_change'],\n",
        "        \"call_5min_low\": f5[0], \"call_5min_high\": f5[1],\n",
        "        \"call_10min_low\": f10[0], \"call_10min_high\": f10[1],\n",
        "        \"call_p5_lo\": f5[0] - st['last'], \"call_p5_hi\": f5[1] - st['last'],\n",
        "        \"call_p10_lo\": f10[0] - st['last'], \"call_p10_hi\": f10[1] - st['last'],\n",
        "    })\n",
        "\n",
        "call_df = pd.DataFrame(call_records).set_index('strike') if call_records else pd.DataFrame()\n",
        "\n",
        "# -------------------------\n",
        "# Summarize all strikes for puts\n",
        "# -------------------------\n",
        "put_records = []\n",
        "for strike, ts in put_series.items():\n",
        "    if len(ts) < 3:\n",
        "        continue\n",
        "    st = summarize(ts)\n",
        "    if st is None:\n",
        "        continue\n",
        "    f5, f10 = forecast_from_pct(st['last'], st['pct_change'])\n",
        "    put_records.append({\n",
        "        \"strike\": strike,\n",
        "        \"n_obs_put\": st['n'],\n",
        "        \"first_put_premium\": st['first'],\n",
        "        \"last_put_premium\": st['last'],\n",
        "        \"peak_put_premium\": st['peak'],\n",
        "        \"trough_put_premium\": st['trough'],\n",
        "        \"abs_change_put\": st['abs_change'],\n",
        "        \"pct_change_put\": st['pct_change'],\n",
        "        \"put_5min_low\": f5[0], \"put_5min_high\": f5[1],\n",
        "        \"put_10min_low\": f10[0], \"put_10min_high\": f10[1],\n",
        "        \"put_p5_lo\": f5[0] - st['last'], \"put_p5_hi\": f5[1] - st['last'],\n",
        "        \"put_p10_lo\": f10[0] - st['last'], \"put_p10_hi\": f10[1] - st['last'],\n",
        "    })\n",
        "\n",
        "put_df = pd.DataFrame(put_records).set_index('strike') if put_records else pd.DataFrame()\n",
        "\n",
        "# -------------------------\n",
        "# Merge CE + PE summary frames\n",
        "# -------------------------\n",
        "if not call_df.empty and not put_df.empty:\n",
        "    merged = pd.concat([call_df, put_df], axis=1, sort=True).reset_index().rename(columns={'index':'strike'})\n",
        "elif not call_df.empty:\n",
        "    merged = call_df.reset_index().rename(columns={'index':'strike'})\n",
        "    merged['strike'] = merged['strike'].astype(int)\n",
        "    # ensure put columns exist\n",
        "    for col in ['pct_change_put','last_put_premium','put_moneyflow']:\n",
        "        if col not in merged.columns:\n",
        "            merged[col] = np.nan\n",
        "elif not put_df.empty:\n",
        "    merged = put_df.reset_index().rename(columns={'index':'strike'})\n",
        "    merged['strike'] = merged['strike'].astype(int)\n",
        "    for col in ['pct_change_call','last_call_premium','call_moneyflow']:\n",
        "        if col not in merged.columns:\n",
        "            merged[col] = np.nan\n",
        "else:\n",
        "    merged = pd.DataFrame(columns=['strike'])\n",
        "\n",
        "# -------------------------\n",
        "# Extract tags and moneyflow from the raw df\n",
        "# Generic: any column that contains \"Tag\" or \"MoneyFlow\" or \"CallMoneyFlow\"/\"PutMoneyFlow\"\n",
        "# -------------------------\n",
        "tag_keywords = [\"RSI\",\"MACD\",\"VWAP\",\"RSI_MACD\",\"VWAP_Divergence\",\"OI_Support_Call\",\"Put Buying\",\"Call Writing\",\"Bearish\",\"Bullish\"]\n",
        "\n",
        "# initialize strike_info\n",
        "strike_info = {}\n",
        "for s in merged['strike'].dropna().astype(int).unique():\n",
        "    strike_info[int(s)] = {\"tags\": Counter(), \"call_moneyflow\": 0.0, \"put_moneyflow\": 0.0}\n",
        "\n",
        "# scan raw dataframe for tag/moneyflow columns\n",
        "for _, row in df.iterrows():\n",
        "    for sc in (prev_str, curr_str, next_str):\n",
        "        if sc not in df.columns:\n",
        "            continue\n",
        "        if pd.isna(row.get(sc)):\n",
        "            continue\n",
        "        try:\n",
        "            s = int(row.get(sc))\n",
        "        except Exception:\n",
        "            continue\n",
        "        if s not in strike_info:\n",
        "            continue\n",
        "\n",
        "        # tags\n",
        "        for col in df.columns:\n",
        "            if \"Tag\" in col or \"tag\" in col:\n",
        "                v = row.get(col)\n",
        "                if isinstance(v, str):\n",
        "                    for kw in tag_keywords:\n",
        "                        if kw.lower() in v.lower():\n",
        "                            strike_info[s]['tags'][kw] += 1\n",
        "\n",
        "        # moneyflow columns (broad match)\n",
        "        for col in df.columns:\n",
        "            if \"CallMoneyFlow\" in col or (\"Call\" in col and \"MoneyFlow\" in col):\n",
        "                try:\n",
        "                    val = row.get(col)\n",
        "                    if not pd.isna(val):\n",
        "                        strike_info[s]['call_moneyflow'] += float(val)\n",
        "                except Exception:\n",
        "                    pass\n",
        "            if \"PutMoneyFlow\" in col or (\"Put\" in col and \"MoneyFlow\" in col):\n",
        "                try:\n",
        "                    val = row.get(col)\n",
        "                    if not pd.isna(val):\n",
        "                        strike_info[s]['put_moneyflow'] += float(val)\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "# attach tags and moneyflow to merged\n",
        "def format_tags(s):\n",
        "    t = strike_info.get(int(s), {}).get('tags', {})\n",
        "    return \";\".join([f\"{k}:{v}\" for k,v in t.items()]) if t else \"\"\n",
        "\n",
        "merged['tags'] = merged['strike'].apply(lambda s: format_tags(s) if not pd.isna(s) else \"\")\n",
        "merged['call_moneyflow'] = merged['strike'].apply(lambda s: strike_info.get(int(s), {}).get('call_moneyflow', 0.0))\n",
        "merged['put_moneyflow']  = merged['strike'].apply(lambda s: strike_info.get(int(s), {}).get('put_moneyflow', 0.0))\n",
        "\n",
        "# -------------------------\n",
        "# Decide recommended_action using both call & put momentum + tag boosts\n",
        "# -------------------------\n",
        "def decide_action(row):\n",
        "    call_pct = row.get('pct_change_call', 0) if not pd.isna(row.get('pct_change_call')) else 0\n",
        "    put_pct  = row.get('pct_change_put', 0)  if not pd.isna(row.get('pct_change_put')) else 0\n",
        "    tags = (row.get('tags') or \"\").lower()\n",
        "    bear_boost = ('put buying' in tags) or ('call writing' in tags) or ('bearish' in tags)\n",
        "    bull_boost = ('call buying' in tags) or ('bullish' in tags) or ('oi_support_call' in tags)\n",
        "\n",
        "    # Prioritize stronger momentum, but allow tag boosts\n",
        "    if (put_pct > 8) or (put_pct > 5 and bear_boost):\n",
        "        return \"BUY_PUT\"\n",
        "    if (call_pct > 8) or (call_pct > 5 and bull_boost):\n",
        "        return \"BUY_CALL\"\n",
        "    return \"HOLD\"\n",
        "\n",
        "merged['recommended_action'] = merged.apply(decide_action, axis=1)\n",
        "\n",
        "# -------------------------\n",
        "# Top BUY_PUTs\n",
        "# -------------------------\n",
        "top_buy_puts = merged[merged['recommended_action'] == 'BUY_PUT'].copy()\n",
        "if 'pct_change_put' in top_buy_puts.columns:\n",
        "    top_buy_puts = top_buy_puts.sort_values('pct_change_put', ascending=False).head(100)\n",
        "else:\n",
        "    top_buy_puts = top_buy_puts.head(100)\n",
        "\n",
        "# -------------------------\n",
        "# Save merged CSVs\n",
        "# -------------------------\n",
        "merged.to_csv(OUT_MERGED, index=False)\n",
        "top_buy_puts.to_csv(OUT_TOP_PUTS, index=False)\n",
        "print(f\"Saved merged output -> {OUT_MERGED}\")\n",
        "print(f\"Saved top buy-put output -> {OUT_TOP_PUTS}\")\n",
        "\n",
        "# -------------------------\n",
        "# Spot vs momentum chart (summary)\n",
        "# -------------------------\n",
        "# Get last spot if exists\n",
        "if 'SpotPrice' in df.columns:\n",
        "    df2 = df.sort_values('LTT')\n",
        "    last_spot = df2['SpotPrice'].dropna().iloc[-1] if len(df2['SpotPrice'].dropna())>0 else np.nan\n",
        "else:\n",
        "    last_spot = np.nan\n",
        "\n",
        "avg_call_mom = merged['pct_change_call'].replace([np.inf, -np.inf], np.nan).dropna().mean() if 'pct_change_call' in merged.columns else np.nan\n",
        "avg_put_mom  = merged['pct_change_put'].replace([np.inf, -np.inf], np.nan).dropna().mean() if 'pct_change_put' in merged.columns else np.nan\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.title(\"Spot (last) vs Avg Call/Put %change (momentum summary)\")\n",
        "plt.bar([0,1,2], [last_spot if not math.isnan(last_spot) else 0,\n",
        "                  avg_call_mom if not pd.isna(avg_call_mom) else 0,\n",
        "                  avg_put_mom if not pd.isna(avg_put_mom) else 0],\n",
        "        tick_label=['Last Spot','Avg Call %','Avg Put %'])\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUT_PLOT, dpi=150)\n",
        "plt.close()\n",
        "print(f\"Saved momentum plot -> {OUT_PLOT}\")\n",
        "\n",
        "# -------------------------\n",
        "# Alerts: Put reversal candidates\n",
        "# criteria: pct_change_put > 8% AND (put buying tag or put_moneyflow > 0 or 'bearish' tag)\n",
        "# -------------------------\n",
        "alerts = []\n",
        "for _, row in merged.iterrows():\n",
        "    pct_put = row.get('pct_change_put') if 'pct_change_put' in row else None\n",
        "    if pct_put is None or (isinstance(pct_put, float) and np.isnan(pct_put)):\n",
        "        continue\n",
        "    tags = (row.get('tags') or \"\").lower()\n",
        "    put_moneyflow = row.get('put_moneyflow', 0.0) if 'put_moneyflow' in row else 0.0\n",
        "    if pct_put > 8 and (('put buying' in tags) or ('bearish' in tags) or (put_moneyflow and put_moneyflow > 0)):\n",
        "        alerts.append({\n",
        "            \"strike\": int(row['strike']),\n",
        "            \"pct_change_put\": pct_put,\n",
        "            \"put_moneyflow\": put_moneyflow,\n",
        "            \"tags\": row.get('tags', \"\"),\n",
        "            \"recommended_action\": row.get('recommended_action', \"\"),\n",
        "            \"reason\": \"Put momentum >8% and bearish tag/moneyflow\"\n",
        "        })\n",
        "\n",
        "if alerts:\n",
        "    alerts_df = pd.DataFrame(alerts)\n",
        "    alerts_df.to_csv(OUT_ALERTS, index=False)\n",
        "    print(f\"Put reversal alerts saved -> {OUT_ALERTS}\")\n",
        "    print(\"Sample alerts:\")\n",
        "    print(alerts_df.head(20).to_string(index=False))\n",
        "else:\n",
        "    print(\"No put reversal alerts detected.\")\n",
        "\n",
        "# -------------------------\n",
        "# Done\n",
        "# -------------------------\n",
        "print(\"Pipeline finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAULbqhH8qy9",
        "outputId": "69ccfaca-bc4a-488f-c051-294e1a4d2402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-288545211.py:41: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  df = pd.read_csv(INPUT_CSV, low_memory=False, parse_dates=['LTT'], infer_datetime_format=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected columns:\n",
            " Call premiums: Previous_Call_Premium, Current_Call_Premium, Next_Call_Premium\n",
            " Put premiums : Previous_Put_Premium, Current_Put_Premium, Next_Put_Premium\n",
            " Strike cols  : Previous_Strikeprice, Current_Strikeprice, Next_Strikeprice\n",
            "Saved merged output -> MERGED_CE_PE_FORECAST.csv\n",
            "Saved top buy-put output -> TOP_BUY_PUTS.csv\n",
            "Saved momentum plot -> SPOT_VS_MOMENTUM.png\n",
            "Put reversal alerts saved -> PUT_REVERSAL_ALERTS.csv\n",
            "Sample alerts:\n",
            " strike  pct_change_put  put_moneyflow                                                                                       tags recommended_action                                     reason\n",
            "  58300      276.490439        15400.0  RSI:11407;MACD:11407;VWAP:24842;RSI_MACD:11407;VWAP_Divergence:24842;OI_Support_Call:8690            BUY_PUT Put momentum >8% and bearish tag/moneyflow\n",
            "  58400       51.577564        59230.5 VWAP:50812;VWAP_Divergence:50812;OI_Support_Call:18047;RSI:20310;MACD:20310;RSI_MACD:20310            BUY_PUT Put momentum >8% and bearish tag/moneyflow\n",
            "Pipeline finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# full_ce_pe_pipeline_use_ltp.py\n",
        "import os, sys, math\n",
        "from collections import defaultdict, Counter\n",
        "from datetime import timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "INPUT_CSV = \"flattened_snapshots.csv\"   # your uploaded file\n",
        "OUT_MERGED = \"MERGED_CE_PE_FORECAST.csv\"\n",
        "OUT_TOP_PUTS = \"TOP_BUY_PUTS.csv\"\n",
        "OUT_ALERTS = \"PUT_REVERSAL_ALERTS.csv\"\n",
        "OUT_PLOT = \"SPOT_VS_MOMENTUM.png\"\n",
        "\n",
        "if not os.path.exists(INPUT_CSV):\n",
        "    print(\"ERROR: input file not found:\", INPUT_CSV)\n",
        "    sys.exit(1)\n",
        "\n",
        "# ---------- LOAD ----------\n",
        "df = pd.read_csv(INPUT_CSV, low_memory=False, parse_dates=['LTT'], infer_datetime_format=True)\n",
        "\n",
        "# ---------- USE LTP COLUMNS (explicit) ----------\n",
        "# Call LTP columns (preferred)\n",
        "PREV_CALL_LTP = \"Previous_Call_ltp\"\n",
        "CURR_CALL_LTP = \"Current_Call_ltp\"\n",
        "NEXT_CALL_LTP = \"Next_Call_ltp\"\n",
        "\n",
        "# Put LTP columns (preferred)\n",
        "PREV_PUT_LTP  = \"Previous_Put_ltp\"\n",
        "CURR_PUT_LTP  = \"Current_Put_ltp\"\n",
        "NEXT_PUT_LTP  = \"Next_Put_ltp\"\n",
        "\n",
        "# Strike columns (explicit)\n",
        "PREV_STR = \"Previous_Strikeprice\"\n",
        "CURR_STR  = \"Current_Strikeprice\"\n",
        "NEXT_STR  = \"Next_Strikeprice\"\n",
        "\n",
        "# Validate existence (fall back to *_Premium if LTP missing)\n",
        "def ensure(col_ltp, col_premium):\n",
        "    if col_ltp in df.columns:\n",
        "        return col_ltp\n",
        "    if col_premium in df.columns:\n",
        "        return col_premium\n",
        "    return None\n",
        "\n",
        "# If your file also contains *_Premium columns, we ignore them because you asked \"Use LTP\".\n",
        "prev_call_col = PREV_CALL_LTP if PREV_CALL_LTP in df.columns else ( \"Previous_Call_Premium\" if \"Previous_Call_Premium\" in df.columns else None )\n",
        "curr_call_col = CURR_CALL_LTP if CURR_CALL_LTP in df.columns else ( \"Current_Call_Premium\"  if \"Current_Call_Premium\"  in df.columns else None )\n",
        "next_call_col = NEXT_CALL_LTP if NEXT_CALL_LTP in df.columns else ( \"Next_Call_Premium\"     if \"Next_Call_Premium\"     in df.columns else None )\n",
        "\n",
        "prev_put_col  = PREV_PUT_LTP  if PREV_PUT_LTP  in df.columns else ( \"Previous_Put_Premium\" if \"Previous_Put_Premium\" in df.columns else None )\n",
        "curr_put_col  = CURR_PUT_LTP  if CURR_PUT_LTP  in df.columns else ( \"Current_Put_Premium\"  if \"Current_Put_Premium\"  in df.columns else None )\n",
        "next_put_col  = NEXT_PUT_LTP  if NEXT_PUT_LTP  in df.columns else ( \"Next_Put_Premium\"     if \"Next_Put_Premium\"     in df.columns else None )\n",
        "\n",
        "# Show what we're using\n",
        "print(\"Using columns (LTP prioritized):\")\n",
        "print(\"Calls:\", prev_call_col, curr_call_col, next_call_col)\n",
        "print(\"Puts :\", prev_put_col,  curr_put_col,  next_put_col)\n",
        "print(\"Strikes:\", PREV_STR, CURR_STR, NEXT_STR)\n",
        "\n",
        "# ---------- BUILD SERIES ----------\n",
        "def build_series(prev_col, curr_col, next_col, prev_str_col=PREV_STR, curr_str_col=CURR_STR, next_str_col=NEXT_STR):\n",
        "    series = defaultdict(list)\n",
        "    for _, row in df.iterrows():\n",
        "        t = row.get(\"LTT\")\n",
        "        if pd.isna(t):\n",
        "            continue\n",
        "        triples = [\n",
        "            (prev_str_col, prev_col),\n",
        "            (curr_str_col, curr_col),\n",
        "            (next_str_col, next_col)\n",
        "        ]\n",
        "        for sc, pc in triples:\n",
        "            if sc is None or pc is None:\n",
        "                continue\n",
        "            if sc not in df.columns or pc not in df.columns:\n",
        "                continue\n",
        "            if pd.isna(row.get(sc)) or pd.isna(row.get(pc)):\n",
        "                continue\n",
        "            try:\n",
        "                s = int(row.get(sc))\n",
        "                p = float(row.get(pc))\n",
        "            except Exception:\n",
        "                continue\n",
        "            series[s].append((t, p))\n",
        "    return series\n",
        "\n",
        "call_series = build_series(prev_call_col, curr_call_col, next_call_col)\n",
        "put_series  = build_series(prev_put_col, curr_put_col, next_put_col)\n",
        "\n",
        "print(\"Detected call strikes:\", len(call_series))\n",
        "print(\"Detected put strikes :\", len(put_series))\n",
        "\n",
        "# ---------- SUMMARIZE & FORECAST HELPERS ----------\n",
        "def summarize(pairs):\n",
        "    sr = sorted(pairs, key=lambda x: x[0])\n",
        "    ps = [p for _, p in sr]\n",
        "    if not ps:\n",
        "        return None\n",
        "    first, last = ps[0], ps[-1]\n",
        "    peak, trough = max(ps), min(ps)\n",
        "    abs_chg = last - first\n",
        "    pct_chg = (abs_chg / first * 100) if first != 0 else np.nan\n",
        "    return {\"first\": first, \"last\": last, \"peak\": peak, \"trough\": trough,\n",
        "            \"abs_change\": abs_chg, \"pct_change\": pct_chg, \"n_obs\": len(ps)}\n",
        "\n",
        "def forecast_from_pct(last, pct):\n",
        "    if pct >= 25:\n",
        "        return (last+15, last+35), (last+25, last+60)\n",
        "    elif pct >= 8:\n",
        "        return (last+6, last+18), (last+12, last+30)\n",
        "    elif pct > 0:\n",
        "        return (last+2, last+8), (last+5, last+15)\n",
        "    else:\n",
        "        return (last-5, last+2), (last-8, last+5)\n",
        "\n",
        "# ---------- BUILD CALL & PUT SUMMARIES ----------\n",
        "call_records = []\n",
        "for s, ts in call_series.items():\n",
        "    if len(ts) < 1:\n",
        "        continue\n",
        "    st = summarize(ts)\n",
        "    if st is None:\n",
        "        continue\n",
        "    f5, f10 = forecast_from_pct(st[\"last\"], st[\"pct_change\"])\n",
        "    call_records.append({\n",
        "        \"strike\": s,\n",
        "        \"n_obs_call\": st[\"n_obs\"],\n",
        "        \"first_call_ltp\": st[\"first\"],\n",
        "        \"last_call_ltp\": st[\"last\"],\n",
        "        \"peak_call_ltp\": st[\"peak\"],\n",
        "        \"trough_call_ltp\": st[\"trough\"],\n",
        "        \"abs_change_call\": st[\"abs_change\"],\n",
        "        \"pct_change_call\": st[\"pct_change\"],\n",
        "        \"call_5min_low\": f5[0], \"call_5min_high\": f5[1],\n",
        "        \"call_10min_low\": f10[0], \"call_10min_high\": f10[1],\n",
        "        \"call_p5_lo\": f5[0]-st[\"last\"], \"call_p5_hi\": f5[1]-st[\"last\"],\n",
        "        \"call_p10_lo\": f10[0]-st[\"last\"], \"call_p10_hi\": f10[1]-st[\"last\"]\n",
        "    })\n",
        "\n",
        "put_records = []\n",
        "for s, ts in put_series.items():\n",
        "    if len(ts) < 1:\n",
        "        continue\n",
        "    st = summarize(ts)\n",
        "    if st is None:\n",
        "        continue\n",
        "    f5, f10 = forecast_from_pct(st[\"last\"], st[\"pct_change\"])\n",
        "    put_records.append({\n",
        "        \"strike\": s,\n",
        "        \"n_obs_put\": st[\"n_obs\"],\n",
        "        \"first_put_ltp\": st[\"first\"],\n",
        "        \"last_put_ltp\": st[\"last\"],\n",
        "        \"peak_put_ltp\": st[\"peak\"],\n",
        "        \"trough_put_ltp\": st[\"trough\"],\n",
        "        \"abs_change_put\": st[\"abs_change\"],\n",
        "        \"pct_change_put\": st[\"pct_change\"],\n",
        "        \"put_5min_low\": f5[0], \"put_5min_high\": f5[1],\n",
        "        \"put_10min_low\": f10[0], \"put_10min_high\": f10[1],\n",
        "        \"put_p5_lo\": f5[0]-st[\"last\"], \"put_p5_hi\": f5[1]-st[\"last\"],\n",
        "        \"put_p10_lo\": f10[0]-st[\"last\"], \"put_p10_hi\": f10[1]-st[\"last\"]\n",
        "    })\n",
        "\n",
        "call_df = pd.DataFrame(call_records)\n",
        "put_df  = pd.DataFrame(put_records)\n",
        "\n",
        "# ---------- OUTER MERGE (call + put) ----------\n",
        "if not call_df.empty and not put_df.empty:\n",
        "    merged = pd.merge(call_df, put_df, on=\"strike\", how=\"outer\", sort=True)\n",
        "elif not call_df.empty:\n",
        "    merged = call_df.copy()\n",
        "    # ensure put columns exist\n",
        "    for c in [\"pct_change_put\",\"last_put_ltp\",\"put_moneyflow\"]:\n",
        "        if c not in merged.columns:\n",
        "            merged[c] = np.nan\n",
        "elif not put_df.empty:\n",
        "    merged = put_df.copy()\n",
        "    for c in [\"pct_change_call\",\"last_call_ltp\",\"call_moneyflow\"]:\n",
        "        if c not in merged.columns:\n",
        "            merged[c] = np.nan\n",
        "else:\n",
        "    merged = pd.DataFrame(columns=[\"strike\"])\n",
        "\n",
        "# ---------- EXTRACT TAGS & MONEYFLOW (use explicit columns you provided) ----------\n",
        "tag_cols = [\"Previous_StrategyTag\",\"Current_StrategyTag\",\"Next_StrategyTag\"]\n",
        "call_money_cols = [\"Previous_CallMoneyFlow\",\"Current_CallMoneyFlow\",\"Next_CallMoneyFlow\",\n",
        "                   \"Previous_TotalcallMoneyFlow\",\"Current_TotalcallMoneyFlow\",\"Next_TotalcallMoneyFlow\"]\n",
        "put_money_cols  = [\"Previous_PutMoneyFlow\",\"Current_PutMoneyFlow\",\"Next_PutMoneyFlow\",\n",
        "                   \"Previous_TotalputMoneyFlow\",\"Current_TotalputMoneyFlow\",\"Next_TotalputMoneyFlow\"]\n",
        "\n",
        "# initialize strike_info\n",
        "strike_info = {}\n",
        "for s in merged[\"strike\"].dropna().astype(int).unique():\n",
        "    strike_info[int(s)] = {\"tags\":Counter(), \"call_moneyflow\":0.0, \"put_moneyflow\":0.0}\n",
        "\n",
        "# scan\n",
        "for _, row in df.iterrows():\n",
        "    for sc in (PREV_STR, CURR_STR, NEXT_STR):\n",
        "        if sc not in df.columns:\n",
        "            continue\n",
        "        if pd.isna(row.get(sc)):\n",
        "            continue\n",
        "        try:\n",
        "            s = int(row.get(sc))\n",
        "        except:\n",
        "            continue\n",
        "        if s not in strike_info:\n",
        "            continue\n",
        "        # tags\n",
        "        for col in tag_cols:\n",
        "            if col in df.columns:\n",
        "                v = row.get(col)\n",
        "                if isinstance(v, str) and v.strip():\n",
        "                    # split common separators\n",
        "                    for token in [t.strip() for t in v.replace(\"|\",\";\").split(\";\") if t.strip()]:\n",
        "                        # increment token counter\n",
        "                        strike_info[s][\"tags\"][token] += 1\n",
        "        # moneyflows\n",
        "        for col in call_money_cols:\n",
        "            if col in df.columns:\n",
        "                try:\n",
        "                    val = row.get(col)\n",
        "                    if not pd.isna(val):\n",
        "                        strike_info[s][\"call_moneyflow\"] += float(val)\n",
        "                except:\n",
        "                    pass\n",
        "        for col in put_money_cols:\n",
        "            if col in df.columns:\n",
        "                try:\n",
        "                    val = row.get(col)\n",
        "                    if not pd.isna(val):\n",
        "                        strike_info[s][\"put_moneyflow\"] += float(val)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "# attach tags/moneyflow to merged\n",
        "def fmt_tags(s):\n",
        "    t = strike_info.get(int(s), {}).get(\"tags\", {})\n",
        "    return \";\".join([f\"{k}:{v}\" for k,v in t.items()]) if t else \"\"\n",
        "\n",
        "merged[\"tags\"] = merged[\"strike\"].apply(lambda s: fmt_tags(s) if not pd.isna(s) else \"\")\n",
        "merged[\"call_moneyflow\"] = merged[\"strike\"].apply(lambda s: strike_info.get(int(s), {}).get(\"call_moneyflow\", 0.0))\n",
        "merged[\"put_moneyflow\"]  = merged[\"strike\"].apply(lambda s: strike_info.get(int(s), {}).get(\"put_moneyflow\", 0.0))\n",
        "\n",
        "# ---------- DECIDE RECOMMENDED ACTION (dual-side logic) ----------\n",
        "def decide_action(row):\n",
        "    call_pct = row.get(\"pct_change_call\") if \"pct_change_call\" in row else (row.get(\"pct_change_call\") if \"pct_change_call\" in row.index else 0)\n",
        "    put_pct  = row.get(\"pct_change_put\")  if \"pct_change_put\" in row else (row.get(\"pct_change_put\") if \"pct_change_put\" in row.index else 0)\n",
        "    call_pct = 0 if pd.isna(call_pct) else call_pct\n",
        "    put_pct  = 0 if pd.isna(put_pct) else put_pct\n",
        "\n",
        "    tags = (row.get(\"tags\") or \"\").lower()\n",
        "    bear_boost = (\"put buying\" in tags) or (\"call writing\" in tags) or (\"bearish\" in tags)\n",
        "    bull_boost = (\"call buying\" in tags) or (\"bullish\" in tags) or (\"oi_support_call\" in tags)\n",
        "\n",
        "    # priority: put momentum (bearish) or call momentum (bullish)\n",
        "    if (put_pct > 8) or (put_pct > 5 and bear_boost):\n",
        "        return \"BUY_PUT\"\n",
        "    if (call_pct > 8) or (call_pct > 5 and bull_boost):\n",
        "        return \"BUY_CALL\"\n",
        "    return \"HOLD\"\n",
        "\n",
        "merged[\"recommended_action\"] = merged.apply(decide_action, axis=1)\n",
        "\n",
        "# ---------- TOP BUY_PUTS ----------\n",
        "top_buy_puts = merged[merged[\"recommended_action\"]==\"BUY_PUT\"].copy()\n",
        "if \"pct_change_put\" in top_buy_puts.columns:\n",
        "    top_buy_puts = top_buy_puts.sort_values(\"pct_change_put\", ascending=False).head(200)\n",
        "else:\n",
        "    top_buy_puts = top_buy_puts.head(200)\n",
        "\n",
        "# ---------- SAVE CSVs ----------\n",
        "merged.to_csv(OUT_MERGED, index=False)\n",
        "top_buy_puts.to_csv(OUT_TOP_PUTS, index=False)\n",
        "print(\"Saved:\", OUT_MERGED, OUT_TOP_PUTS)\n",
        "\n",
        "# ---------- SPOT vs momentum plot ----------\n",
        "last_spot = df[\"SpotPrice\"].dropna().iloc[-1] if \"SpotPrice\" in df.columns and not df[\"SpotPrice\"].dropna().empty else np.nan\n",
        "avg_call_pct = merged[\"pct_change_call\"].replace([np.inf, -np.inf], np.nan).dropna().mean() if \"pct_change_call\" in merged.columns else np.nan\n",
        "avg_put_pct  = merged[\"pct_change_put\"].replace([np.inf, -np.inf], np.nan).dropna().mean()  if \"pct_change_put\" in merged.columns  else np.nan\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.title(\"Spot (last) vs Avg CE/PE %change\")\n",
        "vals = [last_spot if not math.isnan(last_spot) else 0, avg_call_pct if not pd.isna(avg_call_pct) else 0, avg_put_pct if not pd.isna(avg_put_pct) else 0]\n",
        "plt.bar([\"Spot Last\",\"Avg CE %\",\"Avg PE %\"], vals)\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUT_PLOT, dpi=150)\n",
        "plt.close()\n",
        "print(\"Saved:\", OUT_PLOT)\n",
        "\n",
        "# ---------- ALERTS: PUT REVERSAL CANDIDATES ----------\n",
        "alerts = []\n",
        "for _, r in merged.iterrows():\n",
        "    pct_put = r.get(\"pct_change_put\")\n",
        "    if pct_put is None or (isinstance(pct_put, float) and np.isnan(pct_put)):\n",
        "        continue\n",
        "    tags = (r.get(\"tags\") or \"\").lower()\n",
        "    put_money = r.get(\"put_moneyflow\", 0.0) if \"put_moneyflow\" in r else 0.0\n",
        "    if pct_put > 8 and ((\"put buying\" in tags) or (\"bearish\" in tags) or (put_money and put_money > 0)):\n",
        "        alerts.append({\n",
        "            \"strike\": int(r[\"strike\"]),\n",
        "            \"pct_change_put\": pct_put,\n",
        "            \"put_moneyflow\": put_money,\n",
        "            \"tags\": r.get(\"tags\",\"\"),\n",
        "            \"recommended_action\": r.get(\"recommended_action\",\"\"),\n",
        "            \"reason\": \"Put momentum >8% and bearish tag/moneyflow\"\n",
        "        })\n",
        "\n",
        "alerts_df = pd.DataFrame(alerts)\n",
        "if not alerts_df.empty:\n",
        "    alerts_df.to_csv(OUT_ALERTS, index=False)\n",
        "    print(\"Saved:\", OUT_ALERTS)\n",
        "    print(\"Sample alerts:\\n\", alerts_df.head(20).to_string(index=False))\n",
        "else:\n",
        "    print(\"No put reversal alerts found.\")\n",
        "\n",
        "print(\"Pipeline complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FmK_rSS-RIF",
        "outputId": "7baa0b5b-6633-4be1-c41d-c40ac9212673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1326839975.py:22: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  df = pd.read_csv(INPUT_CSV, low_memory=False, parse_dates=['LTT'], infer_datetime_format=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using columns (LTP prioritized):\n",
            "Calls: Previous_Call_ltp Current_Call_ltp Next_Call_ltp\n",
            "Puts : Previous_Put_ltp Current_Put_ltp Next_Put_ltp\n",
            "Strikes: Previous_Strikeprice Current_Strikeprice Next_Strikeprice\n",
            "Detected call strikes: 8\n",
            "Detected put strikes : 8\n",
            "Saved: MERGED_CE_PE_FORECAST.csv TOP_BUY_PUTS.csv\n",
            "Saved: SPOT_VS_MOMENTUM.png\n",
            "Saved: PUT_REVERSAL_ALERTS.csv\n",
            "Sample alerts:\n",
            "  strike  pct_change_put  put_moneyflow                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 tags recommended_action                                     reason\n",
            "  58500        9.674843   1.429351e+13 RSI_MACD_Bull VWAP_Divergence PnL_Explosion:6182;RSI_MACD_Bull VWAP_Divergence PnL_Explosion OI_Support_Call:2869;VWAP_Divergence PnL_Explosion:17872;VWAP_Divergence PnL_Explosion OI_Support_Call:9097;RSI_MACD_Bear VWAP_Divergence PnL_Explosion OI_Support_Put:3170;RSI_MACD_Bear VWAP_Divergence PnL_Explosion OI_Support_Call:2407;DeltaNeutral:1540;LowRealHighIV:1351;DeltaNeutral LowRealHighIV:593;VWAP_Divergence PnL_Explosion Prem_Momentum- OI_Support_Call:6;RSI_MACD_Bear VWAP_Divergence PnL_Explosion:2461;RSI_MACD_Bear VWAP_Divergence PnL_Explosion Prem_Momentum+ OI_Support_Call:34;RSI_MACD_Bear VWAP_Divergence PnL_Explosion Prem_Momentum+ OI_Support_Put:38;VWAP_Divergence PnL_Explosion Prem_Momentum+ OI_Support_Call:155;RSI_MACD_Bull VWAP_Divergence PnL_Explosion Prem_Momentum+ OI_Support_Call:51;VWAP_Divergence PnL_Explosion Prem_Momentum+:144;RSI_MACD_Bull VWAP_Divergence PnL_Explosion Prem_Momentum- OI_Support_Call:1;RSI_MACD_Bear VWAP_Divergence PnL_Explosion Prem_Momentum-:3;RSI_MACD_Bull VWAP_Divergence PnL_Explosion Prem_Momentum+:84;RSI_MACD_Bear VWAP_Divergence PnL_Explosion Prem_Momentum+:7;RSI_MACD_Bear VWAP_Divergence PnL_Explosion Prem_Momentum- OI_Support_Call:3;RSI_MACD_Bull VWAP_Divergence PnL_Explosion Prem_Momentum-:5;VWAP_Divergence PnL_Explosion Prem_Momentum-:12;RSI_MACD_Bear VWAP_Divergence PnL_Explosion Prem_Momentum- OI_Support_Put:14            BUY_PUT Put momentum >8% and bearish tag/moneyflow\n",
            "Pipeline complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# full_ce_pe_pipeline_use_ltp_enriched.py\n",
        "import os, sys, math\n",
        "from collections import defaultdict, Counter\n",
        "from datetime import timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "INPUT_CSV = \"flattened_snapshots.csv\"   # your uploaded file\n",
        "OUT_MERGED = \"MERGED_CE_PE_FORECAST.csv\"\n",
        "OUT_TOP_PUTS = \"TOP_BUY_PUTS.csv\"\n",
        "OUT_ALERTS = \"PUT_REVERSAL_ALERTS.csv\"\n",
        "OUT_PLOT = \"SPOT_VS_MOMENTUM.png\"\n",
        "\n",
        "if not os.path.exists(INPUT_CSV):\n",
        "    print(\"ERROR: input file not found:\", INPUT_CSV)\n",
        "    sys.exit(1)\n",
        "\n",
        "# ---------- LOAD ----------\n",
        "df = pd.read_csv(INPUT_CSV, low_memory=False, parse_dates=['LTT'], infer_datetime_format=True)\n",
        "\n",
        "# ---------- USE LTP COLUMNS (explicit) ----------\n",
        "PREV_CALL_LTP = \"Previous_Call_ltp\"\n",
        "CURR_CALL_LTP = \"Current_Call_ltp\"\n",
        "NEXT_CALL_LTP = \"Next_Call_ltp\"\n",
        "\n",
        "PREV_PUT_LTP  = \"Previous_Put_ltp\"\n",
        "CURR_PUT_LTP  = \"Current_Put_ltp\"\n",
        "NEXT_PUT_LTP  = \"Next_Put_ltp\"\n",
        "\n",
        "PREV_STR = \"Previous_Strikeprice\"\n",
        "CURR_STR  = \"Current_Strikeprice\"\n",
        "NEXT_STR  = \"Next_Strikeprice\"\n",
        "\n",
        "# choose LTP (fallback to _Premium only if ltp missing)\n",
        "prev_call_col = PREV_CALL_LTP if PREV_CALL_LTP in df.columns else (\"Previous_Call_Premium\" if \"Previous_Call_Premium\" in df.columns else None)\n",
        "curr_call_col = CURR_CALL_LTP if CURR_CALL_LTP in df.columns else (\"Current_Call_Premium\"  if \"Current_Call_Premium\"  in df.columns else None)\n",
        "next_call_col = NEXT_CALL_LTP if NEXT_CALL_LTP in df.columns else (\"Next_Call_Premium\"     if \"Next_Call_Premium\"     in df.columns else None)\n",
        "\n",
        "prev_put_col  = PREV_PUT_LTP  if PREV_PUT_LTP  in df.columns else (\"Previous_Put_Premium\" if \"Previous_Put_Premium\" in df.columns else None)\n",
        "curr_put_col  = CURR_PUT_LTP  if CURR_PUT_LTP  in df.columns else (\"Current_Put_Premium\"  if \"Current_Put_Premium\"  in df.columns else None)\n",
        "next_put_col  = NEXT_PUT_LTP  if NEXT_PUT_LTP  in df.columns else (\"Next_Put_Premium\"     if \"Next_Put_Premium\"     in df.columns else None)\n",
        "\n",
        "print(\"Using columns (LTP prioritized):\")\n",
        "print(\"Calls:\", prev_call_col, curr_call_col, next_call_col)\n",
        "print(\"Puts :\", prev_put_col,  curr_put_col,  next_put_col)\n",
        "print(\"Strikes:\", PREV_STR, CURR_STR, NEXT_STR)\n",
        "\n",
        "# ---------- BUILD SERIES ----------\n",
        "def build_series(prev_col, curr_col, next_col, prev_str_col=PREV_STR, curr_str_col=CURR_STR, next_str_col=NEXT_STR):\n",
        "    series = defaultdict(list)\n",
        "    for _, row in df.iterrows():\n",
        "        t = row.get(\"LTT\")\n",
        "        if pd.isna(t):\n",
        "            continue\n",
        "        triples = [\n",
        "            (prev_str_col, prev_col),\n",
        "            (curr_str_col, curr_col),\n",
        "            (next_str_col, next_col)\n",
        "        ]\n",
        "        for sc, pc in triples:\n",
        "            if sc is None or pc is None:\n",
        "                continue\n",
        "            if sc not in df.columns or pc not in df.columns:\n",
        "                continue\n",
        "            if pd.isna(row.get(sc)) or pd.isna(row.get(pc)):\n",
        "                continue\n",
        "            try:\n",
        "                s = int(row.get(sc))\n",
        "                p = float(row.get(pc))\n",
        "            except Exception:\n",
        "                continue\n",
        "            series[s].append((t, p))\n",
        "    return series\n",
        "\n",
        "call_series = build_series(prev_call_col, curr_call_col, next_call_col)\n",
        "put_series  = build_series(prev_put_col, curr_put_col, next_put_col)\n",
        "\n",
        "print(\"Detected call strikes:\", len(call_series))\n",
        "print(\"Detected put strikes :\", len(put_series))\n",
        "\n",
        "# ---------- SUMMARIZE & FORECAST HELPERS ----------\n",
        "def summarize(pairs):\n",
        "    sr = sorted(pairs, key=lambda x: x[0])\n",
        "    ps = [p for _, p in sr]\n",
        "    if not ps:\n",
        "        return None\n",
        "    first, last = ps[0], ps[-1]\n",
        "    peak, trough = max(ps), min(ps)\n",
        "    abs_chg = last - first\n",
        "    pct_chg = (abs_chg / first * 100) if first != 0 else np.nan\n",
        "    return {\"first\": first, \"last\": last, \"peak\": peak, \"trough\": trough,\n",
        "            \"abs_change\": abs_chg, \"pct_change\": pct_chg, \"n_obs\": len(ps)}\n",
        "\n",
        "def forecast_from_pct(last, pct):\n",
        "    if pct >= 25:\n",
        "        return (last+15, last+35), (last+25, last+60)\n",
        "    elif pct >= 8:\n",
        "        return (last+6, last+18), (last+12, last+30)\n",
        "    elif pct > 0:\n",
        "        return (last+2, last+8), (last+5, last+15)\n",
        "    else:\n",
        "        return (last-5, last+2), (last-8, last+5)\n",
        "\n",
        "# ---------- BUILD CALL & PUT SUMMARIES ----------\n",
        "call_records = []\n",
        "for s, ts in call_series.items():\n",
        "    if len(ts) < 1:\n",
        "        continue\n",
        "    st = summarize(ts)\n",
        "    if st is None:\n",
        "        continue\n",
        "    f5, f10 = forecast_from_pct(st[\"last\"], st[\"pct_change\"])\n",
        "    call_records.append({\n",
        "        \"strike\": s,\n",
        "        \"n_obs_call\": st[\"n_obs\"],\n",
        "        \"first_call_ltp\": st[\"first\"],\n",
        "        \"last_call_ltp\": st[\"last\"],\n",
        "        \"peak_call_ltp\": st[\"peak\"],\n",
        "        \"trough_call_ltp\": st[\"trough\"],\n",
        "        \"abs_change_call\": st[\"abs_change\"],\n",
        "        \"pct_change_call\": st[\"pct_change\"],\n",
        "        \"call_5min_low\": f5[0], \"call_5min_high\": f5[1],\n",
        "        \"call_10min_low\": f10[0], \"call_10min_high\": f10[1],\n",
        "        \"call_p5_lo\": f5[0]-st[\"last\"], \"call_p5_hi\": f5[1]-st[\"last\"],\n",
        "        \"call_p10_lo\": f10[0]-st[\"last\"], \"call_p10_hi\": f10[1]-st[\"last\"]\n",
        "    })\n",
        "\n",
        "put_records = []\n",
        "for s, ts in put_series.items():\n",
        "    if len(ts) < 1:\n",
        "        continue\n",
        "    st = summarize(ts)\n",
        "    if st is None:\n",
        "        continue\n",
        "    f5, f10 = forecast_from_pct(st[\"last\"], st[\"pct_change\"])\n",
        "    put_records.append({\n",
        "        \"strike\": s,\n",
        "        \"n_obs_put\": st[\"n_obs\"],\n",
        "        \"first_put_ltp\": st[\"first\"],\n",
        "        \"last_put_ltp\": st[\"last\"],\n",
        "        \"peak_put_ltp\": st[\"peak\"],\n",
        "        \"trough_put_ltp\": st[\"trough\"],\n",
        "        \"abs_change_put\": st[\"abs_change\"],\n",
        "        \"pct_change_put\": st[\"pct_change\"],\n",
        "        \"put_5min_low\": f5[0], \"put_5min_high\": f5[1],\n",
        "        \"put_10min_low\": f10[0], \"put_10min_high\": f10[1],\n",
        "        \"put_p5_lo\": f5[0]-st[\"last\"], \"put_p5_hi\": f5[1]-st[\"last\"],\n",
        "        \"put_p10_lo\": f10[0]-st[\"last\"], \"put_p10_hi\": f10[1]-st[\"last\"]\n",
        "    })\n",
        "\n",
        "call_df = pd.DataFrame(call_records)\n",
        "put_df  = pd.DataFrame(put_records)\n",
        "\n",
        "# ---------- OUTER MERGE (call + put) ----------\n",
        "if not call_df.empty and not put_df.empty:\n",
        "    merged = pd.merge(call_df, put_df, on=\"strike\", how=\"outer\", sort=True)\n",
        "elif not call_df.empty:\n",
        "    merged = call_df.copy()\n",
        "    for c in [\"pct_change_put\",\"last_put_ltp\",\"put_moneyflow\",\"n_obs_put\",\"first_put_ltp\"]:\n",
        "        if c not in merged.columns:\n",
        "            merged[c] = np.nan\n",
        "elif not put_df.empty:\n",
        "    merged = put_df.copy()\n",
        "    for c in [\"pct_change_call\",\"last_call_ltp\",\"call_moneyflow\",\"n_obs_call\",\"first_call_ltp\"]:\n",
        "        if c not in merged.columns:\n",
        "            merged[c] = np.nan\n",
        "else:\n",
        "    merged = pd.DataFrame(columns=[\"strike\"])\n",
        "\n",
        "# ---------- EXTRACT TAGS & MONEYFLOW ----------\n",
        "tag_cols = [\"Previous_StrategyTag\",\"Current_StrategyTag\",\"Next_StrategyTag\"]\n",
        "call_money_cols = [\"Previous_CallMoneyFlow\",\"Current_CallMoneyFlow\",\"Next_CallMoneyFlow\",\n",
        "                   \"Previous_TotalcallMoneyFlow\",\"Current_TotalcallMoneyFlow\",\"Next_TotalcallMoneyFlow\",\n",
        "                   \"Previous_CallMoneyFlow\",\"Current_CallMoneyFlow\",\"Next_CallMoneyFlow\"]\n",
        "put_money_cols  = [\"Previous_PutMoneyFlow\",\"Current_PutMoneyFlow\",\"Next_PutMoneyFlow\",\n",
        "                   \"Previous_TotalputMoneyFlow\",\"Current_TotalputMoneyFlow\",\"Next_TotalputMoneyFlow\",\n",
        "                   \"Previous_PutMoneyFlow\",\"Current_PutMoneyFlow\",\"Next_PutMoneyFlow\"]\n",
        "\n",
        "# initialize strike_info\n",
        "strike_info = {}\n",
        "for s in merged[\"strike\"].dropna().astype(int).unique():\n",
        "    strike_info[int(s)] = {\"tags\":Counter(), \"call_moneyflow\":0.0, \"put_moneyflow\":0.0}\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    for sc in (PREV_STR, CURR_STR, NEXT_STR):\n",
        "        if sc not in df.columns:\n",
        "            continue\n",
        "        if pd.isna(row.get(sc)):\n",
        "            continue\n",
        "        try:\n",
        "            s = int(row.get(sc))\n",
        "        except:\n",
        "            continue\n",
        "        if s not in strike_info:\n",
        "            continue\n",
        "        # tags\n",
        "        for col in tag_cols:\n",
        "            if col in df.columns:\n",
        "                v = row.get(col)\n",
        "                if isinstance(v, str) and v.strip():\n",
        "                    # split common separators\n",
        "                    for token in [t.strip() for t in v.replace(\"|\",\";\").split(\";\") if t.strip()]:\n",
        "                        strike_info[s][\"tags\"][token] += 1\n",
        "        # moneyflows\n",
        "        for col in call_money_cols:\n",
        "            if col in df.columns:\n",
        "                try:\n",
        "                    val = row.get(col)\n",
        "                    if not pd.isna(val):\n",
        "                        strike_info[s][\"call_moneyflow\"] += float(val)\n",
        "                except:\n",
        "                    pass\n",
        "        for col in put_money_cols:\n",
        "            if col in df.columns:\n",
        "                try:\n",
        "                    val = row.get(col)\n",
        "                    if not pd.isna(val):\n",
        "                        strike_info[s][\"put_moneyflow\"] += float(val)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "# attach tags/moneyflow to merged\n",
        "def fmt_tags(s):\n",
        "    t = strike_info.get(int(s), {}).get(\"tags\", {})\n",
        "    return \";\".join([f\"{k}:{v}\" for k,v in t.items()]) if t else \"\"\n",
        "\n",
        "merged[\"tags\"] = merged[\"strike\"].apply(lambda s: fmt_tags(s) if not pd.isna(s) else \"\")\n",
        "merged[\"call_moneyflow\"] = merged[\"strike\"].apply(lambda s: strike_info.get(int(s), {}).get(\"call_moneyflow\", 0.0))\n",
        "merged[\"put_moneyflow\"]  = merged[\"strike\"].apply(lambda s: strike_info.get(int(s), {}).get(\"put_moneyflow\", 0.0))\n",
        "\n",
        "# ---------- DECIDE RECOMMENDED ACTION (dual-side logic) ----------\n",
        "def decide_action(row):\n",
        "    call_pct = row.get(\"pct_change_call\") if \"pct_change_call\" in row else (row.get(\"pct_change_call\") if \"pct_change_call\" in row.index else 0)\n",
        "    put_pct  = row.get(\"pct_change_put\")  if \"pct_change_put\" in row else (row.get(\"pct_change_put\") if \"pct_change_put\" in row.index else 0)\n",
        "    call_pct = 0 if pd.isna(call_pct) else call_pct\n",
        "    put_pct  = 0 if pd.isna(put_pct) else put_pct\n",
        "\n",
        "    tags = (row.get(\"tags\") or \"\").lower()\n",
        "    bear_boost = (\"put buying\" in tags) or (\"call writing\" in tags) or (\"bearish\" in tags)\n",
        "    bull_boost = (\"call buying\" in tags) or (\"bullish\" in tags) or (\"oi_support_call\" in tags)\n",
        "\n",
        "    if (put_pct > 8) or (put_pct > 5 and bear_boost):\n",
        "        return \"BUY_PUT\"\n",
        "    if (call_pct > 8) or (call_pct > 5 and bull_boost):\n",
        "        return \"BUY_CALL\"\n",
        "    return \"HOLD\"\n",
        "\n",
        "merged[\"recommended_action\"] = merged.apply(decide_action, axis=1)\n",
        "\n",
        "# ---------- ENRICH: unified summary columns + reasons + high-conviction stats ----------\n",
        "final_rows = []\n",
        "for _, r in merged.iterrows():\n",
        "    s = int(r[\"strike\"])\n",
        "    # Unnamed: 0 (if present) - pick first matching row sample\n",
        "    sample = df[\n",
        "        ((df.get(\"Current_Strikeprice\") == s) if \"Current_Strikeprice\" in df.columns else False) |\n",
        "        ((df.get(\"Previous_Strikeprice\") == s) if \"Previous_Strikeprice\" in df.columns else False) |\n",
        "        ((df.get(\"Next_Strikeprice\") == s) if \"Next_Strikeprice\" in df.columns else False)\n",
        "    ]\n",
        "    unnamed_val = sample[\"Unnamed: 0\"].iloc[0] if (\"Unnamed: 0\" in df.columns and not sample.empty) else np.nan\n",
        "\n",
        "    # n_obs (prefer aggregated call+put counts)\n",
        "    n_obs_call = int(r.get(\"n_obs_call\")) if pd.notna(r.get(\"n_obs_call\")) else 0\n",
        "    n_obs_put  = int(r.get(\"n_obs_put\"))  if pd.notna(r.get(\"n_obs_put\"))  else 0\n",
        "    n_obs = n_obs_call + n_obs_put\n",
        "\n",
        "    # Choose preferred premium fields: prefer CALL summary, else PUT\n",
        "    def choose(field_call, field_put):\n",
        "        if field_call in r and pd.notna(r.get(field_call)):\n",
        "            return r.get(field_call)\n",
        "        if field_put in r and pd.notna(r.get(field_put)):\n",
        "            return r.get(field_put)\n",
        "        return np.nan\n",
        "\n",
        "    first_premium = choose(\"first_call_ltp\", \"first_put_ltp\")\n",
        "    last_premium  = choose(\"last_call_ltp\", \"last_put_ltp\")\n",
        "    peak_premium  = choose(\"peak_call_ltp\", \"peak_put_ltp\")\n",
        "    trough_premium= choose(\"trough_call_ltp\", \"trough_put_ltp\")\n",
        "    abs_change    = choose(\"abs_change_call\", \"abs_change_put\")\n",
        "    pct_change    = choose(\"pct_change_call\", \"pct_change_put\")\n",
        "\n",
        "    # 5/10min forecast unified\n",
        "    five_low  = choose(\"call_5min_low\", \"put_5min_low\")\n",
        "    five_high = choose(\"call_5min_high\", \"put_5min_high\")\n",
        "    ten_low   = choose(\"call_10min_low\", \"put_10min_low\")\n",
        "    ten_high  = choose(\"call_10min_high\", \"put_10min_high\")\n",
        "\n",
        "    # expected deltas\n",
        "    p5_expected_lo = (five_low - last_premium) if (pd.notna(five_low) and pd.notna(last_premium)) else np.nan\n",
        "    p5_expected_hi = (five_high - last_premium) if (pd.notna(five_high) and pd.notna(last_premium)) else np.nan\n",
        "    p10_expected_lo = (ten_low - last_premium) if (pd.notna(ten_low) and pd.notna(last_premium)) else np.nan\n",
        "    p10_expected_hi = (ten_high - last_premium) if (pd.notna(ten_high) and pd.notna(last_premium)) else np.nan\n",
        "\n",
        "    # reasons: build from tags + pct change + moneyflow\n",
        "    tags_text = r.get(\"tags\",\"\") or \"\"\n",
        "    reasons = []\n",
        "    if \"RSI\" in tags_text or \"Rsi\" in tags_text or \"rsi\" in tags_text:\n",
        "        reasons.append(\"RSI momentum\")\n",
        "    if \"MACD\" in tags_text or \"macd\" in tags_text:\n",
        "        reasons.append(\"MACD confirmation\")\n",
        "    if \"VWAP\" in tags_text or \"vwap\" in tags_text:\n",
        "        reasons.append(\"VWAP divergence\")\n",
        "    if \"OI\" in tags_text or \"oi\" in tags_text:\n",
        "        reasons.append(\"OI support/resistance\")\n",
        "    # moneyflow hints\n",
        "    call_mf = r.get(\"call_moneyflow\", 0.0) if \"call_moneyflow\" in r else 0.0\n",
        "    put_mf  = r.get(\"put_moneyflow\", 0.0)  if \"put_moneyflow\" in r else 0.0\n",
        "    try:\n",
        "        if float(call_mf) > 0:\n",
        "            reasons.append(\"Call net buying\")\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        if float(put_mf) > 0:\n",
        "            reasons.append(\"Put net buying\")\n",
        "    except:\n",
        "        pass\n",
        "    # pct-based reason\n",
        "    try:\n",
        "        if pd.notna(pct_change):\n",
        "            if pct_change > 10:\n",
        "                reasons.append(\"Strong premium move\")\n",
        "            elif pct_change > 3:\n",
        "                reasons.append(\"Moderate premium move\")\n",
        "    except:\n",
        "        pass\n",
        "    if not reasons:\n",
        "        reasons.append(\"No strong signals\")\n",
        "\n",
        "    reasons_txt = \"; \".join(dict.fromkeys(reasons))  # dedupe preserving order\n",
        "\n",
        "    # high-conviction stats (use Current_IsHighConvictionSignal column if present)\n",
        "    hc_col = \"Current_IsHighConvictionSignal\"\n",
        "    hc_total = 0\n",
        "    hc_success = 0\n",
        "    if hc_col in df.columns:\n",
        "        hc_rows = df[(df.get(\"Current_Strikeprice\")==s) & (df.get(hc_col)==True)]\n",
        "        hc_total = int(hc_rows.shape[0])\n",
        "        for _, hrow in hc_rows.iterrows():\n",
        "            t0 = hrow.get(\"LTT\")\n",
        "            # pick which premium to measure based on recommended_action for this strike\n",
        "            action = r.get(\"recommended_action\",\"HOLD\")\n",
        "            p0 = None\n",
        "            if action == \"BUY_CALL\" and curr_call_col in df.columns:\n",
        "                p0 = hrow.get(curr_call_col)\n",
        "            elif action == \"BUY_PUT\" and curr_put_col in df.columns:\n",
        "                p0 = hrow.get(curr_put_col)\n",
        "            else:\n",
        "                # fallback to call premium if exists, else put\n",
        "                p0 = hrow.get(curr_call_col) if curr_call_col in df.columns else hrow.get(curr_put_col) if curr_put_col in df.columns else None\n",
        "\n",
        "            if pd.isna(t0) or p0 is None or pd.isna(p0):\n",
        "                continue\n",
        "            window = df[(df[\"LTT\"] >= t0) & (df[\"LTT\"] <= t0 + timedelta(minutes=3)) & ((df.get(\"Current_Strikeprice\")==s) if \"Current_Strikeprice\" in df.columns else False)]\n",
        "            if window.empty:\n",
        "                continue\n",
        "            # success = premium increased in the next 3 minutes (for both call & put we want premium ↑ for success)\n",
        "            # use the same column we selected for p0\n",
        "            if action == \"BUY_PUT\" and curr_put_col in df.columns:\n",
        "                try:\n",
        "                    if window[curr_put_col].max() > p0:\n",
        "                        hc_success += 1\n",
        "                except:\n",
        "                    pass\n",
        "            else:\n",
        "                # assume call\n",
        "                if curr_call_col in df.columns:\n",
        "                    try:\n",
        "                        if window[curr_call_col].max() > p0:\n",
        "                            hc_success += 1\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "    hc_rate = (hc_success / hc_total) if hc_total > 0 else None\n",
        "\n",
        "    final_rows.append({\n",
        "        # unified fields (user requested)\n",
        "        \"strike\": s,\n",
        "        \"Unnamed: 0\": unnamed_val,\n",
        "        \"n_obs\": n_obs,\n",
        "        \"first_premium\": first_premium,\n",
        "        \"last_premium\": last_premium,\n",
        "        \"peak_premium\": peak_premium,\n",
        "        \"trough_premium\": trough_premium,\n",
        "        \"abs_change\": abs_change,\n",
        "        \"pct_change\": pct_change,\n",
        "        \"5min_low\": five_low,\n",
        "        \"5min_high\": five_high,\n",
        "        \"10min_low\": ten_low,\n",
        "        \"10min_high\": ten_high,\n",
        "        \"p5_expected_lo\": p5_expected_lo,\n",
        "        \"p5_expected_hi\": p5_expected_hi,\n",
        "        \"p10_expected_lo\": p10_expected_lo,\n",
        "        \"p10_expected_hi\": p10_expected_hi,\n",
        "\n",
        "        # moneyflow & tags + reasons + recommended action\n",
        "        \"call_moneyflow\": r.get(\"call_moneyflow\", 0.0),\n",
        "        \"put_moneyflow\":  r.get(\"put_moneyflow\", 0.0),\n",
        "        \"tags\": r.get(\"tags\",\"\"),\n",
        "        \"reasons\": reasons_txt,\n",
        "        \"recommended_action\": r.get(\"recommended_action\",\"HOLD\"),\n",
        "\n",
        "        # high conviction\n",
        "        \"highconv_total\": hc_total,\n",
        "        \"highconv_success\": hc_success,\n",
        "        \"highconv_hit_rate\": hc_rate,\n",
        "\n",
        "        # keep reference original fields (helpful)\n",
        "        \"Current_Strikeprice\": s\n",
        "    })\n",
        "\n",
        "final_df = pd.DataFrame(final_rows)\n",
        "\n",
        "# preserve original merged columns as well (optional): merge back the merged DF by strike to keep call/put detail\n",
        "final_df = final_df.merge(merged.add_prefix(\"merged_\"), left_on=\"strike\", right_on=\"merged_strike\", how=\"left\")\n",
        "\n",
        "# ---------- SAVE CSVs ----------\n",
        "final_df.to_csv(OUT_MERGED, index=False)\n",
        "top_buy_puts = merged[merged[\"recommended_action\"]==\"BUY_PUT\"].copy()\n",
        "if \"pct_change_put\" in top_buy_puts.columns:\n",
        "    top_buy_puts = top_buy_puts.sort_values(\"pct_change_put\", ascending=False).head(200)\n",
        "else:\n",
        "    top_buy_puts = top_buy_puts.head(200)\n",
        "top_buy_puts.to_csv(OUT_TOP_PUTS, index=False)\n",
        "print(\"Saved:\", OUT_MERGED, OUT_TOP_PUTS)\n",
        "\n",
        "# ---------- SPOT vs momentum plot ----------\n",
        "last_spot = df[\"SpotPrice\"].dropna().iloc[-1] if \"SpotPrice\" in df.columns and not df[\"SpotPrice\"].dropna().empty else np.nan\n",
        "avg_call_pct = merged[\"pct_change_call\"].replace([np.inf, -np.inf], np.nan).dropna().mean() if \"pct_change_call\" in merged.columns else np.nan\n",
        "avg_put_pct  = merged[\"pct_change_put\"].replace([np.inf, -np.inf], np.nan).dropna().mean()  if \"pct_change_put\" in merged.columns  else np.nan\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.title(\"Spot (last) vs Avg CE/PE %change\")\n",
        "vals = [last_spot if not math.isnan(last_spot) else 0, avg_call_pct if not pd.isna(avg_call_pct) else 0, avg_put_pct if not pd.isna(avg_put_pct) else 0]\n",
        "plt.bar([\"Spot Last\",\"Avg CE %\",\"Avg PE %\"], vals)\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUT_PLOT, dpi=150)\n",
        "plt.close()\n",
        "print(\"Saved:\", OUT_PLOT)\n",
        "\n",
        "# ---------- ALERTS: PUT REVERSAL CANDIDATES ----------\n",
        "alerts = []\n",
        "for _, r in merged.iterrows():\n",
        "    pct_put = r.get(\"pct_change_put\")\n",
        "    if pct_put is None or (isinstance(pct_put, float) and np.isnan(pct_put)):\n",
        "        continue\n",
        "    tags = (r.get(\"tags\") or \"\").lower()\n",
        "    put_money = r.get(\"put_moneyflow\", 0.0) if \"put_moneyflow\" in r else 0.0\n",
        "    if pct_put > 8 and ((\"put buying\" in tags) or (\"bearish\" in tags) or (put_money and put_money > 0)):\n",
        "        alerts.append({\n",
        "            \"strike\": int(r[\"strike\"]),\n",
        "            \"pct_change_put\": pct_put,\n",
        "            \"put_moneyflow\": put_money,\n",
        "            \"tags\": r.get(\"tags\",\"\"),\n",
        "            \"recommended_action\": r.get(\"recommended_action\",\"\"),\n",
        "            \"reason\": \"Put momentum >8% and bearish tag/moneyflow\"\n",
        "        })\n",
        "\n",
        "alerts_df = pd.DataFrame(alerts)\n",
        "if not alerts_df.empty:\n",
        "    alerts_df.to_csv(OUT_ALERTS, index=False)\n",
        "    print(\"Saved:\", OUT_ALERTS)\n",
        "    print(\"Sample alerts:\\n\", alerts_df.head(20).to_string(index=False))\n",
        "else:\n",
        "    print(\"No put reversal alerts found.\")\n",
        "\n",
        "print(\"Pipeline complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNoQK7Ju_k7l",
        "outputId": "f9928190-6d45-49d1-decf-2d0f96c9bd30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-915387648.py:22: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  df = pd.read_csv(INPUT_CSV, low_memory=False, parse_dates=['LTT'], infer_datetime_format=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using columns (LTP prioritized):\n",
            "Calls: Previous_Call_ltp Current_Call_ltp Next_Call_ltp\n",
            "Puts : Previous_Put_ltp Current_Put_ltp Next_Put_ltp\n",
            "Strikes: Previous_Strikeprice Current_Strikeprice Next_Strikeprice\n",
            "Detected call strikes: 8\n",
            "Detected put strikes : 8\n",
            "Saved: MERGED_CE_PE_FORECAST.csv TOP_BUY_PUTS.csv\n",
            "Saved: SPOT_VS_MOMENTUM.png\n",
            "Saved: PUT_REVERSAL_ALERTS.csv\n",
            "Sample alerts:\n",
            "  strike  pct_change_put  put_moneyflow                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 tags recommended_action                                     reason\n",
            "  58500        9.674843   1.429351e+13 RSI_MACD_Bull VWAP_Divergence PnL_Explosion:6182;RSI_MACD_Bull VWAP_Divergence PnL_Explosion OI_Support_Call:2869;VWAP_Divergence PnL_Explosion:17872;VWAP_Divergence PnL_Explosion OI_Support_Call:9097;RSI_MACD_Bear VWAP_Divergence PnL_Explosion OI_Support_Put:3170;RSI_MACD_Bear VWAP_Divergence PnL_Explosion OI_Support_Call:2407;DeltaNeutral:1540;LowRealHighIV:1351;DeltaNeutral LowRealHighIV:593;VWAP_Divergence PnL_Explosion Prem_Momentum- OI_Support_Call:6;RSI_MACD_Bear VWAP_Divergence PnL_Explosion:2461;RSI_MACD_Bear VWAP_Divergence PnL_Explosion Prem_Momentum+ OI_Support_Call:34;RSI_MACD_Bear VWAP_Divergence PnL_Explosion Prem_Momentum+ OI_Support_Put:38;VWAP_Divergence PnL_Explosion Prem_Momentum+ OI_Support_Call:155;RSI_MACD_Bull VWAP_Divergence PnL_Explosion Prem_Momentum+ OI_Support_Call:51;VWAP_Divergence PnL_Explosion Prem_Momentum+:144;RSI_MACD_Bull VWAP_Divergence PnL_Explosion Prem_Momentum- OI_Support_Call:1;RSI_MACD_Bear VWAP_Divergence PnL_Explosion Prem_Momentum-:3;RSI_MACD_Bull VWAP_Divergence PnL_Explosion Prem_Momentum+:84;RSI_MACD_Bear VWAP_Divergence PnL_Explosion Prem_Momentum+:7;RSI_MACD_Bear VWAP_Divergence PnL_Explosion Prem_Momentum- OI_Support_Call:3;RSI_MACD_Bull VWAP_Divergence PnL_Explosion Prem_Momentum-:5;VWAP_Divergence PnL_Explosion Prem_Momentum-:12;RSI_MACD_Bear VWAP_Divergence PnL_Explosion Prem_Momentum- OI_Support_Put:14            BUY_PUT Put momentum >8% and bearish tag/moneyflow\n",
            "Pipeline complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# full_ce_pe_pipeline_use_ltp_enriched_plus_extras.py\n",
        "import os, sys, math, json\n",
        "from collections import defaultdict, Counter\n",
        "from datetime import timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "INPUT_CSV = \"flattened_snapshots.csv\"   # your uploaded file\n",
        "OUT_MERGED = \"MERGED_CE_PE_FORECAST.csv\"\n",
        "OUT_TOP_PUTS = \"TOP_BUY_PUTS.csv\"\n",
        "OUT_TOP_CALLS = \"TOP_BUY_CALLS.csv\"\n",
        "OUT_ALL_ACTIONS = \"ALL_ACTIONS.csv\"\n",
        "OUT_TOP_HOLD = \"TOP_HOLD_BREAKOUTS.csv\"\n",
        "OUT_CE_ONLY = \"TOP_CE_ONLY.csv\"\n",
        "OUT_PE_ONLY = \"TOP_PE_ONLY.csv\"\n",
        "OUT_ALERTS = \"PUT_REVERSAL_ALERTS.csv\"\n",
        "OUT_PLOT = \"SPOT_VS_MOMENTUM.png\"\n",
        "\n",
        "OUT_REVERSALS = \"TOP_REVERSALS.csv\"\n",
        "OUT_IV_CRUSH = \"IV_CRUSH.csv\"\n",
        "OUT_HEATMAP = \"STRENGTH_HEATMAP.png\"\n",
        "OUT_AUTOTRADE = \"AUTO_TRADES.json\"\n",
        "\n",
        "# thresholds (tweakable)\n",
        "REVERSAL_DROP_PCT = 8.0          # reversal if premium drops this % from recent peak\n",
        "REVERSAL_WINDOW_MIN = 6          # minutes to lookback for peak\n",
        "IV_CRUSH_ABS = 5.0               # absolute IV drop considered crush (percentage points)\n",
        "IV_CRUSH_REL_PCT = 15.0          # relative IV drop percent\n",
        "AUTO_TRADE_STRENGTH = 8.0        # minimum strength_score to export auto-trade\n",
        "DEFAULT_LOTSIZE = 25\n",
        "\n",
        "if not os.path.exists(INPUT_CSV):\n",
        "    print(\"ERROR: input file not found:\", INPUT_CSV)\n",
        "    sys.exit(1)\n",
        "\n",
        "# ---------- LOAD ----------\n",
        "df = pd.read_csv(INPUT_CSV, low_memory=False, parse_dates=['LTT'], infer_datetime_format=True)\n",
        "\n",
        "# ---------- USE LTP COLUMNS (explicit) ----------\n",
        "PREV_CALL_LTP = \"Previous_Call_ltp\"\n",
        "CURR_CALL_LTP = \"Current_Call_ltp\"\n",
        "NEXT_CALL_LTP = \"Next_Call_ltp\"\n",
        "\n",
        "PREV_PUT_LTP  = \"Previous_Put_ltp\"\n",
        "CURR_PUT_LTP  = \"Current_Put_ltp\"\n",
        "NEXT_PUT_LTP  = \"Next_Put_ltp\"\n",
        "\n",
        "PREV_STR = \"Previous_Strikeprice\"\n",
        "CURR_STR  = \"Current_Strikeprice\"\n",
        "NEXT_STR  = \"Next_Strikeprice\"\n",
        "\n",
        "# choose LTP (fallback to _Premium only if ltp missing)\n",
        "prev_call_col = PREV_CALL_LTP if PREV_CALL_LTP in df.columns else (\"Previous_Call_Premium\" if \"Previous_Call_Premium\" in df.columns else None)\n",
        "curr_call_col = CURR_CALL_LTP if CURR_CALL_LTP in df.columns else (\"Current_Call_Premium\"  if \"Current_Call_Premium\"  in df.columns else None)\n",
        "next_call_col = NEXT_CALL_LTP if NEXT_CALL_LTP in df.columns else (\"Next_Call_Premium\"     if \"Next_Call_Premium\"     in df.columns else None)\n",
        "\n",
        "prev_put_col  = PREV_PUT_LTP  if PREV_PUT_LTP  in df.columns else (\"Previous_Put_Premium\" if \"Previous_Put_Premium\" in df.columns else None)\n",
        "curr_put_col  = CURR_PUT_LTP  if CURR_PUT_LTP  in df.columns else (\"Current_Put_Premium\"  if \"Current_Put_Premium\"  in df.columns else None)\n",
        "next_put_col  = NEXT_PUT_LTP  if NEXT_PUT_LTP  in df.columns else (\"Next_Put_Premium\"     if \"Next_Put_Premium\"     in df.columns else None)\n",
        "\n",
        "print(\"Using columns (LTP prioritized):\")\n",
        "print(\"Calls:\", prev_call_col, curr_call_col, next_call_col)\n",
        "print(\"Puts :\", prev_put_col,  curr_put_col,  next_put_col)\n",
        "print(\"Strikes:\", PREV_STR, CURR_STR, NEXT_STR)\n",
        "\n",
        "# ---------- BUILD SERIES ----------\n",
        "def build_series(prev_col, curr_col, next_col, prev_str_col=PREV_STR, curr_str_col=CURR_STR, next_str_col=NEXT_STR):\n",
        "    series = defaultdict(list)\n",
        "    for _, row in df.iterrows():\n",
        "        t = row.get(\"LTT\")\n",
        "        if pd.isna(t):\n",
        "            continue\n",
        "        triples = [\n",
        "            (prev_str_col, prev_col),\n",
        "            (curr_str_col, curr_col),\n",
        "            (next_str_col, next_col)\n",
        "        ]\n",
        "        for sc, pc in triples:\n",
        "            if sc is None or pc is None:\n",
        "                continue\n",
        "            if sc not in df.columns or pc not in df.columns:\n",
        "                continue\n",
        "            if pd.isna(row.get(sc)) or pd.isna(row.get(pc)):\n",
        "                continue\n",
        "            try:\n",
        "                s = int(row.get(sc))\n",
        "                p = float(row.get(pc))\n",
        "            except Exception:\n",
        "                continue\n",
        "            series[s].append((t, p))\n",
        "    return series\n",
        "\n",
        "call_series = build_series(prev_call_col, curr_call_col, next_call_col)\n",
        "put_series  = build_series(prev_put_col, curr_put_col, next_put_col)\n",
        "\n",
        "print(\"Detected call strikes:\", len(call_series))\n",
        "print(\"Detected put strikes :\", len(put_series))\n",
        "\n",
        "# ---------- SUMMARIZE & FORECAST HELPERS ----------\n",
        "def summarize(pairs):\n",
        "    sr = sorted(pairs, key=lambda x: x[0])\n",
        "    ps = [p for _, p in sr]\n",
        "    if not ps:\n",
        "        return None\n",
        "    first, last = ps[0], ps[-1]\n",
        "    peak, trough = max(ps), min(ps)\n",
        "    abs_chg = last - first\n",
        "    pct_chg = (abs_chg / first * 100) if first != 0 else np.nan\n",
        "    return {\"first\": first, \"last\": last, \"peak\": peak, \"trough\": trough,\n",
        "            \"abs_change\": abs_chg, \"pct_change\": pct_chg, \"n_obs\": len(ps), \"series_sorted\": sr}\n",
        "\n",
        "def forecast_from_pct(last, pct):\n",
        "    if pct >= 25:\n",
        "        return (last+15, last+35), (last+25, last+60)\n",
        "    elif pct >= 8:\n",
        "        return (last+6, last+18), (last+12, last+30)\n",
        "    elif pct > 0:\n",
        "        return (last+2, last+8), (last+5, last+15)\n",
        "    else:\n",
        "        return (last-5, last+2), (last-8, last+5)\n",
        "\n",
        "# ---------- BUILD CALL & PUT SUMMARIES ----------\n",
        "call_records = []\n",
        "for s, ts in call_series.items():\n",
        "    if len(ts) < 1:\n",
        "        continue\n",
        "    st = summarize(ts)\n",
        "    if st is None:\n",
        "        continue\n",
        "    f5, f10 = forecast_from_pct(st[\"last\"], st[\"pct_change\"])\n",
        "    call_records.append({\n",
        "        \"strike\": s,\n",
        "        \"n_obs_call\": st[\"n_obs\"],\n",
        "        \"first_call_ltp\": st[\"first\"],\n",
        "        \"last_call_ltp\": st[\"last\"],\n",
        "        \"peak_call_ltp\": st[\"peak\"],\n",
        "        \"trough_call_ltp\": st[\"trough\"],\n",
        "        \"abs_change_call\": st[\"abs_change\"],\n",
        "        \"pct_change_call\": st[\"pct_change\"],\n",
        "        \"call_5min_low\": f5[0], \"call_5min_high\": f5[1],\n",
        "        \"call_10min_low\": f10[0], \"call_10min_high\": f10[1],\n",
        "        \"call_p5_lo\": f5[0]-st[\"last\"], \"call_p5_hi\": f5[1]-st[\"last\"],\n",
        "        \"call_p10_lo\": f10[0]-st[\"last\"], \"call_p10_hi\": f10[1]-st[\"last\"],\n",
        "        \"call_series_sorted\": st[\"series_sorted\"]\n",
        "    })\n",
        "\n",
        "put_records = []\n",
        "for s, ts in put_series.items():\n",
        "    if len(ts) < 1:\n",
        "        continue\n",
        "    st = summarize(ts)\n",
        "    if st is None:\n",
        "        continue\n",
        "    f5, f10 = forecast_from_pct(st[\"last\"], st[\"pct_change\"])\n",
        "    put_records.append({\n",
        "        \"strike\": s,\n",
        "        \"n_obs_put\": st[\"n_obs\"],\n",
        "        \"first_put_ltp\": st[\"first\"],\n",
        "        \"last_put_ltp\": st[\"last\"],\n",
        "        \"peak_put_ltp\": st[\"peak\"],\n",
        "        \"trough_put_ltp\": st[\"trough\"],\n",
        "        \"abs_change_put\": st[\"abs_change\"],\n",
        "        \"pct_change_put\": st[\"pct_change\"],\n",
        "        \"put_5min_low\": f5[0], \"put_5min_high\": f5[1],\n",
        "        \"put_10min_low\": f10[0], \"put_10min_high\": f10[1],\n",
        "        \"put_p5_lo\": f5[0]-st[\"last\"], \"put_p5_hi\": f5[1]-st[\"last\"],\n",
        "        \"put_p10_lo\": f10[0]-st[\"last\"], \"put_p10_hi\": f10[1]-st[\"last\"],\n",
        "        \"put_series_sorted\": st[\"series_sorted\"]\n",
        "    })\n",
        "\n",
        "call_df = pd.DataFrame(call_records)\n",
        "put_df  = pd.DataFrame(put_records)\n",
        "\n",
        "# ---------- OUTER MERGE (call + put) ----------\n",
        "if not call_df.empty and not put_df.empty:\n",
        "    merged = pd.merge(call_df, put_df, on=\"strike\", how=\"outer\", sort=True)\n",
        "elif not call_df.empty:\n",
        "    merged = call_df.copy()\n",
        "    for c in [\"pct_change_put\",\"last_put_ltp\",\"put_moneyflow\",\"n_obs_put\",\"first_put_ltp\"]:\n",
        "        if c not in merged.columns:\n",
        "            merged[c] = np.nan\n",
        "elif not put_df.empty:\n",
        "    merged = put_df.copy()\n",
        "    for c in [\"pct_change_call\",\"last_call_ltp\",\"call_moneyflow\",\"n_obs_call\",\"first_call_ltp\"]:\n",
        "        if c not in merged.columns:\n",
        "            merged[c] = np.nan\n",
        "else:\n",
        "    merged = pd.DataFrame(columns=[\"strike\"])\n",
        "\n",
        "# ---------- EXTRACT TAGS & MONEYFLOW ----------\n",
        "tag_cols = [\"Previous_StrategyTag\",\"Current_StrategyTag\",\"Next_StrategyTag\"]\n",
        "call_money_cols = [\"Previous_CallMoneyFlow\",\"Current_CallMoneyFlow\",\"Next_CallMoneyFlow\",\n",
        "                   \"Previous_TotalcallMoneyFlow\",\"Current_TotalcallMoneyFlow\",\"Next_TotalcallMoneyFlow\"]\n",
        "put_money_cols  = [\"Previous_PutMoneyFlow\",\"Current_PutMoneyFlow\",\"Next_PutMoneyFlow\",\n",
        "                   \"Previous_TotalputMoneyFlow\",\"Current_TotalputMoneyFlow\",\"Next_TotalputMoneyFlow\"]\n",
        "\n",
        "# initialize strike_info\n",
        "strike_info = {}\n",
        "for s in merged[\"strike\"].dropna().astype(int).unique():\n",
        "    strike_info[int(s)] = {\"tags\":Counter(), \"call_moneyflow\":0.0, \"put_moneyflow\":0.0}\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    for sc in (PREV_STR, CURR_STR, NEXT_STR):\n",
        "        if sc not in df.columns:\n",
        "            continue\n",
        "        if pd.isna(row.get(sc)):\n",
        "            continue\n",
        "        try:\n",
        "            s = int(row.get(sc))\n",
        "        except:\n",
        "            continue\n",
        "        if s not in strike_info:\n",
        "            continue\n",
        "        # tags\n",
        "        for col in tag_cols:\n",
        "            if col in df.columns:\n",
        "                v = row.get(col)\n",
        "                if isinstance(v, str) and v.strip():\n",
        "                    for token in [t.strip() for t in v.replace(\"|\",\";\").split(\";\") if t.strip()]:\n",
        "                        strike_info[s][\"tags\"][token] += 1\n",
        "        # moneyflows\n",
        "        for col in call_money_cols:\n",
        "            if col in df.columns:\n",
        "                try:\n",
        "                    val = row.get(col)\n",
        "                    if not pd.isna(val):\n",
        "                        strike_info[s][\"call_moneyflow\"] += float(val)\n",
        "                except:\n",
        "                    pass\n",
        "        for col in put_money_cols:\n",
        "            if col in df.columns:\n",
        "                try:\n",
        "                    val = row.get(col)\n",
        "                    if not pd.isna(val):\n",
        "                        strike_info[s][\"put_moneyflow\"] += float(val)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "# attach tags/moneyflow to merged\n",
        "def fmt_tags(s):\n",
        "    t = strike_info.get(int(s), {}).get(\"tags\", {})\n",
        "    return \";\".join([f\"{k}:{v}\" for k,v in t.items()]) if t else \"\"\n",
        "\n",
        "merged[\"tags\"] = merged[\"strike\"].apply(lambda s: fmt_tags(s) if not pd.isna(s) else \"\")\n",
        "merged[\"call_moneyflow\"] = merged[\"strike\"].apply(lambda s: strike_info.get(int(s), {}).get(\"call_moneyflow\", 0.0))\n",
        "merged[\"put_moneyflow\"]  = merged[\"strike\"].apply(lambda s: strike_info.get(int(s), {}).get(\"put_moneyflow\", 0.0))\n",
        "\n",
        "# ---------- DECIDE RECOMMENDED ACTION (dual-side logic) ----------\n",
        "def decide_action(row):\n",
        "    call_pct = row.get(\"pct_change_call\", 0)\n",
        "    put_pct  = row.get(\"pct_change_put\", 0)\n",
        "    call_pct = 0 if pd.isna(call_pct) else call_pct\n",
        "    put_pct  = 0 if pd.isna(put_pct) else put_pct\n",
        "\n",
        "    tags = (row.get(\"tags\") or \"\").lower()\n",
        "    bear_boost = (\"put buying\" in tags) or (\"call writing\" in tags) or (\"bearish\" in tags)\n",
        "    bull_boost = (\"call buying\" in tags) or (\"bullish\" in tags) or (\"oi_support_call\" in tags)\n",
        "\n",
        "    if (put_pct > 8) or (put_pct > 5 and bear_boost):\n",
        "        return \"BUY_PUT\"\n",
        "    if (call_pct > 8) or (call_pct > 5 and bull_boost):\n",
        "        return \"BUY_CALL\"\n",
        "    return \"HOLD\"\n",
        "\n",
        "merged[\"recommended_action\"] = merged.apply(decide_action, axis=1)\n",
        "\n",
        "# ---------- ENRICH: unified summary columns + reasons + high-conviction stats ----------\n",
        "final_rows = []\n",
        "for _, r in merged.iterrows():\n",
        "    s = int(r[\"strike\"])\n",
        "    sample = df[\n",
        "        ((df.get(\"Current_Strikeprice\") == s) if \"Current_Strikeprice\" in df.columns else False) |\n",
        "        ((df.get(\"Previous_Strikeprice\") == s) if \"Previous_Strikeprice\" in df.columns else False) |\n",
        "        ((df.get(\"Next_Strikeprice\") == s) if \"Next_Strikeprice\" in df.columns else False)\n",
        "    ]\n",
        "    unnamed_val = sample[\"Unnamed: 0\"].iloc[0] if (\"Unnamed: 0\" in df.columns and not sample.empty) else np.nan\n",
        "\n",
        "    n_obs_call = int(r.get(\"n_obs_call\")) if pd.notna(r.get(\"n_obs_call\")) else 0\n",
        "    n_obs_put  = int(r.get(\"n_obs_put\"))  if pd.notna(r.get(\"n_obs_put\"))  else 0\n",
        "    n_obs = n_obs_call + n_obs_put\n",
        "\n",
        "    def choose(field_call, field_put):\n",
        "        if field_call in r and pd.notna(r.get(field_call)):\n",
        "            return r.get(field_call)\n",
        "        if field_put in r and pd.notna(r.get(field_put)):\n",
        "            return r.get(field_put)\n",
        "        return np.nan\n",
        "\n",
        "    first_premium = choose(\"first_call_ltp\", \"first_put_ltp\")\n",
        "    last_premium  = choose(\"last_call_ltp\", \"last_put_ltp\")\n",
        "    peak_premium  = choose(\"peak_call_ltp\", \"peak_put_ltp\")\n",
        "    trough_premium= choose(\"trough_call_ltp\", \"trough_put_ltp\")\n",
        "    abs_change    = choose(\"abs_change_call\", \"abs_change_put\")\n",
        "    pct_change    = choose(\"pct_change_call\", \"pct_change_put\")\n",
        "\n",
        "    five_low  = choose(\"call_5min_low\", \"put_5min_low\")\n",
        "    five_high = choose(\"call_5min_high\", \"put_5min_high\")\n",
        "    ten_low   = choose(\"call_10min_low\", \"put_10min_low\")\n",
        "    ten_high  = choose(\"call_10min_high\", \"put_10min_high\")\n",
        "\n",
        "    p5_expected_lo = (five_low - last_premium) if (pd.notna(five_low) and pd.notna(last_premium)) else np.nan\n",
        "    p5_expected_hi = (five_high - last_premium) if (pd.notna(five_high) and pd.notna(last_premium)) else np.nan\n",
        "    p10_expected_lo = (ten_low - last_premium) if (pd.notna(ten_low) and pd.notna(last_premium)) else np.nan\n",
        "    p10_expected_hi = (ten_high - last_premium) if (pd.notna(ten_high) and pd.notna(last_premium)) else np.nan\n",
        "\n",
        "    tags_text = r.get(\"tags\",\"\") or \"\"\n",
        "    reasons = []\n",
        "    if any(k.lower() in tags_text.lower() for k in [\"rsi\",\"RSI\"]):\n",
        "        reasons.append(\"RSI momentum\")\n",
        "    if \"macd\" in tags_text.lower():\n",
        "        reasons.append(\"MACD confirmation\")\n",
        "    if \"vwap\" in tags_text.lower():\n",
        "        reasons.append(\"VWAP divergence\")\n",
        "    if \"oi\" in tags_text.lower():\n",
        "        reasons.append(\"OI support/resistance\")\n",
        "\n",
        "    call_mf = r.get(\"call_moneyflow\", 0.0) if \"call_moneyflow\" in r else 0.0\n",
        "    put_mf  = r.get(\"put_moneyflow\", 0.0)  if \"put_moneyflow\" in r else 0.0\n",
        "    try:\n",
        "        if float(call_mf) > 0:\n",
        "            reasons.append(\"Call net buying\")\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        if float(put_mf) > 0:\n",
        "            reasons.append(\"Put net buying\")\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        if pd.notna(pct_change):\n",
        "            if pct_change > 10:\n",
        "                reasons.append(\"Strong premium move\")\n",
        "            elif pct_change > 3:\n",
        "                reasons.append(\"Moderate premium move\")\n",
        "    except:\n",
        "        pass\n",
        "    if not reasons:\n",
        "        reasons.append(\"No strong signals\")\n",
        "    reasons_txt = \"; \".join(dict.fromkeys(reasons))\n",
        "\n",
        "    hc_col = \"Current_IsHighConvictionSignal\"\n",
        "    hc_total = 0\n",
        "    hc_success = 0\n",
        "    if hc_col in df.columns:\n",
        "        hc_rows = df[(df.get(\"Current_Strikeprice\")==s) & (df.get(hc_col)==True)]\n",
        "        hc_total = int(hc_rows.shape[0])\n",
        "        for _, hrow in hc_rows.iterrows():\n",
        "            t0 = hrow.get(\"LTT\")\n",
        "            action = r.get(\"recommended_action\",\"HOLD\")\n",
        "            p0 = None\n",
        "            if action == \"BUY_CALL\" and curr_call_col in df.columns:\n",
        "                p0 = hrow.get(curr_call_col)\n",
        "            elif action == \"BUY_PUT\" and curr_put_col in df.columns:\n",
        "                p0 = hrow.get(curr_put_col)\n",
        "            else:\n",
        "                p0 = hrow.get(curr_call_col) if curr_call_col in df.columns else hrow.get(curr_put_col) if curr_put_col in df.columns else None\n",
        "\n",
        "            if pd.isna(t0) or p0 is None or pd.isna(p0):\n",
        "                continue\n",
        "            window = df[(df[\"LTT\"] >= t0) & (df[\"LTT\"] <= t0 + timedelta(minutes=3)) & ((df.get(\"Current_Strikeprice\")==s) if \"Current_Strikeprice\" in df.columns else False)]\n",
        "            if window.empty:\n",
        "                continue\n",
        "            if action == \"BUY_PUT\" and curr_put_col in df.columns:\n",
        "                try:\n",
        "                    if window[curr_put_col].max() > p0:\n",
        "                        hc_success += 1\n",
        "                except:\n",
        "                    pass\n",
        "            else:\n",
        "                if curr_call_col in df.columns:\n",
        "                    try:\n",
        "                        if window[curr_call_col].max() > p0:\n",
        "                            hc_success += 1\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "    hc_rate = (hc_success / hc_total) if hc_total > 0 else None\n",
        "\n",
        "    final_rows.append({\n",
        "        \"strike\": s,\n",
        "        \"Unnamed: 0\": unnamed_val,\n",
        "        \"n_obs\": n_obs,\n",
        "        \"first_premium\": first_premium,\n",
        "        \"last_premium\": last_premium,\n",
        "        \"peak_premium\": peak_premium,\n",
        "        \"trough_premium\": trough_premium,\n",
        "        \"abs_change\": abs_change,\n",
        "        \"pct_change\": pct_change,\n",
        "        \"5min_low\": five_low,\n",
        "        \"5min_high\": five_high,\n",
        "        \"10min_low\": ten_low,\n",
        "        \"10min_high\": ten_high,\n",
        "        \"p5_expected_lo\": p5_expected_lo,\n",
        "        \"p5_expected_hi\": p5_expected_hi,\n",
        "        \"p10_expected_lo\": p10_expected_lo,\n",
        "        \"p10_expected_hi\": p10_expected_hi,\n",
        "        \"call_moneyflow\": r.get(\"call_moneyflow\", 0.0),\n",
        "        \"put_moneyflow\":  r.get(\"put_moneyflow\", 0.0),\n",
        "        \"tags\": r.get(\"tags\",\"\"),\n",
        "        \"reasons\": reasons_txt,\n",
        "        \"recommended_action\": r.get(\"recommended_action\",\"HOLD\"),\n",
        "        \"highconv_total\": hc_total,\n",
        "        \"highconv_success\": hc_success,\n",
        "        \"highconv_hit_rate\": hc_rate,\n",
        "        \"Current_Strikeprice\": s,\n",
        "        # include merged details for debugging\n",
        "        \"merged_row\": r.to_dict()\n",
        "    })\n",
        "\n",
        "final_df = pd.DataFrame(final_rows)\n",
        "\n",
        "# preserve original merged columns as well\n",
        "final_df = final_df.merge(merged.add_prefix(\"merged_\"), left_on=\"strike\", right_on=\"merged_strike\", how=\"left\")\n",
        "\n",
        "# ---------- SAVE CSVs ----------\n",
        "final_df.to_csv(OUT_MERGED, index=False)\n",
        "\n",
        "# top puts\n",
        "top_buy_puts = merged[merged[\"recommended_action\"]==\"BUY_PUT\"].copy()\n",
        "if \"pct_change_put\" in top_buy_puts.columns:\n",
        "    top_buy_puts = top_buy_puts.sort_values(\"pct_change_put\", ascending=False).head(200)\n",
        "else:\n",
        "    top_buy_puts = top_buy_puts.head(200)\n",
        "top_buy_puts.to_csv(OUT_TOP_PUTS, index=False)\n",
        "\n",
        "# top calls\n",
        "top_buy_calls = merged[merged[\"recommended_action\"] == \"BUY_CALL\"].copy()\n",
        "if \"pct_change_call\" in top_buy_calls.columns:\n",
        "    top_buy_calls = top_buy_calls.sort_values(\"pct_change_call\", ascending=False).head(200)\n",
        "else:\n",
        "    top_buy_calls = top_buy_calls.head(200)\n",
        "top_buy_calls.to_csv(OUT_TOP_CALLS, index=False)\n",
        "\n",
        "# all actions\n",
        "full_actions = merged.copy()\n",
        "def score_row(r):\n",
        "    cp = r.get(\"pct_change_call\", 0)\n",
        "    pp = r.get(\"pct_change_put\", 0)\n",
        "    if pd.isna(cp): cp = 0\n",
        "    if pd.isna(pp): pp = 0\n",
        "    if r.get(\"recommended_action\") == \"BUY_CALL\":\n",
        "        return cp\n",
        "    if r.get(\"recommended_action\") == \"BUY_PUT\":\n",
        "        return pp\n",
        "    return min(cp, pp) / 10\n",
        "full_actions[\"strength_score\"] = full_actions.apply(score_row, axis=1)\n",
        "full_actions = full_actions.sort_values(\"strength_score\", ascending=False)\n",
        "full_actions.to_csv(OUT_ALL_ACTIONS, index=False)\n",
        "\n",
        "# CE only / PE only\n",
        "ce_only = merged.copy()\n",
        "ce_only = ce_only[ce_only[\"pct_change_call\"].notna() & (ce_only[\"n_obs_call\"].fillna(0) > 0)]\n",
        "ce_only.to_csv(OUT_CE_ONLY, index=False)\n",
        "\n",
        "pe_only = merged.copy()\n",
        "pe_only = pe_only[pe_only[\"pct_change_put\"].notna() & (pe_only[\"n_obs_put\"].fillna(0) > 0)]\n",
        "pe_only.to_csv(OUT_PE_ONLY, index=False)\n",
        "\n",
        "print(\"Saved:\", OUT_MERGED, OUT_TOP_PUTS, OUT_TOP_CALLS, OUT_ALL_ACTIONS, OUT_CE_ONLY, OUT_PE_ONLY)\n",
        "\n",
        "# ---------- SPOT vs momentum plot ----------\n",
        "last_spot = df[\"SpotPrice\"].dropna().iloc[-1] if \"SpotPrice\" in df.columns and not df[\"SpotPrice\"].dropna().empty else np.nan\n",
        "avg_call_pct = merged[\"pct_change_call\"].replace([np.inf, -np.inf], np.nan).dropna().mean() if \"pct_change_call\" in merged.columns else np.nan\n",
        "avg_put_pct  = merged[\"pct_change_put\"].replace([np.inf, -np.inf], np.nan).dropna().mean()  if \"pct_change_put\" in merged.columns  else np.nan\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.title(\"Spot (last) vs Avg CE/PE %change\")\n",
        "vals = [last_spot if not math.isnan(last_spot) else 0, avg_call_pct if not pd.isna(avg_call_pct) else 0, avg_put_pct if not pd.isna(avg_put_pct) else 0]\n",
        "plt.bar([\"Spot Last\",\"Avg CE %\",\"Avg PE %\"], vals)\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUT_PLOT, dpi=150)\n",
        "plt.close()\n",
        "print(\"Saved:\", OUT_PLOT)\n",
        "\n",
        "# ---------- ALERTS: PUT REVERSAL CANDIDATES ----------\n",
        "alerts = []\n",
        "for _, r in merged.iterrows():\n",
        "    pct_put = r.get(\"pct_change_put\")\n",
        "    if pct_put is None or (isinstance(pct_put, float) and np.isnan(pct_put)):\n",
        "        continue\n",
        "    tags = (r.get(\"tags\") or \"\").lower()\n",
        "    put_money = r.get(\"put_moneyflow\", 0.0) if \"put_moneyflow\" in r else 0.0\n",
        "    if pct_put > 8 and ((\"put buying\" in tags) or (\"bearish\" in tags) or (put_money and put_money > 0)):\n",
        "        alerts.append({\n",
        "            \"strike\": int(r[\"strike\"]),\n",
        "            \"pct_change_put\": pct_put,\n",
        "            \"put_moneyflow\": put_money,\n",
        "            \"tags\": r.get(\"tags\",\"\"),\n",
        "            \"recommended_action\": r.get(\"recommended_action\",\"\"),\n",
        "            \"reason\": \"Put momentum >8% and bearish tag/moneyflow\"\n",
        "        })\n",
        "\n",
        "alerts_df = pd.DataFrame(alerts)\n",
        "if not alerts_df.empty:\n",
        "    alerts_df.to_csv(OUT_ALERTS, index=False)\n",
        "    print(\"Saved:\", OUT_ALERTS)\n",
        "    print(\"Sample alerts:\\n\", alerts_df.head(20).to_string(index=False))\n",
        "else:\n",
        "    print(\"No put reversal alerts found.\")\n",
        "\n",
        "# ---------- OPTIONAL HOLD/CE/PE tables (already saved above) ----------\n",
        "hold_df = merged[merged[\"recommended_action\"] == \"HOLD\"].copy()\n",
        "\n",
        "# breakout score (already used earlier)\n",
        "def breakout_score(row):\n",
        "    score = 0\n",
        "    if pd.notna(row.get(\"pct_change_call\")):\n",
        "        score += max(row[\"pct_change_call\"], 0)\n",
        "    if pd.notna(row.get(\"pct_change_put\")):\n",
        "        score += max(row[\"pct_change_put\"], 0)\n",
        "    try:\n",
        "        score += (row.get(\"call_moneyflow\", 0) > 0) * 5\n",
        "        score += (row.get(\"put_moneyflow\", 0) > 0) * 5\n",
        "    except:\n",
        "        pass\n",
        "    tags = str(row.get(\"tags\", \"\")).lower()\n",
        "    if \"breakout\" in tags: score += 10\n",
        "    if \"momentum\" in tags: score += 10\n",
        "    if \"rsi\" in tags: score += 5\n",
        "    if \"macd\" in tags: score += 5\n",
        "    return score\n",
        "\n",
        "hold_df[\"breakout_score\"] = hold_df.apply(breakout_score, axis=1)\n",
        "hold_df = hold_df.sort_values(\"breakout_score\", ascending=False)\n",
        "hold_df.to_csv(OUT_TOP_HOLD, index=False)\n",
        "print(\"Saved:\", OUT_TOP_HOLD)\n",
        "\n",
        "# ---------- REVERSAL DETECTION ----------\n",
        "# Heuristic: find recent peak in last REVERSAL_WINDOW_MIN minutes in strike series and if\n",
        "# premium dropped by REVERSAL_DROP_PCT% or more from that peak -> reversal candidate\n",
        "reversals = []\n",
        "for s, rec in (call_records + put_records):\n",
        "    # note: call_records/put_records entries are dicts with 'strike' and 'call_series_sorted'/'put_series_sorted'\n",
        "    pass\n",
        "\n",
        "# work with call_series & put_series which contain sorted lists (unsorted originally)\n",
        "def detect_reversals_for_series(series_map, side_name):\n",
        "    out = []\n",
        "    for s, pairs in series_map.items():\n",
        "        # sorted by time\n",
        "        sr = sorted(pairs, key=lambda x: x[0])\n",
        "        if len(sr) < 3:\n",
        "            continue\n",
        "        # consider last timestamp\n",
        "        t_last = sr[-1][0]\n",
        "        # find peak within lookback window\n",
        "        lookback = pd.Timedelta(minutes=REVERSAL_WINDOW_MIN)\n",
        "        window = [(t,p) for t,p in sr if t >= (t_last - lookback)]\n",
        "        if not window:\n",
        "            continue\n",
        "        # peak price in window and last price\n",
        "        peak_p = max([p for _,p in window])\n",
        "        last_p = window[-1][1]\n",
        "        # percentage drop from peak to last\n",
        "        drop_pct = 0.0\n",
        "        try:\n",
        "            drop_pct = (peak_p - last_p) / peak_p * 100 if peak_p != 0 else 0.0\n",
        "        except:\n",
        "            drop_pct = 0.0\n",
        "        if drop_pct >= REVERSAL_DROP_PCT:\n",
        "            out.append({\n",
        "                \"strike\": s,\n",
        "                \"side\": side_name,\n",
        "                \"peak_in_window\": peak_p,\n",
        "                \"last_price\": last_p,\n",
        "                \"drop_pct\": drop_pct,\n",
        "                \"window_minutes\": REVERSAL_WINDOW_MIN\n",
        "            })\n",
        "    return out\n",
        "\n",
        "# call_series & put_series are maps of strike -> [(t,p),...]\n",
        "call_reversals = detect_reversals_for_series(call_series, \"CALL\")\n",
        "put_reversals  = detect_reversals_for_series(put_series, \"PUT\")\n",
        "\n",
        "rev_df = pd.DataFrame(call_reversals + put_reversals)\n",
        "if not rev_df.empty:\n",
        "    rev_df.to_csv(OUT_REVERSALS, index=False)\n",
        "    print(\"Saved:\", OUT_REVERSALS)\n",
        "else:\n",
        "    print(\"No strong reversals detected by heuristic.\")\n",
        "\n",
        "# ---------- IV-CRUSH DETECTION ----------\n",
        "# Heuristic: for each strike, compute recent IV change (Current_IV vs previous IV)\n",
        "iv_crush_list = []\n",
        "iv_cols_candidates = [\n",
        "    (\"Previous_Call_IV\",\"Current_Call_IV\",\"Next_Call_IV\"),\n",
        "    (\"Previous_Put_IV\",\"Current_Put_IV\",\"Next_Put_IV\")\n",
        "]\n",
        "\n",
        "# We'll operate per-strike, scanning rows where Current_Strikeprice == s and checking IV drops between previous/current or current/next when present\n",
        "for idx, row in df.iterrows():\n",
        "    for side, (prev_col, curr_col, next_col) in ((\"CALL\", iv_cols_candidates[0]), (\"PUT\", iv_cols_candidates[1])):\n",
        "        if curr_col not in df.columns:\n",
        "            continue\n",
        "        s = row.get(\"Current_Strikeprice\") if \"Current_Strikeprice\" in df.columns else None\n",
        "        if pd.isna(s):\n",
        "            continue\n",
        "        try:\n",
        "            s = int(s)\n",
        "        except:\n",
        "            continue\n",
        "        iv_curr = row.get(curr_col)\n",
        "        iv_prev = row.get(prev_col) if prev_col in df.columns else None\n",
        "        # if both present, compute absolute and relative drop\n",
        "        if pd.notna(iv_curr) and pd.notna(iv_prev):\n",
        "            try:\n",
        "                abs_drop = float(iv_prev) - float(iv_curr)\n",
        "                rel_drop = (abs_drop / float(iv_prev) * 100) if float(iv_prev) != 0 else 0.0\n",
        "            except:\n",
        "                abs_drop = 0.0\n",
        "                rel_drop = 0.0\n",
        "            if (abs_drop >= IV_CRUSH_ABS) or (rel_drop >= IV_CRUSH_REL_PCT):\n",
        "                iv_crush_list.append({\n",
        "                    \"strike\": s,\n",
        "                    \"side\": side,\n",
        "                    \"iv_prev\": iv_prev,\n",
        "                    \"iv_curr\": iv_curr,\n",
        "                    \"abs_drop\": abs_drop,\n",
        "                    \"rel_drop_pct\": rel_drop,\n",
        "                    \"row_index\": idx,\n",
        "                    \"LTT\": row.get(\"LTT\")\n",
        "                })\n",
        "\n",
        "iv_crush_df = pd.DataFrame(iv_crush_list)\n",
        "if not iv_crush_df.empty:\n",
        "    iv_crush_df.to_csv(OUT_IV_CRUSH, index=False)\n",
        "    print(\"Saved:\", OUT_IV_CRUSH)\n",
        "else:\n",
        "    print(\"No IV crush candidates found with thresholds.\")\n",
        "\n",
        "# ---------- STRENGTH HEATMAP ----------\n",
        "# Build a small matrix: for each strike (rows) show [avg pct_change_call, avg pct_change_put, strength_score]\n",
        "heat_rows = []\n",
        "for _, r in merged.iterrows():\n",
        "    strike = int(r[\"strike\"])\n",
        "    a = {}\n",
        "    a[\"strike\"] = strike\n",
        "    a[\"avg_pct_call\"] = r.get(\"pct_change_call\") if not pd.isna(r.get(\"pct_change_call\")) else 0.0\n",
        "    a[\"avg_pct_put\"]  = r.get(\"pct_change_put\")  if not pd.isna(r.get(\"pct_change_put\")) else 0.0\n",
        "    # reuse earlier scoring\n",
        "    a[\"strength\"] = score_row(r)\n",
        "    heat_rows.append(a)\n",
        "\n",
        "heat_df = pd.DataFrame(heat_rows).sort_values(\"strike\", ascending=True)\n",
        "if not heat_df.empty:\n",
        "    # create heatmap data: columns = [avg_pct_call, avg_pct_put, strength]\n",
        "    hm = heat_df[[\"avg_pct_call\",\"avg_pct_put\",\"strength\"]].fillna(0).to_numpy()\n",
        "    plt.figure(figsize=(6, max(6, len(hm)/20)))\n",
        "    plt.title(\"Strike strength heatmap (rows=strikes)\")\n",
        "    # imshow without custom colors (matplotlib default)\n",
        "    plt.imshow(hm, aspect='auto', interpolation='nearest')\n",
        "    plt.colorbar(label='value')\n",
        "    plt.yticks(range(len(heat_df)), heat_df[\"strike\"].astype(str))\n",
        "    plt.xticks([0,1,2], [\"avg_pct_call\",\"avg_pct_put\",\"strength\"])\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUT_HEATMAP, dpi=150)\n",
        "    plt.close()\n",
        "    print(\"Saved:\", OUT_HEATMAP)\n",
        "else:\n",
        "    print(\"Heatmap skipped (no heat data).\")\n",
        "\n",
        "# ---------- AUTO-TRADE EXPORT (JSON) ----------\n",
        "# Export simple order suggestions for strong signals\n",
        "auto_trades = []\n",
        "for _, r in full_actions.iterrows():\n",
        "    strength = r.get(\"strength_score\", 0)\n",
        "    if pd.isna(strength) or strength < AUTO_TRADE_STRENGTH:\n",
        "        continue\n",
        "    action = r.get(\"recommended_action\", \"HOLD\")\n",
        "    if action not in (\"BUY_CALL\",\"BUY_PUT\"):\n",
        "        continue\n",
        "    strike = int(r[\"strike\"])\n",
        "    # choose price: prefer last_call_ltp/last_put_ltp then merged fields\n",
        "    if action == \"BUY_CALL\":\n",
        "        price = r.get(\"last_call_ltp\") if not pd.isna(r.get(\"last_call_ltp\")) else r.get(\"merged_last_call_ltp\") if \"merged_last_call_ltp\" in r else None\n",
        "    else:\n",
        "        price = r.get(\"last_put_ltp\") if not pd.isna(r.get(\"last_put_ltp\")) else r.get(\"merged_last_put_ltp\") if \"merged_last_put_ltp\" in r else None\n",
        "\n",
        "    lot = DEFAULT_LOTSIZE\n",
        "    # if df has Current_Lotsize or Current_Lotsize column, prefer it\n",
        "    if \"Current_Lotsize\" in df.columns:\n",
        "        # pick most recent lotsize for this strike\n",
        "        sample = df[df.get(\"Current_Strikeprice\")==strike] if \"Current_Strikeprice\" in df.columns else df[df.get(\"Previous_Strikeprice\")==strike] if \"Previous_Strikeprice\" in df.columns else None\n",
        "        if sample is not None and not sample.empty and \"Current_Lotsize\" in sample.columns:\n",
        "            try:\n",
        "                lot = int(sample[\"Current_Lotsize\"].iloc[-1])\n",
        "            except:\n",
        "                lot = DEFAULT_LOTSIZE\n",
        "\n",
        "    order = {\n",
        "        \"strike\": strike,\n",
        "        \"side\": \"CE\" if action==\"BUY_CALL\" else \"PE\",\n",
        "        \"action\": action,\n",
        "        \"suggested_price\": float(price) if price is not None and not pd.isna(price) else None,\n",
        "        \"lots\": lot,\n",
        "        \"strength_score\": float(strength)\n",
        "    }\n",
        "    auto_trades.append(order)\n",
        "\n",
        "# write JSON\n",
        "with open(OUT_AUTOTRADE, \"w\") as fh:\n",
        "    json.dump(auto_trades, fh, indent=2, default=str)\n",
        "\n",
        "print(\"Saved:\", OUT_AUTOTRADE, \" (\", len(auto_trades), \"orders )\")\n",
        "\n",
        "print(\"Pipeline complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "Ga3KPkZTBPVm",
        "outputId": "d3af9e32-8bd9-446b-84dd-2335c14d4b5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3154631351.py:40: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  df = pd.read_csv(INPUT_CSV, low_memory=False, parse_dates=['LTT'], infer_datetime_format=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using columns (LTP prioritized):\n",
            "Calls: Previous_Call_ltp Current_Call_ltp Next_Call_ltp\n",
            "Puts : Previous_Put_ltp Current_Put_ltp Next_Put_ltp\n",
            "Strikes: Previous_Strikeprice Current_Strikeprice Next_Strikeprice\n",
            "Detected call strikes: 8\n",
            "Detected put strikes : 8\n",
            "Saved: MERGED_CE_PE_FORECAST.csv TOP_BUY_PUTS.csv TOP_BUY_CALLS.csv ALL_ACTIONS.csv TOP_CE_ONLY.csv TOP_PE_ONLY.csv\n",
            "Saved: SPOT_VS_MOMENTUM.png\n",
            "Saved: PUT_REVERSAL_ALERTS.csv\n",
            "Sample alerts:\n",
            "  strike  pct_change_put  put_moneyflow                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 tags recommended_action                                     reason\n",
            "  58500        9.674843   1.429351e+13 RSI_MACD_Bull VWAP_Divergence PnL_Explosion:6182;RSI_MACD_Bull VWAP_Divergence PnL_Explosion OI_Support_Call:2869;VWAP_Divergence PnL_Explosion:17872;VWAP_Divergence PnL_Explosion OI_Support_Call:9097;RSI_MACD_Bear VWAP_Divergence PnL_Explosion OI_Support_Put:3170;RSI_MACD_Bear VWAP_Divergence PnL_Explosion OI_Support_Call:2407;DeltaNeutral:1540;LowRealHighIV:1351;DeltaNeutral LowRealHighIV:593;VWAP_Divergence PnL_Explosion Prem_Momentum- OI_Support_Call:6;RSI_MACD_Bear VWAP_Divergence PnL_Explosion:2461;RSI_MACD_Bear VWAP_Divergence PnL_Explosion Prem_Momentum+ OI_Support_Call:34;RSI_MACD_Bear VWAP_Divergence PnL_Explosion Prem_Momentum+ OI_Support_Put:38;VWAP_Divergence PnL_Explosion Prem_Momentum+ OI_Support_Call:155;RSI_MACD_Bull VWAP_Divergence PnL_Explosion Prem_Momentum+ OI_Support_Call:51;VWAP_Divergence PnL_Explosion Prem_Momentum+:144;RSI_MACD_Bull VWAP_Divergence PnL_Explosion Prem_Momentum- OI_Support_Call:1;RSI_MACD_Bear VWAP_Divergence PnL_Explosion Prem_Momentum-:3;RSI_MACD_Bull VWAP_Divergence PnL_Explosion Prem_Momentum+:84;RSI_MACD_Bear VWAP_Divergence PnL_Explosion Prem_Momentum+:7;RSI_MACD_Bear VWAP_Divergence PnL_Explosion Prem_Momentum- OI_Support_Call:3;RSI_MACD_Bull VWAP_Divergence PnL_Explosion Prem_Momentum-:5;VWAP_Divergence PnL_Explosion Prem_Momentum-:12;RSI_MACD_Bear VWAP_Divergence PnL_Explosion Prem_Momentum- OI_Support_Put:14            BUY_PUT Put momentum >8% and bearish tag/moneyflow\n",
            "Saved: TOP_HOLD_BREAKOUTS.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3154631351.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;31m# premium dropped by REVERSAL_DROP_PCT% or more from that peak -> reversal candidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0mreversals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcall_records\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mput_records\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m     \u001b[0;31m# note: call_records/put_records entries are dicts with 'strike' and 'call_series_sorted'/'put_series_sorted'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# full_ce_pe_pipeline_use_ltp_enriched_final.py\n",
        "\n",
        "import os, sys, math, json\n",
        "from collections import defaultdict, Counter\n",
        "from datetime import timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------------- CONFIG ---------------------- #\n",
        "INPUT_CSV = \"flattened_snapshots.csv\"\n",
        "\n",
        "OUT_MERGED = \"MERGED_CE_PE_FORECAST.csv\"\n",
        "OUT_TOP_PUTS = \"TOP_BUY_PUTS.csv\"\n",
        "OUT_TOP_CALLS = \"TOP_BUY_CALLS.csv\"\n",
        "OUT_ALL_ACTIONS = \"ALL_ACTIONS.csv\"\n",
        "\n",
        "OUT_CE_ONLY = \"TOP_CE_ONLY.csv\"\n",
        "OUT_PE_ONLY = \"TOP_PE_ONLY.csv\"\n",
        "OUT_HOLD_BREAKOUTS = \"TOP_HOLD_BREAKOUTS.csv\"\n",
        "\n",
        "OUT_REVERSALS = \"REVERSALS.csv\"\n",
        "OUT_IV_CRUSH = \"IV_CRUSH.csv\"\n",
        "\n",
        "OUT_PLOT = \"SPOT_VS_MOMENTUM.png\"\n",
        "OUT_HEATMAP = \"PREMIUM_HEATMAP.png\"\n",
        "OUT_AUTOTRADE = \"AUTO_TRADE_SIGNALS.json\"\n",
        "\n",
        "REVERSAL_WINDOW_MIN = 5\n",
        "REVERSAL_DROP_PCT = 12        # % drop from peak to last\n",
        "IV_CRUSH_DROP = 15            # % drop in IV\n",
        "\n",
        "\n",
        "# ---------------------- LOAD ---------------------- #\n",
        "if not os.path.exists(INPUT_CSV):\n",
        "    print(\"ERROR: input file not found:\", INPUT_CSV)\n",
        "    sys.exit(1)\n",
        "\n",
        "df = pd.read_csv(\n",
        "    INPUT_CSV,\n",
        "    low_memory=False,\n",
        "    parse_dates=['LTT']  # infer_datetime_format is deprecated\n",
        ")\n",
        "\n",
        "# ---------------------- USE LTP COLUMNS ---------------------- #\n",
        "PREV_CALL_LTP = \"Previous_Call_ltp\"\n",
        "CURR_CALL_LTP = \"Current_Call_ltp\"\n",
        "NEXT_CALL_LTP = \"Next_Call_ltp\"\n",
        "\n",
        "PREV_PUT_LTP  = \"Previous_Put_ltp\"\n",
        "CURR_PUT_LTP  = \"Current_Put_ltp\"\n",
        "NEXT_PUT_LTP  = \"Next_Put_ltp\"\n",
        "\n",
        "PREV_STR = \"Previous_Strikeprice\"\n",
        "CURR_STR = \"Current_Strikeprice\"\n",
        "NEXT_STR = \"Next_Strikeprice\"\n",
        "\n",
        "# Fallback to Premium if LTP missing (rare)\n",
        "def pick(col_ltp, col_pre):\n",
        "    if col_ltp in df.columns:\n",
        "        return col_ltp\n",
        "    if col_pre in df.columns:\n",
        "        return col_pre\n",
        "    return None\n",
        "\n",
        "prev_call_col = pick(PREV_CALL_LTP, \"Previous_Call_Premium\")\n",
        "curr_call_col = pick(CURR_CALL_LTP, \"Current_Call_Premium\")\n",
        "next_call_col = pick(NEXT_CALL_LTP, \"Next_Call_Premium\")\n",
        "\n",
        "prev_put_col = pick(PREV_PUT_LTP, \"Previous_Put_Premium\")\n",
        "curr_put_col = pick(CURR_PUT_LTP, \"Current_Put_Premium\")\n",
        "next_put_col = pick(NEXT_PUT_LTP, \"Next_Put_Premium\")\n",
        "\n",
        "print(\"Using Call columns:\", prev_call_col, curr_call_col, next_call_col)\n",
        "print(\"Using Put  columns:\", prev_put_col,  curr_put_col,  next_put_col)\n",
        "\n",
        "\n",
        "# ---------------------- BUILD SERIES ---------------------- #\n",
        "def build_series(prev_col, curr_col, next_col):\n",
        "    series = defaultdict(list)\n",
        "    for _, row in df.iterrows():\n",
        "        t = row[\"LTT\"]\n",
        "        for sc, pc in [(PREV_STR, prev_col), (CURR_STR, curr_col), (NEXT_STR, next_col)]:\n",
        "            if sc not in row or pc not in row:\n",
        "                continue\n",
        "            if pd.isna(row[sc]) or pd.isna(row[pc]):\n",
        "                continue\n",
        "            try:\n",
        "                s = int(row[sc])\n",
        "                p = float(row[pc])\n",
        "                series[s].append((t, p))\n",
        "            except:\n",
        "                continue\n",
        "    return series\n",
        "\n",
        "\n",
        "call_series = build_series(prev_call_col, curr_call_col, next_call_col)\n",
        "put_series = build_series(prev_put_col, curr_put_col, next_put_col)\n",
        "\n",
        "print(\"Detected call strikes:\", len(call_series))\n",
        "print(\"Detected put strikes :\", len(put_series))\n",
        "\n",
        "\n",
        "# ---------------------- SUMMARY + FORECAST ---------------------- #\n",
        "def summarize(pairs):\n",
        "    sr = sorted(pairs, key=lambda x: x[0])\n",
        "    ps = [p for _, p in sr]\n",
        "    if not ps:\n",
        "        return None\n",
        "    first, last = ps[0], ps[-1]\n",
        "    abs_chg = last - first\n",
        "    pct_chg = abs_chg / first * 100 if first != 0 else np.nan\n",
        "    return {\n",
        "        \"first\": first,\n",
        "        \"last\": last,\n",
        "        \"peak\": max(ps),\n",
        "        \"trough\": min(ps),\n",
        "        \"abs_change\": abs_chg,\n",
        "        \"pct_change\": pct_chg,\n",
        "        \"n_obs\": len(ps)\n",
        "    }\n",
        "\n",
        "\n",
        "def forecast(last, pct):\n",
        "    if pct >= 25:\n",
        "        return (last+15, last+35), (last+25, last+60)\n",
        "    if pct >= 8:\n",
        "        return (last+6, last+18), (last+12, last+30)\n",
        "    if pct > 0:\n",
        "        return (last+2, last+8), (last+5, last+15)\n",
        "    return (last-5, last+2), (last-8, last+5)\n",
        "\n",
        "\n",
        "# ---------------------- CALL / PUT SUMMARY TABLES ---------------------- #\n",
        "call_records, put_records = [], []\n",
        "\n",
        "for s, ts in call_series.items():\n",
        "    st = summarize(ts)\n",
        "    if not st: continue\n",
        "    f5, f10 = forecast(st[\"last\"], st[\"pct_change\"])\n",
        "    call_records.append({\n",
        "        \"strike\": s,\n",
        "        \"n_obs_call\": st[\"n_obs\"],\n",
        "        \"first_call_ltp\": st[\"first\"],\n",
        "        \"last_call_ltp\": st[\"last\"],\n",
        "        \"peak_call_ltp\": st[\"peak\"],\n",
        "        \"trough_call_ltp\": st[\"trough\"],\n",
        "        \"abs_change_call\": st[\"abs_change\"],\n",
        "        \"pct_change_call\": st[\"pct_change\"],\n",
        "        \"call_5min_low\": f5[0], \"call_5min_high\": f5[1],\n",
        "        \"call_10min_low\": f10[0], \"call_10min_high\": f10[1]\n",
        "    })\n",
        "\n",
        "for s, ts in put_series.items():\n",
        "    st = summarize(ts)\n",
        "    if not st: continue\n",
        "    f5, f10 = forecast(st[\"last\"], st[\"pct_change\"])\n",
        "    put_records.append({\n",
        "        \"strike\": s,\n",
        "        \"n_obs_put\": st[\"n_obs\"],\n",
        "        \"first_put_ltp\": st[\"first\"],\n",
        "        \"last_put_ltp\": st[\"last\"],\n",
        "        \"peak_put_ltp\": st[\"peak\"],\n",
        "        \"trough_put_ltp\": st[\"trough\"],\n",
        "        \"abs_change_put\": st[\"abs_change\"],\n",
        "        \"pct_change_put\": st[\"pct_change\"],\n",
        "        \"put_5min_low\": f5[0], \"put_5min_high\": f5[1],\n",
        "        \"put_10min_low\": f10[0], \"put_10min_high\": f10[1]\n",
        "    })\n",
        "\n",
        "call_df = pd.DataFrame(call_records)\n",
        "put_df  = pd.DataFrame(put_records)\n",
        "\n",
        "merged = pd.merge(call_df, put_df, on=\"strike\", how=\"outer\")\n",
        "\n",
        "\n",
        "# ---------------------- TAGS + MONEYFLOW ---------------------- #\n",
        "tag_cols = [\"Previous_StrategyTag\",\"Current_StrategyTag\",\"Next_StrategyTag\"]\n",
        "call_mflow = [\"Previous_CallMoneyFlow\",\"Current_CallMoneyFlow\",\"Next_CallMoneyFlow\"]\n",
        "put_mflow  = [\"Previous_PutMoneyFlow\",\"Current_PutMoneyFlow\",\"Next_PutMoneyFlow\"]\n",
        "\n",
        "strike_info = {int(s):{\"tags\":Counter(),\"call_mf\":0,\"put_mf\":0} for s in merged[\"strike\"]}\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    for sc in [PREV_STR, CURR_STR, NEXT_STR]:\n",
        "        if pd.isna(row.get(sc)): continue\n",
        "        s = int(row[sc])\n",
        "        if s not in strike_info: continue\n",
        "\n",
        "        # tags\n",
        "        for tc in tag_cols:\n",
        "            if tc in row and isinstance(row[tc], str):\n",
        "                tokens = [x.strip() for x in row[tc].replace(\"|\",\";\").split(\";\") if x.strip()]\n",
        "                for t in tokens:\n",
        "                    strike_info[s][\"tags\"][t] += 1\n",
        "\n",
        "        # moneyflow\n",
        "        for col in call_mflow:\n",
        "            if col in row and not pd.isna(row[col]):\n",
        "                strike_info[s][\"call_mf\"] += float(row[col])\n",
        "        for col in put_mflow:\n",
        "            if col in row and not pd.isna(row[col]):\n",
        "                strike_info[s][\"put_mf\"] += float(row[col])\n",
        "\n",
        "merged[\"tags\"] = merged[\"strike\"].apply(lambda s: \";\".join([f\"{k}:{v}\" for k,v in strike_info[int(s)][\"tags\"].items()]))\n",
        "merged[\"call_moneyflow\"] = merged[\"strike\"].apply(lambda s: strike_info[int(s)][\"call_mf\"])\n",
        "merged[\"put_moneyflow\"]  = merged[\"strike\"].apply(lambda s: strike_info[int(s)][\"put_mf\"])\n",
        "\n",
        "\n",
        "# ---------------------- ACTION DECIDER ---------------------- #\n",
        "def decide(row):\n",
        "    cp = row.get(\"pct_change_call\",0) or 0\n",
        "    pp = row.get(\"pct_change_put\",0) or 0\n",
        "    t = row.get(\"tags\",\"\").lower()\n",
        "\n",
        "    bull = (\"call buying\" in t) or (cp > 8)\n",
        "    bear = (\"put buying\" in t) or (pp > 8)\n",
        "\n",
        "    if bear: return \"BUY_PUT\"\n",
        "    if bull: return \"BUY_CALL\"\n",
        "    return \"HOLD\"\n",
        "\n",
        "merged[\"recommended_action\"] = merged.apply(decide, axis=1)\n",
        "\n",
        "\n",
        "# ---------------------- REVERSAL DETECTION (corrected) ---------------------- #\n",
        "def detect_reversals(series_map, side):\n",
        "    out = []\n",
        "    for s, ts in series_map.items():\n",
        "        sr = sorted(ts, key=lambda x:x[0])\n",
        "        if len(sr) < 3:\n",
        "            continue\n",
        "        t_last = sr[-1][0]\n",
        "        window = [(t,p) for t,p in sr if t >= (t_last - timedelta(minutes=REVERSAL_WINDOW_MIN))]\n",
        "        if not window:\n",
        "            continue\n",
        "        peak = max([p for _,p in window])\n",
        "        last = window[-1][1]\n",
        "        drop = (peak-last)/peak*100 if peak>0 else 0\n",
        "        if drop >= REVERSAL_DROP_PCT:\n",
        "            out.append({\"strike\":s,\"side\":side,\"peak\":peak,\"last\":last,\"drop_pct\":drop})\n",
        "    return out\n",
        "\n",
        "rev_df = pd.DataFrame(detect_reversals(call_series,\"CALL\") +\n",
        "                      detect_reversals(put_series,\"PUT\"))\n",
        "rev_df.to_csv(OUT_REVERSALS, index=False)\n",
        "\n",
        "\n",
        "# ---------------------- IV CRUSH DETECTION ---------------------- #\n",
        "iv_cols_prev = [\"Previous_Call_IV\",\"Previous_Put_IV\"]\n",
        "iv_cols_curr = [\"Current_Call_IV\",\"Current_Put_IV\"]\n",
        "\n",
        "iv_crush = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    for prev_iv, curr_iv, side in [\n",
        "        (\"Previous_Call_IV\",\"Current_Call_IV\",\"CALL\"),\n",
        "        (\"Previous_Put_IV\",\"Current_Put_IV\",\"PUT\")\n",
        "    ]:\n",
        "        if prev_iv in row and curr_iv in row:\n",
        "            if pd.notna(row[prev_iv]) and pd.notna(row[curr_iv]):\n",
        "                if row[prev_iv] > 0:\n",
        "                    drop = (row[prev_iv] - row[curr_iv]) / row[prev_iv] * 100\n",
        "                    if drop >= IV_CRUSH_DROP:\n",
        "                        iv_crush.append({\n",
        "                            \"LTT\": row[\"LTT\"],\n",
        "                            \"strike\": row.get(\"Current_Strikeprice\"),\n",
        "                            \"side\": side,\n",
        "                            \"prev_iv\": row[prev_iv],\n",
        "                            \"curr_iv\": row[curr_iv],\n",
        "                            \"drop_pct\": drop\n",
        "                        })\n",
        "\n",
        "iv_crush_df = pd.DataFrame(iv_crush)\n",
        "iv_crush_df.to_csv(OUT_IV_CRUSH, index=False)\n",
        "\n",
        "\n",
        "# ---------------------- HEATMAP ---------------------- #\n",
        "plt.figure(figsize=(10,8))\n",
        "heatmap_df = merged[[\"pct_change_call\",\"pct_change_put\"]].fillna(0)\n",
        "plt.imshow(heatmap_df.values, aspect='auto')\n",
        "plt.colorbar(label=\"Premium % Change\")\n",
        "plt.title(\"CALL/PUT Premium Heatmap\")\n",
        "plt.savefig(OUT_HEATMAP, dpi=150)\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# ---------------------- AUTO-TRADE EXPORT ---------------------- #\n",
        "signals = []\n",
        "\n",
        "for _, r in merged.iterrows():\n",
        "    if r[\"recommended_action\"] in [\"BUY_CALL\",\"BUY_PUT\"]:\n",
        "        signals.append({\n",
        "            \"strike\": int(r[\"strike\"]),\n",
        "            \"action\": r[\"recommended_action\"],\n",
        "            \"strength\": float(r.get(\"pct_change_call\",0)) if r[\"recommended_action\"]==\"BUY_CALL\"\n",
        "                        else float(r.get(\"pct_change_put\",0)),\n",
        "            \"reason\": r.get(\"tags\",\"\")\n",
        "        })\n",
        "\n",
        "with open(OUT_AUTOTRADE,\"w\") as f:\n",
        "    json.dump(signals, f, indent=4)\n",
        "\n",
        "\n",
        "# ---------------------- WRITE OUTPUT CSVs ---------------------- #\n",
        "merged.to_csv(OUT_MERGED,index=False)\n",
        "merged[merged[\"recommended_action\"]==\"BUY_PUT\"].sort_values(\"pct_change_put\",ascending=False).head(200).to_csv(OUT_TOP_PUTS,index=False)\n",
        "merged[merged[\"recommended_action\"]==\"BUY_CALL\"].sort_values(\"pct_change_call\",ascending=False).head(200).to_csv(OUT_TOP_CALLS,index=False)\n",
        "\n",
        "# All actions sorted by strength\n",
        "merged[\"strength\"] = merged.apply(lambda r: max(\n",
        "    r.get(\"pct_change_call\",0) or 0,\n",
        "    r.get(\"pct_change_put\",0) or 0\n",
        "), axis=1)\n",
        "merged.sort_values(\"strength\",ascending=False).to_csv(OUT_ALL_ACTIONS,index=False)\n",
        "\n",
        "# CE-only & PE-only\n",
        "merged[merged[\"pct_change_call\"].notna()].to_csv(OUT_CE_ONLY,index=False)\n",
        "merged[merged[\"pct_change_put\"].notna()].to_csv(OUT_PE_ONLY,index=False)\n",
        "\n",
        "# HOLD breakout\n",
        "hold_df = merged[merged[\"recommended_action\"]==\"HOLD\"].copy()\n",
        "hold_df[\"breakout_score\"] = (\n",
        "    hold_df[\"pct_change_call\"].clip(lower=0).fillna(0)\n",
        "    + hold_df[\"pct_change_put\"].clip(lower=0).fillna(0)\n",
        ")\n",
        "hold_df.sort_values(\"breakout_score\",ascending=False).to_csv(OUT_HOLD_BREAKOUTS,index=False)\n",
        "\n",
        "print(\"✔ All processing complete.\")\n",
        "print(\"Generated:\")\n",
        "print(\"-\", OUT_MERGED)\n",
        "print(\"-\", OUT_TOP_PUTS)\n",
        "print(\"-\", OUT_TOP_CALLS)\n",
        "print(\"-\", OUT_ALL_ACTIONS)\n",
        "print(\"-\", OUT_CE_ONLY)\n",
        "print(\"-\", OUT_PE_ONLY)\n",
        "print(\"-\", OUT_HOLD_BREAKOUTS)\n",
        "print(\"-\", OUT_REVERSALS)\n",
        "print(\"-\", OUT_IV_CRUSH)\n",
        "print(\"-\", OUT_PLOT)\n",
        "print(\"-\", OUT_HEATMAP)\n",
        "print(\"-\", OUT_AUTOTRADE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV9fBq7TB6lN",
        "outputId": "2dba422a-70a7-416f-a78d-bbfe9efccd29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Call columns: Previous_Call_ltp Current_Call_ltp Next_Call_ltp\n",
            "Using Put  columns: Previous_Put_ltp Current_Put_ltp Next_Put_ltp\n",
            "Detected call strikes: 8\n",
            "Detected put strikes : 8\n",
            "✔ All processing complete.\n",
            "Generated:\n",
            "- MERGED_CE_PE_FORECAST.csv\n",
            "- TOP_BUY_PUTS.csv\n",
            "- TOP_BUY_CALLS.csv\n",
            "- ALL_ACTIONS.csv\n",
            "- TOP_CE_ONLY.csv\n",
            "- TOP_PE_ONLY.csv\n",
            "- TOP_HOLD_BREAKOUTS.csv\n",
            "- REVERSALS.csv\n",
            "- IV_CRUSH.csv\n",
            "- SPOT_VS_MOMENTUM.png\n",
            "- PREMIUM_HEATMAP.png\n",
            "- AUTO_TRADE_SIGNALS.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# full_ce_pe_pipeline_use_ltp_enriched_final_restored.py\n",
        "\"\"\"\n",
        "Consolidated CE+PE pipeline using LTP where available.\n",
        "Produces a merged enriched CSV with all requested columns,\n",
        "plus auxiliary outputs: top calls/puts, alerts, reversals, IV-crush,\n",
        "heatmap, auto-trade export, CE-only / PE-only lists, hold-breakouts.\n",
        "\n",
        "Drop this file next to your flattened_snapshots.csv and run:\n",
        "    python full_ce_pe_pipeline_use_ltp_enriched_final_restored.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from collections import defaultdict, Counter\n",
        "from datetime import timedelta\n",
        "import math\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------------- CONFIG ---------------------- #\n",
        "INPUT_CSV = \"flattened_snapshots.csv\"\n",
        "\n",
        "OUT_MERGED = \"MERGED_CE_PE_FORECAST.csv\"\n",
        "OUT_TOP_PUTS = \"TOP_BUY_PUTS.csv\"\n",
        "OUT_TOP_CALLS = \"TOP_BUY_CALLS.csv\"\n",
        "OUT_ALL_ACTIONS = \"ALL_ACTIONS.csv\"\n",
        "\n",
        "OUT_CE_ONLY = \"TOP_CE_ONLY.csv\"\n",
        "OUT_PE_ONLY = \"TOP_PE_ONLY.csv\"\n",
        "OUT_HOLD_BREAKOUTS = \"TOP_HOLD_BREAKOUTS.csv\"\n",
        "\n",
        "OUT_REVERSALS = \"REVERSALS.csv\"\n",
        "OUT_IV_CRUSH = \"IV_CRUSH.csv\"\n",
        "\n",
        "OUT_PLOT = \"SPOT_VS_MOMENTUM.png\"\n",
        "OUT_HEATMAP = \"PREMIUM_HEATMAP.png\"\n",
        "OUT_AUTOTRADE = \"AUTO_TRADE_SIGNALS.json\"\n",
        "\n",
        "REVERSAL_WINDOW_MIN = 5\n",
        "REVERSAL_DROP_PCT = 12        # % drop from peak to last to mark reversal\n",
        "IV_CRUSH_DROP = 15            # % drop in IV to mark IV crush\n",
        "\n",
        "# ---------------------- LOAD ---------------------- #\n",
        "if not os.path.exists(INPUT_CSV):\n",
        "    print(f\"ERROR: input file not found: {INPUT_CSV}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Parse LTT as datetime if present; if not, we'll proceed without times.\n",
        "df = pd.read_csv(INPUT_CSV, low_memory=False)\n",
        "if \"LTT\" in df.columns:\n",
        "    try:\n",
        "        df[\"LTT\"] = pd.to_datetime(df[\"LTT\"])\n",
        "    except Exception:\n",
        "        # keep raw if parse fails\n",
        "        pass\n",
        "\n",
        "# ---------------------- COLUMN SELECTION (LTP preferred) ---------------------- #\n",
        "# Preferred LTP column names\n",
        "PREV_CALL_LTP = \"Previous_Call_ltp\"\n",
        "CURR_CALL_LTP = \"Current_Call_ltp\"\n",
        "NEXT_CALL_LTP = \"Next_Call_ltp\"\n",
        "\n",
        "PREV_PUT_LTP = \"Previous_Put_ltp\"\n",
        "CURR_PUT_LTP = \"Current_Put_ltp\"\n",
        "NEXT_PUT_LTP = \"Next_Put_ltp\"\n",
        "\n",
        "# Strike columns\n",
        "PREV_STR = \"Previous_Strikeprice\"\n",
        "CURR_STR = \"Current_Strikeprice\"\n",
        "NEXT_STR = \"Next_Strikeprice\"\n",
        "\n",
        "# fallback prefer _Premium if ltp missing\n",
        "def pick(col_ltp, col_prem):\n",
        "    if col_ltp in df.columns:\n",
        "        return col_ltp\n",
        "    if col_prem in df.columns:\n",
        "        return col_prem\n",
        "    return None\n",
        "\n",
        "prev_call_col = pick(PREV_CALL_LTP, \"Previous_Call_Premium\")\n",
        "curr_call_col = pick(CURR_CALL_LTP, \"Current_Call_Premium\")\n",
        "next_call_col = pick(NEXT_CALL_LTP, \"Next_Call_Premium\")\n",
        "\n",
        "prev_put_col = pick(PREV_PUT_LTP, \"Previous_Put_Premium\")\n",
        "curr_put_col = pick(CURR_PUT_LTP, \"Current_Put_Premium\")\n",
        "next_put_col = pick(NEXT_PUT_LTP, \"Next_Put_Premium\")\n",
        "\n",
        "print(\"Using Call columns:\", prev_call_col, curr_call_col, next_call_col)\n",
        "print(\"Using Put  columns:\", prev_put_col, curr_put_col, next_put_col)\n",
        "print(\"Using Strike columns:\", PREV_STR, CURR_STR, NEXT_STR)\n",
        "\n",
        "# ---------------------- BUILD TIMESERIES PER STRIKE ---------------------- #\n",
        "def build_series(str_col_prev, str_col_curr, str_col_next, val_prev, val_curr, val_next):\n",
        "    series = defaultdict(list)\n",
        "    for idx, row in df.iterrows():\n",
        "        t = row.get(\"LTT\", None)\n",
        "        triples = [\n",
        "            (str_col_prev, val_prev),\n",
        "            (str_col_curr, val_curr),\n",
        "            (str_col_next, val_next)\n",
        "        ]\n",
        "        for sc, vc in triples:\n",
        "            if sc is None or vc is None:\n",
        "                continue\n",
        "            if sc not in df.columns or vc not in df.columns:\n",
        "                continue\n",
        "            scv = row.get(sc)\n",
        "            vcv = row.get(vc)\n",
        "            if pd.isna(scv) or pd.isna(vcv):\n",
        "                continue\n",
        "            try:\n",
        "                s = int(scv)\n",
        "                p = float(vcv)\n",
        "            except Exception:\n",
        "                continue\n",
        "            # keep timestamp if present, else use index-based monotonic increasing fallback\n",
        "            if t is None or pd.isna(t):\n",
        "                # use index as proxy timestamp\n",
        "                t_use = pd.Timestamp(idx)\n",
        "            else:\n",
        "                t_use = t\n",
        "            series[s].append((t_use, p))\n",
        "    return series\n",
        "\n",
        "call_series = build_series(PREV_STR, CURR_STR, NEXT_STR, prev_call_col, curr_call_col, next_call_col)\n",
        "put_series  = build_series(PREV_STR, CURR_STR, NEXT_STR, prev_put_col, curr_put_col, next_put_col)\n",
        "\n",
        "print(\"Detected call strikes:\", len(call_series))\n",
        "print(\"Detected put strikes :\", len(put_series))\n",
        "\n",
        "# ---------------------- SUMMARY & FORECAST HELPERS ---------------------- #\n",
        "def summarize(pairs):\n",
        "    pairs_sorted = sorted(pairs, key=lambda x: x[0])\n",
        "    prices = [p for _, p in pairs_sorted]\n",
        "    if not prices:\n",
        "        return None\n",
        "    first = prices[0]\n",
        "    last = prices[-1]\n",
        "    peak = max(prices)\n",
        "    trough = min(prices)\n",
        "    abs_change = last - first\n",
        "    pct_change = (abs_change / first * 100) if first != 0 else np.nan\n",
        "    return {\n",
        "        \"first\": first,\n",
        "        \"last\": last,\n",
        "        \"peak\": peak,\n",
        "        \"trough\": trough,\n",
        "        \"abs_change\": abs_change,\n",
        "        \"pct_change\": pct_change,\n",
        "        \"n_obs\": len(prices),\n",
        "        \"series_sorted\": pairs_sorted\n",
        "    }\n",
        "\n",
        "def forecast_from_pct(last, pct):\n",
        "    \"\"\"Return ((5min_low,5min_high),(10min_low,10min_high))\"\"\"\n",
        "    if pd.isna(pct):\n",
        "        pct = 0.0\n",
        "    if pct >= 25:\n",
        "        return (last+15, last+35), (last+25, last+60)\n",
        "    if pct >= 8:\n",
        "        return (last+6, last+18), (last+12, last+30)\n",
        "    if pct > 0:\n",
        "        return (last+2, last+8), (last+5, last+15)\n",
        "    # neutral/slight negative\n",
        "    return (last-5, last+2), (last-8, last+5)\n",
        "\n",
        "# Build per-side summaries\n",
        "call_summary = {}\n",
        "for s, pairs in call_series.items():\n",
        "    st = summarize(pairs)\n",
        "    if st:\n",
        "        call_summary[s] = st\n",
        "\n",
        "put_summary = {}\n",
        "for s, pairs in put_series.items():\n",
        "    st = summarize(pairs)\n",
        "    if st:\n",
        "        put_summary[s] = st\n",
        "\n",
        "# All unique strikes from both sides\n",
        "all_strikes = sorted(set(list(call_summary.keys()) + list(put_summary.keys())))\n",
        "\n",
        "# ---------------------- BUILD MERGED ENRICHED ROWS ---------------------- #\n",
        "rows = []\n",
        "for s in all_strikes:\n",
        "    cs = call_summary.get(s)\n",
        "    ps = put_summary.get(s)\n",
        "\n",
        "    # n_obs unified\n",
        "    n_obs_call = int(cs[\"n_obs\"]) if cs else 0\n",
        "    n_obs_put  = int(ps[\"n_obs\"]) if ps else 0\n",
        "    n_obs = n_obs_call + n_obs_put\n",
        "\n",
        "    # choose first/last/peak/trough: prefer side with more observations (call over put if tie)\n",
        "    def prefer(key_call, key_put):\n",
        "        if cs and key_call in cs:\n",
        "            return cs[key_call]\n",
        "        if ps and key_put in ps:\n",
        "            return ps[key_put]\n",
        "        return np.nan\n",
        "\n",
        "    # However to be explicit use logic:\n",
        "    if n_obs_call >= n_obs_put and cs:\n",
        "        first_premium = cs[\"first\"]\n",
        "        last_premium  = cs[\"last\"]\n",
        "        peak_premium  = cs[\"peak\"]\n",
        "        trough_premium= cs[\"trough\"]\n",
        "        abs_change    = cs[\"abs_change\"]\n",
        "        pct_change    = cs[\"pct_change\"]\n",
        "        f5, f10 = forecast_from_pct(last_premium, pct_change)\n",
        "    elif ps:\n",
        "        first_premium = ps[\"first\"]\n",
        "        last_premium  = ps[\"last\"]\n",
        "        peak_premium  = ps[\"peak\"]\n",
        "        trough_premium= ps[\"trough\"]\n",
        "        abs_change    = ps[\"abs_change\"]\n",
        "        pct_change    = ps[\"pct_change\"]\n",
        "        f5, f10 = forecast_from_pct(last_premium, pct_change)\n",
        "    else:\n",
        "        first_premium = last_premium = peak_premium = trough_premium = abs_change = pct_change = np.nan\n",
        "        f5 = (np.nan, np.nan); f10 = (np.nan, np.nan)\n",
        "\n",
        "    # p5/p10 expected deltas\n",
        "    p5_expected_lo = (f5[0] - last_premium) if (not pd.isna(f5[0]) and not pd.isna(last_premium)) else np.nan\n",
        "    p5_expected_hi = (f5[1] - last_premium) if (not pd.isna(f5[1]) and not pd.isna(last_premium)) else np.nan\n",
        "    p10_expected_lo = (f10[0] - last_premium) if (not pd.isna(f10[0]) and not pd.isna(last_premium)) else np.nan\n",
        "    p10_expected_hi = (f10[1] - last_premium) if (not pd.isna(f10[1]) and not pd.isna(last_premium)) else np.nan\n",
        "\n",
        "    rows.append({\n",
        "        \"strike\": s,\n",
        "        \"Unnamed: 0\": (df[\"Unnamed: 0\"].iloc[0] if \"Unnamed: 0\" in df.columns else np.nan),\n",
        "        \"n_obs\": n_obs,\n",
        "        \"first_premium\": first_premium,\n",
        "        \"last_premium\": last_premium,\n",
        "        \"peak_premium\": peak_premium,\n",
        "        \"trough_premium\": trough_premium,\n",
        "        \"abs_change\": abs_change,\n",
        "        \"pct_change\": pct_change,\n",
        "        \"5min_low\": f5[0],\n",
        "        \"5min_high\": f5[1],\n",
        "        \"10min_low\": f10[0],\n",
        "        \"10min_high\": f10[1],\n",
        "        \"p5_expected_lo\": p5_expected_lo,\n",
        "        \"p5_expected_hi\": p5_expected_hi,\n",
        "        \"p10_expected_lo\": p10_expected_lo,\n",
        "        \"p10_expected_hi\": p10_expected_hi,\n",
        "        # placeholders for call/put details - attach below\n",
        "        \"n_obs_call\": n_obs_call,\n",
        "        \"n_obs_put\": n_obs_put,\n",
        "        \"first_call_ltp\": cs[\"first\"] if cs else np.nan,\n",
        "        \"last_call_ltp\": cs[\"last\"] if cs else np.nan,\n",
        "        \"peak_call_ltp\": cs[\"peak\"] if cs else np.nan,\n",
        "        \"trough_call_ltp\": cs[\"trough\"] if cs else np.nan,\n",
        "        \"abs_change_call\": cs[\"abs_change\"] if cs else np.nan,\n",
        "        \"pct_change_call\": cs[\"pct_change\"] if cs else np.nan,\n",
        "        \"first_put_ltp\": ps[\"first\"] if ps else np.nan,\n",
        "        \"last_put_ltp\": ps[\"last\"] if ps else np.nan,\n",
        "        \"peak_put_ltp\": ps[\"peak\"] if ps else np.nan,\n",
        "        \"trough_put_ltp\": ps[\"trough\"] if ps else np.nan,\n",
        "        \"abs_change_put\": ps[\"abs_change\"] if ps else np.nan,\n",
        "        \"pct_change_put\": ps[\"pct_change\"] if ps else np.nan,\n",
        "    })\n",
        "\n",
        "merged_df = pd.DataFrame(rows)\n",
        "\n",
        "# ---------------------- EXTRACT TAGS & MONEYFLOW FOR EACH STRIKE ---------------------- #\n",
        "tag_cols = [\"Previous_StrategyTag\",\"Current_StrategyTag\",\"Next_StrategyTag\"]\n",
        "call_money_cols = [\"Previous_CallMoneyFlow\",\"Current_CallMoneyFlow\",\"Next_CallMoneyFlow\",\n",
        "                   \"Previous_TotalcallMoneyFlow\",\"Current_TotalcallMoneyFlow\",\"Next_TotalcallMoneyFlow\"]\n",
        "put_money_cols  = [\"Previous_PutMoneyFlow\",\"Current_PutMoneyFlow\",\"Next_PutMoneyFlow\",\n",
        "                   \"Previous_TotalputMoneyFlow\",\"Current_TotalputMoneyFlow\",\"Next_TotalputMoneyFlow\"]\n",
        "\n",
        "# initialize\n",
        "strike_info = {int(s): {\"tags\":Counter(), \"call_moneyflow\":0.0, \"put_moneyflow\":0.0} for s in merged_df[\"strike\"].astype(int)}\n",
        "\n",
        "# iterate source df once and aggregate\n",
        "for idx, row in df.iterrows():\n",
        "    for sc in (PREV_STR, CURR_STR, NEXT_STR):\n",
        "        if sc not in df.columns:\n",
        "            continue\n",
        "        scval = row.get(sc)\n",
        "        if pd.isna(scval):\n",
        "            continue\n",
        "        try:\n",
        "            s = int(scval)\n",
        "        except Exception:\n",
        "            continue\n",
        "        if s not in strike_info:\n",
        "            continue\n",
        "        # tags\n",
        "        for col in tag_cols:\n",
        "            if col in df.columns:\n",
        "                v = row.get(col)\n",
        "                if isinstance(v, str) and v.strip():\n",
        "                    tokens = [t.strip() for t in v.replace(\"|\",\";\").split(\";\") if t.strip()]\n",
        "                    for t in tokens:\n",
        "                        strike_info[s][\"tags\"][t] += 1\n",
        "        # call moneyflow\n",
        "        for col in call_money_cols:\n",
        "            if col in df.columns:\n",
        "                try:\n",
        "                    v = row.get(col)\n",
        "                    if not pd.isna(v):\n",
        "                        strike_info[s][\"call_moneyflow\"] += float(v)\n",
        "                except Exception:\n",
        "                    pass\n",
        "        # put moneyflow\n",
        "        for col in put_money_cols:\n",
        "            if col in df.columns:\n",
        "                try:\n",
        "                    v = row.get(col)\n",
        "                    if not pd.isna(v):\n",
        "                        strike_info[s][\"put_moneyflow\"] += float(v)\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "def tags_to_text(s):\n",
        "    t = strike_info.get(int(s), {}).get(\"tags\", {})\n",
        "    if not t:\n",
        "        return \"\"\n",
        "    return \";\".join([f\"{k}:{v}\" for k,v in t.items()])\n",
        "\n",
        "merged_df[\"tags\"] = merged_df[\"strike\"].apply(lambda s: tags_to_text(s))\n",
        "merged_df[\"call_moneyflow\"] = merged_df[\"strike\"].apply(lambda s: strike_info.get(int(s), {}).get(\"call_moneyflow\", 0.0))\n",
        "merged_df[\"put_moneyflow\"] = merged_df[\"strike\"].apply(lambda s: strike_info.get(int(s), {}).get(\"put_moneyflow\", 0.0))\n",
        "\n",
        "# ---------------------- REASONS (derived) ---------------------- #\n",
        "def build_reasons(row):\n",
        "    reasons = []\n",
        "    tags_text = str(row.get(\"tags\",\"\"))\n",
        "    # tag based\n",
        "    if any(k.lower() in tags_text.lower() for k in [\"rsimacd\",\"rsi\",\"macd\"]):\n",
        "        reasons.append(\"RSI/MACD momentum\")\n",
        "    if \"VWAP\" in tags_text or \"vwap\" in tags_text:\n",
        "        reasons.append(\"VWAP divergence\")\n",
        "    if \"OI\" in tags_text or \"oi\" in tags_text:\n",
        "        reasons.append(\"OI support/resistance\")\n",
        "    # moneyflow\n",
        "    if row.get(\"call_moneyflow\",0) > 0:\n",
        "        reasons.append(\"Call net buying\")\n",
        "    if row.get(\"put_moneyflow\",0) > 0:\n",
        "        reasons.append(\"Put net buying\")\n",
        "    # pct-based\n",
        "    try:\n",
        "        pct = float(row.get(\"pct_change\", 0) or 0)\n",
        "        if pct > 10:\n",
        "            reasons.append(\"Strong premium move\")\n",
        "        elif pct > 3:\n",
        "            reasons.append(\"Moderate premium move\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    if not reasons:\n",
        "        reasons = [\"No strong signals\"]\n",
        "    # dedupe\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for r in reasons:\n",
        "        if r not in seen:\n",
        "            out.append(r)\n",
        "            seen.add(r)\n",
        "    return \"; \".join(out)\n",
        "\n",
        "merged_df[\"reasons\"] = merged_df.apply(build_reasons, axis=1)\n",
        "\n",
        "# ---------------------- RECOMMENDED ACTION (dual-side) ---------------------- #\n",
        "def decide_action(row):\n",
        "    call_pct = row.get(\"pct_change_call\")\n",
        "    put_pct = row.get(\"pct_change_put\")\n",
        "    call_pct = 0 if pd.isna(call_pct) else float(call_pct)\n",
        "    put_pct = 0 if pd.isna(put_pct) else float(put_pct)\n",
        "    tags = (row.get(\"tags\") or \"\").lower()\n",
        "\n",
        "    bull_boost = (\"call buying\" in tags) or (\"oi_support_call\" in tags) or (\"bull\" in tags)\n",
        "    bear_boost = (\"put buying\" in tags) or (\"call writing\" in tags) or (\"bear\" in tags)\n",
        "\n",
        "    # priority rules\n",
        "    if (put_pct > 8) or (put_pct > 5 and bear_boost):\n",
        "        return \"BUY_PUT\"\n",
        "    if (call_pct > 8) or (call_pct > 5 and bull_boost):\n",
        "        return \"BUY_CALL\"\n",
        "    return \"HOLD\"\n",
        "\n",
        "merged_df[\"recommended_action\"] = merged_df.apply(decide_action, axis=1)\n",
        "\n",
        "# ---------------------- HIGH-CONVICTION STATISTICS ---------------------- #\n",
        "hc_col = \"Current_IsHighConvictionSignal\"\n",
        "\n",
        "def compute_highconv_stats(strike, action):\n",
        "    total = 0\n",
        "    success = 0\n",
        "    if hc_col not in df.columns:\n",
        "        return 0, 0, None\n",
        "    # select rows where current strike matches and hc flag true\n",
        "    cond = (df.get(\"Current_Strikeprice\") == strike) & (df.get(hc_col) == True)\n",
        "    hc_rows = df[cond]\n",
        "    total = int(hc_rows.shape[0])\n",
        "    for idx, hr in hc_rows.iterrows():\n",
        "        t0 = hr.get(\"LTT\")\n",
        "        if pd.isna(t0):\n",
        "            continue\n",
        "        # pick premium column based on action preference\n",
        "        if action == \"BUY_PUT\" and curr_put_col in df.columns:\n",
        "            base_col = curr_put_col\n",
        "        elif action == \"BUY_CALL\" and curr_call_col in df.columns:\n",
        "            base_col = curr_call_col\n",
        "        else:\n",
        "            base_col = curr_call_col if curr_call_col in df.columns else curr_put_col if curr_put_col in df.columns else None\n",
        "        if base_col is None:\n",
        "            continue\n",
        "        p0 = hr.get(base_col)\n",
        "        if pd.isna(p0):\n",
        "            continue\n",
        "        # window 3 minutes forward\n",
        "        window = df[(df[\"LTT\"] >= t0) & (df[\"LTT\"] <= (t0 + timedelta(minutes=3)))]\n",
        "        if window.empty:\n",
        "            continue\n",
        "        try:\n",
        "            if window[base_col].max() > p0:\n",
        "                success += 1\n",
        "        except Exception:\n",
        "            pass\n",
        "    rate = (success/total) if total > 0 else None\n",
        "    return total, success, rate\n",
        "\n",
        "hc_totals = []\n",
        "hc_successes = []\n",
        "hc_rates = []\n",
        "for idx, r in merged_df.iterrows():\n",
        "    s = int(r[\"strike\"])\n",
        "    action = r.get(\"recommended_action\",\"HOLD\")\n",
        "    tot, succ, rate = compute_highconv_stats(s, action)\n",
        "    hc_totals.append(tot)\n",
        "    hc_successes.append(succ)\n",
        "    hc_rates.append(rate)\n",
        "\n",
        "merged_df[\"highconv_total\"] = hc_totals\n",
        "merged_df[\"highconv_success\"] = hc_successes\n",
        "merged_df[\"highconv_hit_rate\"] = hc_rates\n",
        "\n",
        "# add Current_Strikeprice column (explicit)\n",
        "merged_df[\"Current_Strikeprice\"] = merged_df[\"strike\"]\n",
        "\n",
        "# ---------------------- REVERSAL DETECTION ---------------------- #\n",
        "def detect_reversals_in_map(series_map, side_label):\n",
        "    out = []\n",
        "    for s, pairs in series_map.items():\n",
        "        sr = sorted(pairs, key=lambda x: x[0])\n",
        "        if len(sr) < 3:\n",
        "            continue\n",
        "        t_last = sr[-1][0]\n",
        "        # window lookback\n",
        "        window = [(t,p) for t,p in sr if t >= (t_last - timedelta(minutes=REVERSAL_WINDOW_MIN))]\n",
        "        if not window:\n",
        "            continue\n",
        "        prices = [p for _,p in window]\n",
        "        peak = max(prices)\n",
        "        last_price = prices[-1]\n",
        "        drop_pct = (peak - last_price) / peak * 100 if peak > 0 else 0\n",
        "        if drop_pct >= REVERSAL_DROP_PCT:\n",
        "            out.append({\n",
        "                \"strike\": s,\n",
        "                \"side\": side_label,\n",
        "                \"peak\": peak,\n",
        "                \"last\": last_price,\n",
        "                \"drop_pct\": drop_pct\n",
        "            })\n",
        "    return out\n",
        "\n",
        "rev_list = detect_reversals_in_map(call_series, \"CALL\") + detect_reversals_in_map(put_series, \"PUT\")\n",
        "rev_df = pd.DataFrame(rev_list)\n",
        "if not rev_df.empty:\n",
        "    rev_df.to_csv(OUT_REVERSALS, index=False)\n",
        "else:\n",
        "    # create empty file\n",
        "    pd.DataFrame(columns=[\"strike\",\"side\",\"peak\",\"last\",\"drop_pct\"]).to_csv(OUT_REVERSALS, index=False)\n",
        "\n",
        "# ---------------------- IV CRUSH DETECTION ---------------------- #\n",
        "iv_crush_events = []\n",
        "# iterate rows and check prev vs curr IV fields (existence optional)\n",
        "for idx, row in df.iterrows():\n",
        "    for prev_iv_col, curr_iv_col, side in [\n",
        "        (\"Previous_Call_IV\",\"Current_Call_IV\",\"CALL\"),\n",
        "        (\"Previous_Put_IV\",\"Current_Put_IV\",\"PUT\")\n",
        "    ]:\n",
        "        if prev_iv_col in df.columns and curr_iv_col in df.columns:\n",
        "            prev_iv = row.get(prev_iv_col)\n",
        "            curr_iv = row.get(curr_iv_col)\n",
        "            if pd.notna(prev_iv) and pd.notna(curr_iv) and prev_iv > 0:\n",
        "                drop_pct = (prev_iv - curr_iv) / prev_iv * 100\n",
        "                if drop_pct >= IV_CRUSH_DROP:\n",
        "                    iv_crush_events.append({\n",
        "                        \"LTT\": row.get(\"LTT\"),\n",
        "                        \"strike\": row.get(\"Current_Strikeprice\"),\n",
        "                        \"side\": side,\n",
        "                        \"prev_iv\": prev_iv,\n",
        "                        \"curr_iv\": curr_iv,\n",
        "                        \"drop_pct\": drop_pct\n",
        "                    })\n",
        "\n",
        "iv_crush_df = pd.DataFrame(iv_crush_events)\n",
        "if not iv_crush_df.empty:\n",
        "    iv_crush_df.to_csv(OUT_IV_CRUSH, index=False)\n",
        "else:\n",
        "    pd.DataFrame(columns=[\"LTT\",\"strike\",\"side\",\"prev_iv\",\"curr_iv\",\"drop_pct\"]).to_csv(OUT_IV_CRUSH, index=False)\n",
        "\n",
        "# ---------------------- HEATMAP (CALL/PUT pct change) ---------------------- #\n",
        "# Prepare simple heatmap matrix: rows = strikes, cols = [pct_change_call, pct_change_put]\n",
        "heatmap_df = merged_df[[\"pct_change_call\",\"pct_change_put\"]].fillna(0)\n",
        "if not heatmap_df.empty:\n",
        "    plt.figure(figsize=(8, max(3, len(heatmap_df)/4)))\n",
        "    plt.imshow(heatmap_df.values, aspect='auto', interpolation='nearest')\n",
        "    plt.colorbar(label=\"Premium % Change\")\n",
        "    plt.title(\"CALL/PUT Premium % Change Heatmap (rows=strikes)\")\n",
        "    plt.ylabel(\"strike index (not price)\")\n",
        "    plt.xlabel(\"0=CE_pct_change, 1=PE_pct_change\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUT_HEATMAP, dpi=150)\n",
        "    plt.close()\n",
        "else:\n",
        "    # produce an empty placeholder\n",
        "    plt.figure(figsize=(6,2)); plt.text(0.5,0.5,\"No data\"); plt.axis('off'); plt.savefig(OUT_HEATMAP); plt.close()\n",
        "\n",
        "# ---------------------- SPOT VS MOMENTUM PLOT ---------------------- #\n",
        "last_spot = df[\"SpotPrice\"].dropna().iloc[-1] if \"SpotPrice\" in df.columns and not df[\"SpotPrice\"].dropna().empty else np.nan\n",
        "avg_ce_pct = merged_df[\"pct_change_call\"].replace([np.inf,-np.inf],np.nan).dropna().mean() if \"pct_change_call\" in merged_df.columns else np.nan\n",
        "avg_pe_pct = merged_df[\"pct_change_put\"].replace([np.inf,-np.inf],np.nan).dropna().mean() if \"pct_change_put\" in merged_df.columns else np.nan\n",
        "\n",
        "vals = [last_spot if not pd.isna(last_spot) else 0, avg_ce_pct if not pd.isna(avg_ce_pct) else 0, avg_pe_pct if not pd.isna(avg_pe_pct) else 0]\n",
        "labels = [\"Spot Last\",\"Avg CE %\",\"Avg PE %\"]\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.title(\"Spot (last) vs Avg CE/PE %change\")\n",
        "plt.bar(labels, vals)\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUT_PLOT, dpi=150)\n",
        "plt.close()\n",
        "\n",
        "# ---------------------- AUTO-TRADE EXPORT ---------------------- #\n",
        "auto_signals = []\n",
        "for idx, r in merged_df.iterrows():\n",
        "    action = r.get(\"recommended_action\",\"HOLD\")\n",
        "    if action in (\"BUY_CALL\",\"BUY_PUT\"):\n",
        "        strength = float(r.get(\"pct_change_call\",0) or r.get(\"pct_change_put\",0) or 0)\n",
        "        auto_signals.append({\n",
        "            \"strike\": int(r[\"strike\"]),\n",
        "            \"action\": action,\n",
        "            \"strength\": strength,\n",
        "            \"reason\": r.get(\"reasons\",\"\"),\n",
        "            \"tags\": r.get(\"tags\",\"\")\n",
        "        })\n",
        "with open(OUT_AUTOTRADE, \"w\") as fh:\n",
        "    json.dump(auto_signals, fh, indent=2, default=str)\n",
        "\n",
        "# ---------------------- WRITE OUTPUTS: main merged + slices ---------------------- #\n",
        "# Save merged enriched table with all requested columns\n",
        "# Ensure column order to match user's expectation\n",
        "cols_order = [\n",
        "    \"strike\",\"Unnamed: 0\",\"n_obs\",\"n_obs_call\",\"n_obs_put\",\n",
        "    \"first_premium\",\"last_premium\",\"peak_premium\",\"trough_premium\",\n",
        "    \"abs_change\",\"pct_change\",\n",
        "    \"5min_low\",\"5min_high\",\"10min_low\",\"10min_high\",\n",
        "    \"p5_expected_lo\",\"p5_expected_hi\",\"p10_expected_lo\",\"p10_expected_hi\",\n",
        "    \"call_moneyflow\",\"put_moneyflow\",\n",
        "    \"tags\",\"reasons\",\"recommended_action\",\n",
        "    \"highconv_total\",\"highconv_success\",\"highconv_hit_rate\",\n",
        "    \"Current_Strikeprice\",\n",
        "    # add call/put detailed columns too\n",
        "    \"first_call_ltp\",\"last_call_ltp\",\"peak_call_ltp\",\"trough_call_ltp\",\"abs_change_call\",\"pct_change_call\",\n",
        "    \"first_put_ltp\",\"last_put_ltp\",\"peak_put_ltp\",\"trough_put_ltp\",\"abs_change_put\",\"pct_change_put\"\n",
        "]\n",
        "\n",
        "# keep only existing columns from the order (some may be missing)\n",
        "cols_existing = [c for c in cols_order if c in merged_df.columns]\n",
        "# append any other columns to preserve details\n",
        "other_cols = [c for c in merged_df.columns if c not in cols_existing]\n",
        "final_cols = cols_existing + other_cols\n",
        "\n",
        "merged_df.to_csv(OUT_MERGED, index=False, columns=final_cols)\n",
        "\n",
        "# Top BUY_PUTS (by pct_change_put)\n",
        "top_puts = merged_df[merged_df[\"recommended_action\"]==\"BUY_PUT\"].copy()\n",
        "if not top_puts.empty and \"pct_change_put\" in top_puts.columns:\n",
        "    top_puts = top_puts.sort_values(\"pct_change_put\", ascending=False).head(200)\n",
        "top_puts.to_csv(OUT_TOP_PUTS, index=False)\n",
        "\n",
        "# Top BUY_CALLS\n",
        "top_calls = merged_df[merged_df[\"recommended_action\"]==\"BUY_CALL\"].copy()\n",
        "if not top_calls.empty and \"pct_change_call\" in top_calls.columns:\n",
        "    top_calls = top_calls.sort_values(\"pct_change_call\", ascending=False).head(200)\n",
        "top_calls.to_csv(OUT_TOP_CALLS, index=False)\n",
        "\n",
        "# All actions sorted by strength (max of CE/PE pct)\n",
        "def compute_strength(r):\n",
        "    try:\n",
        "        a = r.get(\"pct_change_call\", 0) or 0\n",
        "        b = r.get(\"pct_change_put\", 0) or 0\n",
        "        return max(a, b)\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "merged_df[\"strength\"] = merged_df.apply(compute_strength, axis=1)\n",
        "merged_df.sort_values(\"strength\", ascending=False).to_csv(OUT_ALL_ACTIONS, index=False)\n",
        "\n",
        "# CE-only / PE-only tables\n",
        "if \"pct_change_call\" in merged_df.columns:\n",
        "    merged_df[merged_df[\"pct_change_call\"].notna()].to_csv(OUT_CE_ONLY, index=False)\n",
        "else:\n",
        "    pd.DataFrame().to_csv(OUT_CE_ONLY, index=False)\n",
        "if \"pct_change_put\" in merged_df.columns:\n",
        "    merged_df[merged_df[\"pct_change_put\"].notna()].to_csv(OUT_PE_ONLY, index=False)\n",
        "else:\n",
        "    pd.DataFrame().to_csv(OUT_PE_ONLY, index=False)\n",
        "\n",
        "# HOLD breakout candidates\n",
        "hold_df = merged_df[merged_df[\"recommended_action\"]==\"HOLD\"].copy()\n",
        "# define breakout_score as sum of positive pct changes\n",
        "hold_df[\"breakout_score\"] = (hold_df.get(\"pct_change_call\",0).clip(lower=0).fillna(0)\n",
        "                             + hold_df.get(\"pct_change_put\",0).clip(lower=0).fillna(0))\n",
        "hold_df.sort_values(\"breakout_score\", ascending=False).to_csv(OUT_HOLD_BREAKOUTS, index=False)\n",
        "\n",
        "print(\"✔ Pipeline complete. Files generated:\")\n",
        "for f in [OUT_MERGED, OUT_TOP_PUTS, OUT_TOP_CALLS, OUT_ALL_ACTIONS,\n",
        "          OUT_CE_ONLY, OUT_PE_ONLY, OUT_HOLD_BREAKOUTS,\n",
        "          OUT_REVERSALS, OUT_IV_CRUSH, OUT_PLOT, OUT_HEATMAP, OUT_AUTOTRADE]:\n",
        "    print(\"-\", f)\n",
        "\n",
        "# Show columns included in the merged output for immediate verification\n",
        "print(\"\\nMerged CSV columns (sample):\")\n",
        "print(list(merged_df.columns[:50]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teIgxnyXDQ_b",
        "outputId": "3a0722e2-bde5-40bc-dc00-1a1967c1d8e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Call columns: Previous_Call_ltp Current_Call_ltp Next_Call_ltp\n",
            "Using Put  columns: Previous_Put_ltp Current_Put_ltp Next_Put_ltp\n",
            "Using Strike columns: Previous_Strikeprice Current_Strikeprice Next_Strikeprice\n",
            "Detected call strikes: 8\n",
            "Detected put strikes : 8\n",
            "✔ Pipeline complete. Files generated:\n",
            "- MERGED_CE_PE_FORECAST.csv\n",
            "- TOP_BUY_PUTS.csv\n",
            "- TOP_BUY_CALLS.csv\n",
            "- ALL_ACTIONS.csv\n",
            "- TOP_CE_ONLY.csv\n",
            "- TOP_PE_ONLY.csv\n",
            "- TOP_HOLD_BREAKOUTS.csv\n",
            "- REVERSALS.csv\n",
            "- IV_CRUSH.csv\n",
            "- SPOT_VS_MOMENTUM.png\n",
            "- PREMIUM_HEATMAP.png\n",
            "- AUTO_TRADE_SIGNALS.json\n",
            "\n",
            "Merged CSV columns (sample):\n",
            "['strike', 'Unnamed: 0', 'n_obs', 'first_premium', 'last_premium', 'peak_premium', 'trough_premium', 'abs_change', 'pct_change', '5min_low', '5min_high', '10min_low', '10min_high', 'p5_expected_lo', 'p5_expected_hi', 'p10_expected_lo', 'p10_expected_hi', 'n_obs_call', 'n_obs_put', 'first_call_ltp', 'last_call_ltp', 'peak_call_ltp', 'trough_call_ltp', 'abs_change_call', 'pct_change_call', 'first_put_ltp', 'last_put_ltp', 'peak_put_ltp', 'trough_put_ltp', 'abs_change_put', 'pct_change_put', 'tags', 'call_moneyflow', 'put_moneyflow', 'reasons', 'recommended_action', 'highconv_total', 'highconv_success', 'highconv_hit_rate', 'Current_Strikeprice', 'strength']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "realtime_ce_pe_engine.py\n",
        "\n",
        "Tail a live text file of JSON \"snapshots\" (one JSON per line or a JSON list),\n",
        "parse incremental snapshots, maintain a sliding-window in-memory DataFrame,\n",
        "compute per-strike CE/PE summaries & forecasts using LTP when available,\n",
        "detect signals (BUY_CALL / BUY_PUT), reversals, IV crush, and export\n",
        "AUTO_TRADE_SIGNALS.json continuously.\n",
        "\n",
        "No CSVs are written (except optional historical dump). Uses minimal pandas,\n",
        "keeps memory bounded by sliding window.\n",
        "\n",
        "Configurable parameters at top.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import signal\n",
        "from collections import defaultdict, Counter, deque\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Optional, List, Dict, Any\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------- CONFIG ---------------------- #\n",
        "INPUT_FILE = \"14112025_BANK_PNL.txt\"      # path to live appended snapshots (text)\n",
        "REFRESH_SECONDS = 1.0                     # poll interval\n",
        "WINDOW_MINUTES = 15                       # sliding window size for analysis\n",
        "START_FROM_END = True                     # if True, ignore historical lines and start tailing from EOF\n",
        "WRITE_OUTPUT_JSON = True                  # write AUTO_TRADE_SIGNALS.json and LATEST_MERGED.json\n",
        "OUT_AUTOTRADE = \"AUTO_TRADE_SIGNALS.json\"\n",
        "OUT_LATEST = \"LATEST_MERGED.json\"\n",
        "ALERT_LOG = None                          # path to an alert logfile or None to only print to stdout\n",
        "IV_CRUSH_DROP = 15.0                      # percent IV drop threshold\n",
        "REVERSAL_DROP_PCT = 12.0                  # reversal detection threshold (drop from peak)\n",
        "REVERSAL_WINDOW_MIN = 5                   # look-back window for reversal check (minutes)\n",
        "MAX_SNAPSHOTS_STORE = 10000               # maximum snapshots to keep in memory (safety)\n",
        "\n",
        "# ---------------------- GLOBALS ---------------------- #\n",
        "running = True\n",
        "\n",
        "# ---------------------- HELPERS ---------------------- #\n",
        "def graceful_exit(signum, frame):\n",
        "    global running\n",
        "    running = False\n",
        "    print(\"\\nReceived exit signal. Shutting down...\")\n",
        "\n",
        "signal.signal(signal.SIGINT, graceful_exit)\n",
        "signal.signal(signal.SIGTERM, graceful_exit)\n",
        "\n",
        "def parse_line_json(line: str) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Parse a single line that is either:\n",
        "      - a JSON object\n",
        "      - or a wrapper object whose 'Current' key is a JSON string\n",
        "    Returns flattened dict (top-level + flattened Current.Previous/Current/Next blocks) similar to prior code.\n",
        "    \"\"\"\n",
        "    line = line.strip()\n",
        "    if not line:\n",
        "        return None\n",
        "    try:\n",
        "        obj = json.loads(line)\n",
        "    except Exception:\n",
        "        # Not a pure JSON object per line — attempt to find {...} inside line\n",
        "        try:\n",
        "            start = line.index('{')\n",
        "            obj = json.loads(line[start:])\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    # Flatten top-level except nested 'Current' block handled below\n",
        "    flat = {}\n",
        "    for k, v in obj.items():\n",
        "        if k != \"Current\":\n",
        "            flat[k] = v\n",
        "\n",
        "    if \"Current\" in obj:\n",
        "        # Current might be a stringified JSON or a dict\n",
        "        curr_raw = obj[\"Current\"]\n",
        "        if isinstance(curr_raw, str):\n",
        "            try:\n",
        "                curr = json.loads(curr_raw)\n",
        "            except Exception:\n",
        "                curr = {}\n",
        "        elif isinstance(curr_raw, dict):\n",
        "            curr = curr_raw\n",
        "        else:\n",
        "            curr = {}\n",
        "\n",
        "        for section in (\"Previous\", \"Current\", \"Next\"):\n",
        "            if section in curr and isinstance(curr[section], dict):\n",
        "                for key, val in curr[section].items():\n",
        "                    flat[f\"{section}_{key}\"] = val\n",
        "            else:\n",
        "                # leave placeholders? skip to keep lean\n",
        "                pass\n",
        "\n",
        "    return flat\n",
        "\n",
        "# ---------------------- TAILING FILE ---------------------- #\n",
        "def tail_file(path, start_from_end=True):\n",
        "    \"\"\"\n",
        "    Generator yielding new lines appended to the file.\n",
        "    If start_from_end True, begin reading from EOF.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Input file not found: {path}\")\n",
        "    f = open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\")\n",
        "    if start_from_end:\n",
        "        f.seek(0, os.SEEK_END)\n",
        "    else:\n",
        "        f.seek(0)\n",
        "    try:\n",
        "        while running:\n",
        "            where = f.tell()\n",
        "            line = f.readline()\n",
        "            if not line:\n",
        "                time.sleep(REFRESH_SECONDS)\n",
        "                f.seek(where)\n",
        "            else:\n",
        "                yield line\n",
        "    finally:\n",
        "        try:\n",
        "            f.close()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "# ---------------------- IN-MEMORY STORE ---------------------- #\n",
        "class SnapshotWindow:\n",
        "    \"\"\"\n",
        "    Maintain a sliding window of parsed flattened snapshots (list of dicts).\n",
        "    Provide helper to convert to DataFrame for analysis.\n",
        "    \"\"\"\n",
        "    def __init__(self, window_minutes: int = WINDOW_MINUTES, max_snapshots: int = MAX_SNAPSHOTS_STORE):\n",
        "        self.window_minutes = window_minutes\n",
        "        self.max_snapshots = max_snapshots\n",
        "        self.store = deque()  # each item: (timestamp (pd.Timestamp or datetime), flat_dict)\n",
        "\n",
        "    def append(self, flat: Dict[str,Any]):\n",
        "        # find timestamp\n",
        "        t = flat.get(\"LTT\") or flat.get(\"ltt\") or flat.get(\"time\") or None\n",
        "        if isinstance(t, str):\n",
        "            try:\n",
        "                t = pd.to_datetime(t)\n",
        "            except Exception:\n",
        "                t = pd.Timestamp.now()\n",
        "        elif t is None:\n",
        "            t = pd.Timestamp.now()\n",
        "        elif not isinstance(t, (pd.Timestamp, datetime)):\n",
        "            try:\n",
        "                t = pd.to_datetime(t)\n",
        "            except Exception:\n",
        "                t = pd.Timestamp.now()\n",
        "        # push\n",
        "        self.store.append((t, flat))\n",
        "        # enforce max length\n",
        "        while len(self.store) > self.max_snapshots:\n",
        "            self.store.popleft()\n",
        "        # prune by time window\n",
        "        self.prune_old()\n",
        "\n",
        "    def prune_old(self):\n",
        "        if not self.store:\n",
        "            return\n",
        "        cutoff = pd.Timestamp.now() - pd.Timedelta(minutes=self.window_minutes)\n",
        "        while self.store and self.store[0][0] < cutoff:\n",
        "            self.store.popleft()\n",
        "\n",
        "    def to_dataframe(self) -> pd.DataFrame:\n",
        "        if not self.store:\n",
        "            return pd.DataFrame()\n",
        "        rows = []\n",
        "        for t, flat in self.store:\n",
        "            r = dict(flat)\n",
        "            r[\"LTT\"] = t\n",
        "            rows.append(r)\n",
        "        df = pd.DataFrame(rows)\n",
        "        # coerce types lazily\n",
        "        return df\n",
        "\n",
        "# ---------------------- ANALYSIS / SIGNAL LOGIC ---------------------- #\n",
        "def build_series_from_df(df: pd.DataFrame, prev_call_col, curr_call_col, next_call_col,\n",
        "                         prev_put_col, curr_put_col, next_put_col,\n",
        "                         prev_str=\"Previous_Strikeprice\", curr_str=\"Current_Strikeprice\", next_str=\"Next_Strikeprice\"):\n",
        "    \"\"\"\n",
        "    Returns two dicts: call_series[strike] = [(ts,p), ...], put_series likewise.\n",
        "    Uses LTP columns if available (pass correct column names).\n",
        "    \"\"\"\n",
        "    call_series = defaultdict(list)\n",
        "    put_series = defaultdict(list)\n",
        "    if df.empty:\n",
        "        return call_series, put_series\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        t = row.get(\"LTT\") or idx\n",
        "        for sc, pc in ((prev_str, prev_call_col), (curr_str, curr_call_col), (next_str, next_call_col)):\n",
        "            if sc in df.columns and pc and pc in df.columns:\n",
        "                scv = row.get(sc)\n",
        "                pcv = row.get(pc)\n",
        "                if pd.isna(scv) or pd.isna(pcv):\n",
        "                    continue\n",
        "                try:\n",
        "                    s = int(scv)\n",
        "                    p = float(pcv)\n",
        "                except Exception:\n",
        "                    continue\n",
        "                call_series[s].append((t, p))\n",
        "        for sc, pp in ((prev_str, prev_put_col), (curr_str, curr_put_col), (next_str, next_put_col)):\n",
        "            if sc in df.columns and pp and pp in df.columns:\n",
        "                scv = row.get(sc)\n",
        "                ppv = row.get(pp)\n",
        "                if pd.isna(scv) or pd.isna(ppv):\n",
        "                    continue\n",
        "                try:\n",
        "                    s = int(scv)\n",
        "                    p = float(ppv)\n",
        "                except Exception:\n",
        "                    continue\n",
        "                put_series[s].append((t, p))\n",
        "    return call_series, put_series\n",
        "\n",
        "def summarize_series(pairs):\n",
        "    \"\"\"\n",
        "    pairs: [(ts,p),...]\n",
        "    returns dict with first,last,peak,trough,abs_change,pct_change,n_obs,series_sorted\n",
        "    \"\"\"\n",
        "    if not pairs:\n",
        "        return None\n",
        "    sr = sorted(pairs, key=lambda x: x[0])\n",
        "    prices = [p for _,p in sr]\n",
        "    first = prices[0]\n",
        "    last = prices[-1]\n",
        "    peak = max(prices)\n",
        "    trough = min(prices)\n",
        "    abs_chg = last - first\n",
        "    pct_chg = (abs_chg / first * 100) if first != 0 else np.nan\n",
        "    return {\n",
        "        \"first\": first, \"last\": last, \"peak\": peak, \"trough\": trough,\n",
        "        \"abs_change\": abs_chg, \"pct_change\": pct_chg, \"n_obs\": len(prices),\n",
        "        \"series_sorted\": sr\n",
        "    }\n",
        "\n",
        "def forecast_from_pct(last, pct):\n",
        "    if pd.isna(pct):\n",
        "        pct = 0.0\n",
        "    if pct >= 25:\n",
        "        return (last+15, last+35), (last+25, last+60)\n",
        "    if pct >= 8:\n",
        "        return (last+6, last+18), (last+12, last+30)\n",
        "    if pct > 0:\n",
        "        return (last+2, last+8), (last+5, last+15)\n",
        "    return (last-5, last+2), (last-8, last+5)\n",
        "\n",
        "def aggregate_tags_and_mf(df: pd.DataFrame, strikes: List[int]):\n",
        "    \"\"\"\n",
        "    Aggregate tags & moneyflows for each strike present in strikes.\n",
        "    Returns dict strike -> {\"tags\":Counter, \"call_mf\":float, \"put_mf\":float}\n",
        "    \"\"\"\n",
        "    out = {int(s): {\"tags\": Counter(), \"call_mf\":0.0, \"put_mf\":0.0} for s in strikes}\n",
        "    tag_cols = [\"Previous_StrategyTag\",\"Current_StrategyTag\",\"Next_StrategyTag\"]\n",
        "    call_mflow_cols = [\"Previous_CallMoneyFlow\",\"Current_CallMoneyFlow\",\"Next_CallMoneyFlow\",\n",
        "                       \"Previous_TotalcallMoneyFlow\",\"Current_TotalcallMoneyFlow\",\"Next_TotalcallMoneyFlow\"]\n",
        "    put_mflow_cols  = [\"Previous_PutMoneyFlow\",\"Current_PutMoneyFlow\",\"Next_PutMoneyFlow\",\n",
        "                       \"Previous_TotalputMoneyFlow\",\"Current_TotalputMoneyFlow\",\"Next_TotalputMoneyFlow\"]\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        for sc in (\"Previous_Strikeprice\",\"Current_Strikeprice\",\"Next_Strikeprice\"):\n",
        "            if sc not in df.columns:\n",
        "                continue\n",
        "            scv = row.get(sc)\n",
        "            if pd.isna(scv):\n",
        "                continue\n",
        "            try:\n",
        "                s = int(scv)\n",
        "            except Exception:\n",
        "                continue\n",
        "            if s not in out:\n",
        "                continue\n",
        "            # tags\n",
        "            for tc in tag_cols:\n",
        "                if tc in df.columns:\n",
        "                    v = row.get(tc)\n",
        "                    if isinstance(v, str) and v.strip():\n",
        "                        tokens = [t.strip() for t in v.replace(\"|\",\";\").split(\";\") if t.strip()]\n",
        "                        for t in tokens:\n",
        "                            out[s][\"tags\"][t] += 1\n",
        "            # moneyflow\n",
        "            for cm in call_mflow_cols:\n",
        "                if cm in df.columns:\n",
        "                    val = row.get(cm)\n",
        "                    if not pd.isna(val):\n",
        "                        try:\n",
        "                            out[s][\"call_mf\"] += float(val)\n",
        "                        except:\n",
        "                            pass\n",
        "            for pm in put_mflow_cols:\n",
        "                if pm in df.columns:\n",
        "                    val = row.get(pm)\n",
        "                    if not pd.isna(val):\n",
        "                        try:\n",
        "                            out[s][\"put_mf\"] += float(val)\n",
        "                        except:\n",
        "                            pass\n",
        "    return out\n",
        "\n",
        "def decide_action_from_row(row):\n",
        "    \"\"\"\n",
        "    Decide BUY_CALL / BUY_PUT / HOLD given pct_change_call/put and tags string\n",
        "    \"\"\"\n",
        "    call_pct = row.get(\"pct_change_call\") if pd.notna(row.get(\"pct_change_call\")) else 0\n",
        "    put_pct  = row.get(\"pct_change_put\")  if pd.notna(row.get(\"pct_change_put\"))  else 0\n",
        "    tags = (row.get(\"tags\") or \"\").lower()\n",
        "    bull_boost = (\"call buying\" in tags) or (\"oi_support_call\" in tags) or (\"bull\" in tags)\n",
        "    bear_boost = (\"put buying\" in tags) or (\"call writing\" in tags) or (\"bear\" in tags)\n",
        "\n",
        "    if (put_pct > 8) or (put_pct > 5 and bear_boost):\n",
        "        return \"BUY_PUT\"\n",
        "    if (call_pct > 8) or (call_pct > 5 and bull_boost):\n",
        "        return \"BUY_CALL\"\n",
        "    return \"HOLD\"\n",
        "\n",
        "# ---------------------- MAIN ANALYSIS PER TICK ---------------------- #\n",
        "def analyze_window(window: SnapshotWindow):\n",
        "    \"\"\"\n",
        "    Convert stored snapshots to DataFrame, run pipeline, return merged table as list-of-dict and signals list.\n",
        "    \"\"\"\n",
        "    df = window.to_dataframe()\n",
        "    # pick LTP columns with fallback to Premium\n",
        "    def pick(df, ltp, prem):\n",
        "        return ltp if ltp in df.columns else (prem if prem in df.columns else None)\n",
        "\n",
        "    prev_call_col = pick(df, \"Previous_Call_ltp\", \"Previous_Call_Premium\")\n",
        "    curr_call_col = pick(df, \"Current_Call_ltp\", \"Current_Call_Premium\")\n",
        "    next_call_col = pick(df, \"Next_Call_ltp\", \"Next_Call_Premium\")\n",
        "\n",
        "    prev_put_col = pick(df, \"Previous_Put_ltp\", \"Previous_Put_Premium\")\n",
        "    curr_put_col = pick(df, \"Current_Put_ltp\", \"Current_Put_Premium\")\n",
        "    next_put_col = pick(df, \"Next_Put_ltp\", \"Next_Put_Premium\")\n",
        "\n",
        "    # Build series\n",
        "    call_series, put_series = build_series_from_df(df, prev_call_col, curr_call_col, next_call_col,\n",
        "                                                   prev_put_col, curr_put_col, next_put_col)\n",
        "\n",
        "    all_strikes = sorted(set(list(call_series.keys()) + list(put_series.keys())))\n",
        "    # Summarize per strike\n",
        "    merged_rows = []\n",
        "    for s in all_strikes:\n",
        "        cs = summarize_series(call_series.get(s, []))\n",
        "        ps = summarize_series(put_series.get(s, []))\n",
        "        # choose preferred side: more observations wins\n",
        "        n_call = cs[\"n_obs\"] if cs else 0\n",
        "        n_put  = ps[\"n_obs\"] if ps else 0\n",
        "        if n_call >= n_put and cs:\n",
        "            first_p = cs[\"first\"]; last_p = cs[\"last\"]; peak = cs[\"peak\"]; trough = cs[\"trough\"]\n",
        "            abs_chg = cs[\"abs_change\"]; pct = cs[\"pct_change\"]\n",
        "            f5, f10 = forecast_from_pct(last_p, pct)\n",
        "            # attach call-specific columns\n",
        "            row = {\n",
        "                \"strike\": int(s),\n",
        "                \"n_obs\": n_call + n_put,\n",
        "                \"n_obs_call\": n_call, \"n_obs_put\": n_put,\n",
        "                \"first_premium\": first_p, \"last_premium\": last_p, \"peak_premium\": peak, \"trough_premium\": trough,\n",
        "                \"abs_change\": abs_chg, \"pct_change\": pct,\n",
        "                \"5min_low\": f5[0], \"5min_high\": f5[1], \"10min_low\": f10[0], \"10min_high\": f10[1],\n",
        "                \"first_call_ltp\": cs[\"first\"], \"last_call_ltp\": cs[\"last\"], \"peak_call_ltp\": cs[\"peak\"], \"trough_call_ltp\": cs[\"trough\"],\n",
        "                \"abs_change_call\": cs[\"abs_change\"], \"pct_change_call\": cs[\"pct_change\"],\n",
        "                \"first_put_ltp\": ps[\"first\"] if ps else np.nan, \"last_put_ltp\": ps[\"last\"] if ps else np.nan,\n",
        "                \"pct_change_put\": ps[\"pct_change\"] if ps else np.nan\n",
        "            }\n",
        "        elif ps:\n",
        "            first_p = ps[\"first\"]; last_p = ps[\"last\"]; peak = ps[\"peak\"]; trough = ps[\"trough\"]\n",
        "            abs_chg = ps[\"abs_change\"]; pct = ps[\"pct_change\"]\n",
        "            f5, f10 = forecast_from_pct(last_p, pct)\n",
        "            row = {\n",
        "                \"strike\": int(s),\n",
        "                \"n_obs\": n_call + n_put,\n",
        "                \"n_obs_call\": n_call, \"n_obs_put\": n_put,\n",
        "                \"first_premium\": first_p, \"last_premium\": last_p, \"peak_premium\": peak, \"trough_premium\": trough,\n",
        "                \"abs_change\": abs_chg, \"pct_change\": pct,\n",
        "                \"5min_low\": f5[0], \"5min_high\": f5[1], \"10min_low\": f10[0], \"10min_high\": f10[1],\n",
        "                \"first_put_ltp\": ps[\"first\"], \"last_put_ltp\": ps[\"last\"], \"peak_put_ltp\": ps[\"peak\"], \"trough_put_ltp\": ps[\"trough\"],\n",
        "                \"abs_change_put\": ps[\"abs_change\"], \"pct_change_put\": ps[\"pct_change\"],\n",
        "                \"first_call_ltp\": cs[\"first\"] if cs else np.nan, \"last_call_ltp\": cs[\"last\"] if cs else np.nan,\n",
        "                \"pct_change_call\": cs[\"pct_change\"] if cs else np.nan\n",
        "            }\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        merged_rows.append(row)\n",
        "\n",
        "    if not merged_rows:\n",
        "        return [], []\n",
        "\n",
        "    merged_df = pd.DataFrame(merged_rows)\n",
        "    # aggregate tags & moneyflow\n",
        "    strike_info = aggregate_tags_and_mf(df, merged_df[\"strike\"].tolist())\n",
        "    merged_df[\"tags\"] = merged_df[\"strike\"].apply(lambda s: \";\".join([f\"{k}:{v}\" for k,v in strike_info[int(s)][\"tags\"].items()]) if strike_info[int(s)][\"tags\"] else \"\")\n",
        "    merged_df[\"call_moneyflow\"] = merged_df[\"strike\"].apply(lambda s: strike_info[int(s)][\"call_mf\"])\n",
        "    merged_df[\"put_moneyflow\"]  = merged_df[\"strike\"].apply(lambda s: strike_info[int(s)][\"put_mf\"])\n",
        "\n",
        "    # build reasons\n",
        "    def build_reasons(row):\n",
        "        reasons = []\n",
        "        tags_text = str(row.get(\"tags\",\"\"))\n",
        "        if any(k in tags_text.lower() for k in [\"rsi\",\"macd\"]):\n",
        "            reasons.append(\"RSI/MACD momentum\")\n",
        "        if \"vwap\" in tags_text.lower():\n",
        "            reasons.append(\"VWAP divergence\")\n",
        "        if \"oi\" in tags_text.lower():\n",
        "            reasons.append(\"OI support/resistance\")\n",
        "        if row.get(\"call_moneyflow\",0) > 0:\n",
        "            reasons.append(\"Call net buying\")\n",
        "        if row.get(\"put_moneyflow\",0) > 0:\n",
        "            reasons.append(\"Put net buying\")\n",
        "        pct = row.get(\"pct_change\", 0) or 0\n",
        "        try:\n",
        "            if pct > 10:\n",
        "                reasons.append(\"Strong premium move\")\n",
        "            elif pct > 3:\n",
        "                reasons.append(\"Moderate premium move\")\n",
        "        except:\n",
        "            pass\n",
        "        return \"; \".join(dict.fromkeys(reasons)) if reasons else \"No strong signals\"\n",
        "\n",
        "    merged_df[\"reasons\"] = merged_df.apply(build_reasons, axis=1)\n",
        "\n",
        "    # recommended action\n",
        "    merged_df[\"recommended_action\"] = merged_df.apply(decide_action_from_row, axis=1)\n",
        "\n",
        "    # high-conviction stats (quick approximate: current rows marked in df)\n",
        "    hc_col = \"Current_IsHighConvictionSignal\"\n",
        "    totals, successes, rates = [], [], []\n",
        "    for idx, r in merged_df.iterrows():\n",
        "        s = int(r[\"strike\"])\n",
        "        total = 0; success = 0; rate = None\n",
        "        if hc_col in df.columns:\n",
        "            cond = (df.get(\"Current_Strikeprice\")==s) & (df.get(hc_col)==True)\n",
        "            hc_rows = df[cond]\n",
        "            total = int(hc_rows.shape[0])\n",
        "            for _,hr in hc_rows.iterrows():\n",
        "                t0 = hr.get(\"LTT\")\n",
        "                if pd.isna(t0):\n",
        "                    continue\n",
        "                # choose base premium column according to recommended_action\n",
        "                action = r[\"recommended_action\"]\n",
        "                base_col = curr_call_col if (action==\"BUY_CALL\" and curr_call_col in df.columns) else (curr_put_col if (action==\"BUY_PUT\" and curr_put_col in df.columns) else (curr_call_col if curr_call_col in df.columns else None))\n",
        "                if base_col is None:\n",
        "                    continue\n",
        "                p0 = hr.get(base_col)\n",
        "                if pd.isna(p0):\n",
        "                    continue\n",
        "                window = df[(df[\"LTT\"] >= t0) & (df[\"LTT\"] <= (t0 + pd.Timedelta(minutes=3)))]\n",
        "                if window.empty:\n",
        "                    continue\n",
        "                try:\n",
        "                    if window[base_col].max() > p0:\n",
        "                        success += 1\n",
        "                except:\n",
        "                    pass\n",
        "        rate = (success/total) if total>0 else None\n",
        "        totals.append(total); successes.append(success); rates.append(rate)\n",
        "    merged_df[\"highconv_total\"] = totals\n",
        "    merged_df[\"highconv_success\"] = successes\n",
        "    merged_df[\"highconv_hit_rate\"] = rates\n",
        "\n",
        "    # Current_Strikeprice column\n",
        "    merged_df[\"Current_Strikeprice\"] = merged_df[\"strike\"]\n",
        "\n",
        "    # signal export\n",
        "    signals = []\n",
        "    for _, r in merged_df.iterrows():\n",
        "        if r.get(\"recommended_action\") in (\"BUY_CALL\",\"BUY_PUT\"):\n",
        "            strength = float(r.get(\"pct_change_call\",0) or r.get(\"pct_change_put\",0) or 0)\n",
        "            signals.append({\n",
        "                \"strike\": int(r[\"strike\"]),\n",
        "                \"action\": r.get(\"recommended_action\"),\n",
        "                \"strength\": strength,\n",
        "                \"reason\": r.get(\"reasons\",\"\"),\n",
        "                \"tags\": r.get(\"tags\",\"\"),\n",
        "                \"last_premium\": float(r.get(\"last_premium\") or 0),\n",
        "                \"time\": str(pd.Timestamp.now())\n",
        "            })\n",
        "\n",
        "    # detect reversals (peak->drop in last REVERSAL_WINDOW_MIN)\n",
        "    reversals = []\n",
        "    for s in all_strikes:\n",
        "        # choose series from call or put whichever present\n",
        "        series = call_series.get(s) if call_series.get(s) else put_series.get(s)\n",
        "        if not series:\n",
        "            continue\n",
        "        sr = sorted(series, key=lambda x:x[0])\n",
        "        t_last = sr[-1][0]\n",
        "        cutoff = t_last - pd.Timedelta(minutes=REVERSAL_WINDOW_MIN)\n",
        "        window = [p for t,p in sr if t >= cutoff]\n",
        "        if not window:\n",
        "            continue\n",
        "        peak = max(window); lastp = window[-1]\n",
        "        drop = (peak - lastp)/peak*100 if peak>0 else 0\n",
        "        if drop >= REVERSAL_DROP_PCT:\n",
        "            reversals.append({\"strike\":s,\"peak\":peak,\"last\":lastp,\"drop_pct\":drop})\n",
        "\n",
        "    # detect IV crush events from df rows\n",
        "    iv_crush = []\n",
        "    for idx, row in df.iterrows():\n",
        "        for prev_iv, curr_iv, side in [(\"Previous_Call_IV\",\"Current_Call_IV\",\"CALL\"), (\"Previous_Put_IV\",\"Current_Put_IV\",\"PUT\")]:\n",
        "            if prev_iv in row and curr_iv in row and pd.notna(row[prev_iv]) and pd.notna(row[curr_iv]) and row[prev_iv] > 0:\n",
        "                drop = (row[prev_iv] - row[curr_iv]) / row[prev_iv] * 100\n",
        "                if drop >= IV_CRUSH_DROP:\n",
        "                    iv_crush.append({\"time\": str(row.get(\"LTT\")), \"strike\": row.get(\"Current_Strikeprice\"), \"side\": side, \"drop_pct\": drop})\n",
        "\n",
        "    # return merged rows (as dict list) and signals\n",
        "    return merged_df.to_dict(orient=\"records\"), signals, reversals, iv_crush\n",
        "\n",
        "# ---------------------- MAIN LOOP ---------------------- #\n",
        "def run_realtime(input_path: str):\n",
        "    window = SnapshotWindow(window_minutes=WINDOW_MINUTES)\n",
        "    tailer = tail_file(input_path, start_from_end=START_FROM_END)\n",
        "\n",
        "    last_write = 0.0\n",
        "    print(f\"Realtime engine started. Watching: {input_path}\")\n",
        "    printed_header = False\n",
        "\n",
        "    for raw_line in tailer:\n",
        "        if not running:\n",
        "            break\n",
        "        flat = parse_line_json(raw_line)\n",
        "        if flat is None:\n",
        "            continue\n",
        "        window.append(flat)\n",
        "\n",
        "        # analyze once per refresh interval (or immediate)\n",
        "        now = time.time()\n",
        "        # simple throttle: analyze every REFRESH_SECONDS seconds\n",
        "        if now - last_write < max(0.01, REFRESH_SECONDS * 0.9):\n",
        "            continue\n",
        "        last_write = now\n",
        "\n",
        "        merged_rows, signals, reversals, iv_crush = analyze_window(window)\n",
        "\n",
        "        # EXPORT signals and latest merged snapshot\n",
        "        if WRITE_OUTPUT_JSON:\n",
        "            try:\n",
        "                with open(OUT_AUTOTRADE, \"w\") as fh:\n",
        "                    json.dump(signals, fh, indent=2, default=str)\n",
        "                with open(OUT_LATEST, \"w\") as fh:\n",
        "                    json.dump({\"timestamp\": str(pd.Timestamp.now()), \"merged\": merged_rows, \"reversals\": reversals, \"iv_crush\": iv_crush}, fh, indent=2, default=str)\n",
        "            except Exception as e:\n",
        "                print(\"Warning: failed write output json:\", e)\n",
        "\n",
        "        # Print alerts to stdout and optional log file\n",
        "        if signals:\n",
        "            for s in signals:\n",
        "                msg = f\"[SIGNAL] {s['time']} Strike {s['strike']} => {s['action']} (str={s['strength']:.2f}) reason={s['reason']}\"\n",
        "                print(msg)\n",
        "                if ALERT_LOG:\n",
        "                    with open(ALERT_LOG, \"a\") as L:\n",
        "                        L.write(msg + \"\\n\")\n",
        "        if reversals:\n",
        "            for r in reversals:\n",
        "                msg = f\"[REVERSAL] Strike {r['strike']} drop {r['drop_pct']:.1f}% (peak {r['peak']} -> last {r['last']})\"\n",
        "                print(msg)\n",
        "                if ALERT_LOG:\n",
        "                    with open(ALERT_LOG, \"a\") as L:\n",
        "                        L.write(msg + \"\\n\")\n",
        "        if iv_crush:\n",
        "            for e in iv_crush:\n",
        "                msg = f\"[IV_CRUSH] {e['time']} Strike {e['strike']} {e['side']} drop {e['drop_pct']:.1f}%\"\n",
        "                print(msg)\n",
        "                if ALERT_LOG:\n",
        "                    with open(ALERT_LOG, \"a\") as L:\n",
        "                        L.write(msg + \"\\n\")\n",
        "\n",
        "    print(\"Realtime engine stopped.\")\n",
        "\n",
        "# ---------------------- RUN ---------------------- #\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.exists(INPUT_FILE):\n",
        "        print(\"ERROR: input file not found:\", INPUT_FILE)\n",
        "        sys.exit(1)\n",
        "    run_realtime(INPUT_FILE)\n"
      ],
      "metadata": {
        "id": "y-_OVx6gF47Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abe4473a-eb1c-478b-e7bc-129294448aaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Realtime engine started. Watching: 14112025_BANK_PNL.txt\n",
            "\n",
            "Received exit signal. Shutting down...\n",
            "Realtime engine stopped.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "realtime_ce_pe_engine_improved.py\n",
        "\n",
        "Improved realtime tailing and CE/PE analysis engine based on the user's original script.\n",
        "Key improvements:\n",
        " - Robust JSON parsing (handles object-per-line, JSON lists, and stringified 'Current').\n",
        " - Safer tail implementation (handles rotation/truncation) and immediate processing of appended data.\n",
        " - Configurable via top-of-file constants; small CLI wrapper available.\n",
        " - Reduced pandas overhead by building DataFrame carefully and avoiding excessive apply() usage.\n",
        " - Better type coercion and error handling so single malformed snapshot won't stop the engine.\n",
        " - Optional historical dump and optional dry-run (no file writes) flags.\n",
        " - Clearer logging, and optional alert logfile support.\n",
        "\n",
        "Behavior: tail input file, keep a sliding time window of snapshots, compute per-strike summaries,\n",
        "produce AUTO_TRADE_SIGNALS.json and LATEST_MERGED.json (unless WRITE_OUTPUT_JSON=False),\n",
        "print alerts for signals/reversals/iv-crush to stdout and ALERT_LOG if enabled.\n",
        "\n",
        "Note: this file is self-contained and intended to run on a machine that has pandas & numpy installed.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import signal\n",
        "import argparse\n",
        "from collections import defaultdict, Counter, deque\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Optional, List, Dict, Any\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------- CONFIG ---------------------- #\n",
        "INPUT_FILE = \"14112025_BANK_PNL.txt\"      # default path to live appended snapshots (text)\n",
        "REFRESH_SECONDS = 1.0                     # poll interval (seconds)\n",
        "WINDOW_MINUTES = 15                       # sliding window size for analysis\n",
        "START_FROM_END = False  # process existing data first, then tail new changes                     # if True, ignore historical lines and start tailing from EOF\n",
        "WRITE_OUTPUT_JSON = True                  # write AUTO_TRADE_SIGNALS.json and LATEST_MERGED.json\n",
        "OUT_AUTOTRADE = \"AUTO_TRADE_SIGNALS.json\"\n",
        "OUT_LATEST = \"LATEST_MERGED.json\"\n",
        "ALERT_LOG = None                          # path to an alert logfile or None to only print to stdout\n",
        "IV_CRUSH_DROP = 15.0                      # percent IV drop threshold\n",
        "REVERSAL_DROP_PCT = 12.0                  # reversal detection threshold (drop from peak)\n",
        "REVERSAL_WINDOW_MIN = 5                   # look-back window for reversal check (minutes)\n",
        "MAX_SNAPSHOTS_STORE = 10000               # maximum snapshots to keep in memory (safety)\n",
        "HISTORICAL_DUMP = None                    # optional: path to write a periodic historical dump (or None)\n",
        "DRY_RUN = False                           # if True, won't write output files (useful for testing)\n",
        "\n",
        "running = True\n",
        "\n",
        "# ---------------------- SIGNAL HANDLING ---------------------- #\n",
        "def graceful_exit(signum, frame):\n",
        "    global running\n",
        "    running = False\n",
        "    print(\"\\nReceived exit signal. Shutting down...\")\n",
        "\n",
        "signal.signal(signal.SIGINT, graceful_exit)\n",
        "signal.signal(signal.SIGTERM, graceful_exit)\n",
        "\n",
        "# ---------------------- JSON PARSING ---------------------- #\n",
        "\n",
        "def parse_line_json(line: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Parse a line that may contain:\n",
        "      - a single JSON object\n",
        "      - a JSON-encoded list of objects\n",
        "      - a text line that contains a JSON object somewhere inside\n",
        "    Returns a list of flattened dicts (possibly empty).\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    if not line:\n",
        "        return out\n",
        "    s = line.strip()\n",
        "    if not s:\n",
        "        return out\n",
        "\n",
        "    # Try direct load (object or list)\n",
        "    try:\n",
        "        parsed = json.loads(s)\n",
        "    except Exception:\n",
        "        # try to find first '{' and parse from there\n",
        "        try:\n",
        "            i = s.index('{')\n",
        "            parsed = json.loads(s[i:])\n",
        "        except Exception:\n",
        "            return out\n",
        "\n",
        "    # normalize to list\n",
        "    items = parsed if isinstance(parsed, list) else [parsed]\n",
        "\n",
        "    for obj in items:\n",
        "        flat = {}\n",
        "        if not isinstance(obj, dict):\n",
        "            continue\n",
        "        # flatten top-level except 'Current' (handled below)\n",
        "        for k, v in obj.items():\n",
        "            if k != \"Current\":\n",
        "                flat[k] = v\n",
        "        # handle Current that may be a string or dict and may contain Previous/Current/Next blocks\n",
        "        curr_raw = obj.get(\"Current\")\n",
        "        if isinstance(curr_raw, str):\n",
        "            try:\n",
        "                curr = json.loads(curr_raw)\n",
        "            except Exception:\n",
        "                curr = None\n",
        "        elif isinstance(curr_raw, dict):\n",
        "            curr = curr_raw\n",
        "        else:\n",
        "            curr = None\n",
        "\n",
        "        if isinstance(curr, dict):\n",
        "            for section in (\"Previous\", \"Current\", \"Next\"):\n",
        "                block = curr.get(section)\n",
        "                if isinstance(block, dict):\n",
        "                    for key, val in block.items():\n",
        "                        flat[f\"{section}_{key}\"] = val\n",
        "        out.append(flat)\n",
        "    return out\n",
        "\n",
        "# ---------------------- TAIL SUPPORT ---------------------- #\n",
        "\n",
        "def tail_file(path: str, start_from_end: bool = True):(path: str, start_from_end: bool = True):\n",
        "    \"\"\"\n",
        "    Generator yielding new lines appended to the file.\n",
        "    Handles file truncation/rotation by checking file size/inode changes.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Input file not found: {path}\")\n",
        "\n",
        "    with open(path, 'r', encoding='utf-8', errors='ignore') as fh:\n",
        "        if start_from_end:\n",
        "            fh.seek(0, os.SEEK_END)\n",
        "        else:\n",
        "            fh.seek(0)\n",
        "        last_inode = os.fstat(fh.fileno()).st_ino if hasattr(os, 'fstat') else None\n",
        "        while running:\n",
        "            where = fh.tell()\n",
        "            line = fh.readline()\n",
        "            if not line:\n",
        "                # detect truncation/rotation\n",
        "                try:\n",
        "                    st = os.stat(path)\n",
        "                    if last_inode is not None and getattr(st, 'st_ino', None) != last_inode:\n",
        "                        # file rotated - reopen\n",
        "                        fh = open(path, 'r', encoding='utf-8', errors='ignore')\n",
        "                        last_inode = os.fstat(fh.fileno()).st_ino if hasattr(os, 'fstat') else None\n",
        "                        continue\n",
        "                    if st.st_size < where:\n",
        "                        # truncated\n",
        "                        fh.seek(0)\n",
        "                except FileNotFoundError:\n",
        "                    # file removed — wait for it to reappear\n",
        "                    time.sleep(max(0.1, REFRESH_SECONDS))\n",
        "                    continue\n",
        "                time.sleep(max(0.01, REFRESH_SECONDS))\n",
        "                fh.seek(where)\n",
        "                continue\n",
        "            yield line\n",
        "\n",
        "# ---------------------- IN-MEMORY STORE ---------------------- #\n",
        "class SnapshotWindow:\n",
        "    def __init__(self, window_minutes: int = WINDOW_MINUTES, max_snapshots: int = MAX_SNAPSHOTS_STORE):\n",
        "        self.window_minutes = int(window_minutes)\n",
        "        self.max_snapshots = int(max_snapshots)\n",
        "        self.store = deque()  # (timestamp (pd.Timestamp), flat_dict)\n",
        "\n",
        "    def append(self, flat: Dict[str, Any]):\n",
        "        # determine timestamp robustly\n",
        "        t = flat.get('LTT') or flat.get('ltt') or flat.get('time') or flat.get('ts') or None\n",
        "        if isinstance(t, str):\n",
        "            try:\n",
        "                t = pd.to_datetime(t)\n",
        "            except Exception:\n",
        "                t = pd.Timestamp.now()\n",
        "        elif isinstance(t, (pd.Timestamp, datetime)):\n",
        "            t = pd.Timestamp(t)\n",
        "        elif t is None:\n",
        "            t = pd.Timestamp.now()\n",
        "        else:\n",
        "            try:\n",
        "                t = pd.to_datetime(t)\n",
        "            except Exception:\n",
        "                t = pd.Timestamp.now()\n",
        "\n",
        "        self.store.append((t, flat))\n",
        "        # enforce max length\n",
        "        while len(self.store) > self.max_snapshots:\n",
        "            self.store.popleft()\n",
        "        # prune by time window\n",
        "        self.prune_old()\n",
        "\n",
        "    def prune_old(self):\n",
        "        if not self.store:\n",
        "            return\n",
        "        cutoff = pd.Timestamp.now() - pd.Timedelta(minutes=self.window_minutes)\n",
        "        while self.store and self.store[0][0] < cutoff:\n",
        "            self.store.popleft()\n",
        "\n",
        "    def to_dataframe(self) -> pd.DataFrame:\n",
        "        if not self.store:\n",
        "            return pd.DataFrame()\n",
        "        rows = []\n",
        "        for t, flat in self.store:\n",
        "            r = dict(flat)\n",
        "            r['LTT'] = t\n",
        "            rows.append(r)\n",
        "        # Create dataframe once\n",
        "        df = pd.DataFrame(rows)\n",
        "        return df\n",
        "\n",
        "# ---------------------- ANALYSIS HELPERS ---------------------- #\n",
        "\n",
        "def pick_column(df: pd.DataFrame, ltp: str, prem: str) -> Optional[str]:\n",
        "    return ltp if ltp in df.columns else (prem if prem in df.columns else None)\n",
        "\n",
        "\n",
        "def build_series_from_df(df: pd.DataFrame, prev_call_col, curr_call_col, next_call_col,\n",
        "                         prev_put_col, curr_put_col, next_put_col,\n",
        "                         prev_str=\"Previous_Strikeprice\", curr_str=\"Current_Strikeprice\", next_str=\"Next_Strikeprice\"):\n",
        "    call_series = defaultdict(list)\n",
        "    put_series = defaultdict(list)\n",
        "    if df.empty:\n",
        "        return call_series, put_series\n",
        "\n",
        "    # iterate rows once\n",
        "    for _, row in df.iterrows():\n",
        "        t = row.get('LTT')\n",
        "        for sc, pc in ((prev_str, prev_call_col), (curr_str, curr_call_col), (next_str, next_call_col)):\n",
        "            if sc in row and pc and pc in row:\n",
        "                scv = row.get(sc)\n",
        "                pcv = row.get(pc)\n",
        "                if pd.isna(scv) or pd.isna(pcv):\n",
        "                    continue\n",
        "                try:\n",
        "                    s = int(scv)\n",
        "                    p = float(pcv)\n",
        "                except Exception:\n",
        "                    continue\n",
        "                call_series[s].append((t, p))\n",
        "        for sc, pp in ((prev_str, prev_put_col), (curr_str, curr_put_col), (next_str, next_put_col)):\n",
        "            if sc in row and pp and pp in row:\n",
        "                scv = row.get(sc)\n",
        "                ppv = row.get(pp)\n",
        "                if pd.isna(scv) or pd.isna(ppv):\n",
        "                    continue\n",
        "                try:\n",
        "                    s = int(scv)\n",
        "                    p = float(ppv)\n",
        "                except Exception:\n",
        "                    continue\n",
        "                put_series[s].append((t, p))\n",
        "    return call_series, put_series\n",
        "\n",
        "\n",
        "def summarize_series(pairs):\n",
        "    if not pairs:\n",
        "        return None\n",
        "    sr = sorted(pairs, key=lambda x: x[0])\n",
        "    prices = [p for _, p in sr]\n",
        "    first = prices[0]\n",
        "    last = prices[-1]\n",
        "    peak = max(prices)\n",
        "    trough = min(prices)\n",
        "    abs_chg = last - first\n",
        "    pct_chg = (abs_chg / first * 100) if first != 0 else np.nan\n",
        "    return {\n",
        "        'first': first, 'last': last, 'peak': peak, 'trough': trough,\n",
        "        'abs_change': abs_chg, 'pct_change': pct_chg, 'n_obs': len(prices),\n",
        "        'series_sorted': sr\n",
        "    }\n",
        "\n",
        "\n",
        "def forecast_from_pct(last, pct):\n",
        "    if pd.isna(pct):\n",
        "        pct = 0.0\n",
        "    if pct >= 25:\n",
        "        return (last+15, last+35), (last+25, last+60)\n",
        "    if pct >= 8:\n",
        "        return (last+6, last+18), (last+12, last+30)\n",
        "    if pct > 0:\n",
        "        return (last+2, last+8), (last+5, last+15)\n",
        "    return (last-5, last+2), (last-8, last+5)\n",
        "\n",
        "\n",
        "def aggregate_tags_and_mf(df: pd.DataFrame, strikes: List[int]):\n",
        "    out = {int(s): {'tags': Counter(), 'call_mf': 0.0, 'put_mf': 0.0} for s in strikes}\n",
        "    tag_cols = [\"Previous_StrategyTag\", \"Current_StrategyTag\", \"Next_StrategyTag\"]\n",
        "    call_mflow_cols = [\"Previous_CallMoneyFlow\", \"Current_CallMoneyFlow\", \"Next_CallMoneyFlow\",\n",
        "                       \"Previous_TotalcallMoneyFlow\", \"Current_TotalcallMoneyFlow\", \"Next_TotalcallMoneyFlow\"]\n",
        "    put_mflow_cols  = [\"Previous_PutMoneyFlow\", \"Current_PutMoneyFlow\", \"Next_PutMoneyFlow\",\n",
        "                       \"Previous_TotalputMoneyFlow\", \"Current_TotalputMoneyFlow\", \"Next_TotalputMoneyFlow\"]\n",
        "\n",
        "    if df.empty:\n",
        "        return out\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        for sc in (\"Previous_Strikeprice\", \"Current_Strikeprice\", \"Next_Strikeprice\"):\n",
        "            if sc not in row:\n",
        "                continue\n",
        "            scv = row.get(sc)\n",
        "            if pd.isna(scv):\n",
        "                continue\n",
        "            try:\n",
        "                s = int(scv)\n",
        "            except Exception:\n",
        "                continue\n",
        "            if s not in out:\n",
        "                continue\n",
        "            for tc in tag_cols:\n",
        "                if tc in row:\n",
        "                    v = row.get(tc)\n",
        "                    if isinstance(v, str) and v.strip():\n",
        "                        tokens = [t.strip() for t in v.replace('|', ';').split(';') if t.strip()]\n",
        "                        for tkn in tokens:\n",
        "                            out[s]['tags'][tkn] += 1\n",
        "            for cm in call_mflow_cols:\n",
        "                if cm in row:\n",
        "                    val = row.get(cm)\n",
        "                    if not pd.isna(val):\n",
        "                        try:\n",
        "                            out[s]['call_mf'] += float(val)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "            for pm in put_mflow_cols:\n",
        "                if pm in row:\n",
        "                    val = row.get(pm)\n",
        "                    if not pd.isna(val):\n",
        "                        try:\n",
        "                            out[s]['put_mf'] += float(val)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "    return out\n",
        "\n",
        "\n",
        "def decide_action_from_row(row):\n",
        "    call_pct = row.get('pct_change_call') if pd.notna(row.get('pct_change_call')) else 0\n",
        "    put_pct  = row.get('pct_change_put')  if pd.notna(row.get('pct_change_put'))  else 0\n",
        "    tags = (row.get('tags') or '').lower()\n",
        "    bull_boost = ('call buying' in tags) or ('oi_support_call' in tags) or ('bull' in tags)\n",
        "    bear_boost = ('put buying' in tags) or ('call writing' in tags) or ('bear' in tags)\n",
        "\n",
        "    if (put_pct > 8) or (put_pct > 5 and bear_boost):\n",
        "        return 'BUY_PUT'\n",
        "    if (call_pct > 8) or (call_pct > 5 and bull_boost):\n",
        "        return 'BUY_CALL'\n",
        "    return 'HOLD'\n",
        "\n",
        "# ---------------------- CORE ANALYSIS ---------------------- #\n",
        "\n",
        "def analyze_window(window: SnapshotWindow):\n",
        "    df = window.to_dataframe()\n",
        "    # pick columns with fallback\n",
        "    prev_call_col = pick_column(df, 'Previous_Call_ltp', 'Previous_Call_Premium')\n",
        "    curr_call_col = pick_column(df, 'Current_Call_ltp', 'Current_Call_Premium')\n",
        "    next_call_col = pick_column(df, 'Next_Call_ltp', 'Next_Call_Premium')\n",
        "\n",
        "    prev_put_col = pick_column(df, 'Previous_Put_ltp', 'Previous_Put_Premium')\n",
        "    curr_put_col = pick_column(df, 'Current_Put_ltp', 'Current_Put_Premium')\n",
        "    next_put_col = pick_column(df, 'Next_Put_ltp', 'Next_Put_Premium')\n",
        "\n",
        "    call_series, put_series = build_series_from_df(df, prev_call_col, curr_call_col, next_call_col,\n",
        "                                                   prev_put_col, curr_put_col, next_put_col)\n",
        "\n",
        "    all_strikes = sorted(set(list(call_series.keys()) + list(put_series.keys())))\n",
        "\n",
        "    merged_rows = []\n",
        "    for s in all_strikes:\n",
        "        cs = summarize_series(call_series.get(s, []))\n",
        "        ps = summarize_series(put_series.get(s, []))\n",
        "        n_call = cs['n_obs'] if cs else 0\n",
        "        n_put  = ps['n_obs'] if ps else 0\n",
        "        # prefer side with more observations\n",
        "        if n_call >= n_put and cs:\n",
        "            first_p = cs['first']; last_p = cs['last']; peak = cs['peak']; trough = cs['trough']\n",
        "            abs_chg = cs['abs_change']; pct = cs['pct_change']\n",
        "            f5, f10 = forecast_from_pct(last_p, pct)\n",
        "            row = {\n",
        "                'strike': int(s),\n",
        "                'n_obs': n_call + n_put,\n",
        "                'n_obs_call': n_call, 'n_obs_put': n_put,\n",
        "                'first_premium': first_p, 'last_premium': last_p, 'peak_premium': peak, 'trough_premium': trough,\n",
        "                'abs_change': abs_chg, 'pct_change': pct,\n",
        "                '5min_low': f5[0], '5min_high': f5[1], '10min_low': f10[0], '10min_high': f10[1],\n",
        "                'first_call_ltp': cs['first'], 'last_call_ltp': cs['last'], 'peak_call_ltp': cs['peak'], 'trough_call_ltp': cs['trough'],\n",
        "                'abs_change_call': cs['abs_change'], 'pct_change_call': cs['pct_change'],\n",
        "                'first_put_ltp': ps['first'] if ps else np.nan, 'last_put_ltp': ps['last'] if ps else np.nan,\n",
        "                'pct_change_put': ps['pct_change'] if ps else np.nan\n",
        "            }\n",
        "        elif ps:\n",
        "            first_p = ps['first']; last_p = ps['last']; peak = ps['peak']; trough = ps['trough']\n",
        "            abs_chg = ps['abs_change']; pct = ps['pct_change']\n",
        "            f5, f10 = forecast_from_pct(last_p, pct)\n",
        "            row = {\n",
        "                'strike': int(s),\n",
        "                'n_obs': n_call + n_put,\n",
        "                'n_obs_call': n_call, 'n_obs_put': n_put,\n",
        "                'first_premium': first_p, 'last_premium': last_p, 'peak_premium': peak, 'trough_premium': trough,\n",
        "                'abs_change': abs_chg, 'pct_change': pct,\n",
        "                '5min_low': f5[0], '5min_high': f5[1], '10min_low': f10[0], '10min_high': f10[1],\n",
        "                'first_put_ltp': ps['first'], 'last_put_ltp': ps['last'], 'peak_put_ltp': ps['peak'], 'trough_put_ltp': ps['trough'],\n",
        "                'abs_change_put': ps['abs_change'], 'pct_change_put': ps['pct_change'],\n",
        "                'first_call_ltp': cs['first'] if cs else np.nan, 'last_call_ltp': cs['last'] if cs else np.nan,\n",
        "                'pct_change_call': cs['pct_change'] if cs else np.nan\n",
        "            }\n",
        "        else:\n",
        "            continue\n",
        "        merged_rows.append(row)\n",
        "\n",
        "    if not merged_rows:\n",
        "        return [], [], [], []\n",
        "\n",
        "    merged_df = pd.DataFrame(merged_rows)\n",
        "    strike_info = aggregate_tags_and_mf(df, merged_df['strike'].tolist())\n",
        "    # attach tags and moneyflow\n",
        "    merged_df['tags'] = merged_df['strike'].apply(lambda s: ';'.join([f\"{k}:{v}\" for k, v in strike_info[int(s)]['tags'].items()]) if strike_info[int(s)]['tags'] else '')\n",
        "    merged_df['call_moneyflow'] = merged_df['strike'].apply(lambda s: strike_info[int(s)]['call_mf'])\n",
        "    merged_df['put_moneyflow']  = merged_df['strike'].apply(lambda s: strike_info[int(s)]['put_mf'])\n",
        "\n",
        "    # build reasons\n",
        "    def build_reasons(row):\n",
        "        reasons = []\n",
        "        tags_text = str(row.get('tags', ''))\n",
        "        if any(k in tags_text.lower() for k in ['rsi', 'macd']):\n",
        "            reasons.append('RSI/MACD momentum')\n",
        "        if 'vwap' in tags_text.lower():\n",
        "            reasons.append('VWAP divergence')\n",
        "        if 'oi' in tags_text.lower():\n",
        "            reasons.append('OI support/resistance')\n",
        "        if row.get('call_moneyflow', 0) > 0:\n",
        "            reasons.append('Call net buying')\n",
        "        if row.get('put_moneyflow', 0) > 0:\n",
        "            reasons.append('Put net buying')\n",
        "        pct = row.get('pct_change', 0) or 0\n",
        "        try:\n",
        "            if pct > 10:\n",
        "                reasons.append('Strong premium move')\n",
        "            elif pct > 3:\n",
        "                reasons.append('Moderate premium move')\n",
        "        except Exception:\n",
        "            pass\n",
        "        return '; '.join(dict.fromkeys(reasons)) if reasons else 'No strong signals'\n",
        "\n",
        "    merged_df['reasons'] = merged_df.apply(build_reasons, axis=1)\n",
        "    merged_df['recommended_action'] = merged_df.apply(decide_action_from_row, axis=1)\n",
        "\n",
        "    # quick high-conviction stats (approximation)\n",
        "    hc_col = 'Current_IsHighConvictionSignal'\n",
        "    totals, successes, rates = [], [], []\n",
        "    for _, r in merged_df.iterrows():\n",
        "        s = int(r['strike'])\n",
        "        total = success = 0\n",
        "        if hc_col in df.columns:\n",
        "            cond = (df.get('Current_Strikeprice') == s) & (df.get(hc_col) == True)\n",
        "            hc_rows = df[cond]\n",
        "            total = int(hc_rows.shape[0])\n",
        "            for _, hr in hc_rows.iterrows():\n",
        "                t0 = hr.get('LTT')\n",
        "                if pd.isna(t0):\n",
        "                    continue\n",
        "                action = r['recommended_action']\n",
        "                base_col = curr_call_col if (action == 'BUY_CALL' and curr_call_col in df.columns) else (curr_put_col if (action == 'BUY_PUT' and curr_put_col in df.columns) else (curr_call_col if curr_call_col in df.columns else None))\n",
        "                if base_col is None:\n",
        "                    continue\n",
        "                p0 = hr.get(base_col)\n",
        "                if pd.isna(p0):\n",
        "                    continue\n",
        "                window = df[(df['LTT'] >= t0) & (df['LTT'] <= (t0 + pd.Timedelta(minutes=3)))]\n",
        "                if window.empty:\n",
        "                    continue\n",
        "                try:\n",
        "                    if window[base_col].max() > p0:\n",
        "                        success += 1\n",
        "                except Exception:\n",
        "                    pass\n",
        "        rate = (success / total) if total > 0 else None\n",
        "        totals.append(total); successes.append(success); rates.append(rate)\n",
        "    merged_df['highconv_total'] = totals\n",
        "    merged_df['highconv_success'] = successes\n",
        "    merged_df['highconv_hit_rate'] = rates\n",
        "\n",
        "    merged_df['Current_Strikeprice'] = merged_df['strike']\n",
        "\n",
        "    # assemble signals\n",
        "    signals = []\n",
        "    for _, r in merged_df.iterrows():\n",
        "        if r.get('recommended_action') in ('BUY_CALL', 'BUY_PUT'):\n",
        "            strength = float(r.get('pct_change_call', 0) or r.get('pct_change_put', 0) or 0)\n",
        "            signals.append({\n",
        "                'strike': int(r['strike']),\n",
        "                'action': r.get('recommended_action'),\n",
        "                'strength': strength,\n",
        "                'reason': r.get('reasons', ''),\n",
        "                'tags': r.get('tags', ''),\n",
        "                'last_premium': float(r.get('last_premium') or 0),\n",
        "                'time': str(pd.Timestamp.now())\n",
        "            })\n",
        "\n",
        "    # detect reversals\n",
        "    reversals = []\n",
        "    for s in all_strikes:\n",
        "        series = call_series.get(s) if call_series.get(s) else put_series.get(s)\n",
        "        if not series:\n",
        "            continue\n",
        "        sr = sorted(series, key=lambda x: x[0])\n",
        "        t_last = sr[-1][0]\n",
        "        cutoff = t_last - pd.Timedelta(minutes=REVERSAL_WINDOW_MIN)\n",
        "        window = [p for t, p in sr if t >= cutoff]\n",
        "        if not window:\n",
        "            continue\n",
        "        peak = max(window); lastp = window[-1]\n",
        "        drop = (peak - lastp) / peak * 100 if peak > 0 else 0\n",
        "        if drop >= REVERSAL_DROP_PCT:\n",
        "            reversals.append({'strike': s, 'peak': peak, 'last': lastp, 'drop_pct': drop})\n",
        "\n",
        "    # detect IV crush\n",
        "    iv_crush = []\n",
        "    if not df.empty:\n",
        "        for _, row in df.iterrows():\n",
        "            for prev_iv, curr_iv, side in [(\"Previous_Call_IV\", \"Current_Call_IV\", \"CALL\"), (\"Previous_Put_IV\", \"Current_Put_IV\", \"PUT\")]:\n",
        "                if prev_iv in row and curr_iv in row and pd.notna(row[prev_iv]) and pd.notna(row[curr_iv]) and row[prev_iv] > 0:\n",
        "                    drop = (row[prev_iv] - row[curr_iv]) / row[prev_iv] * 100\n",
        "                    if drop >= IV_CRUSH_DROP:\n",
        "                        iv_crush.append({'time': str(row.get('LTT')), 'strike': row.get('Current_Strikeprice'), 'side': side, 'drop_pct': drop})\n",
        "\n",
        "    return merged_df.to_dict(orient='records'), signals, reversals, iv_crush\n",
        "\n",
        "# ---------------------- MAIN LOOP ---------------------- #\n",
        "\n",
        "def run_realtime(input_path: str, start_from_end: bool = START_FROM_END):\n",
        "    window = SnapshotWindow(window_minutes=WINDOW_MINUTES)\n",
        "    tailer = tail_file(input_path, start_from_end)\n",
        "\n",
        "    last_analyze = 0.0\n",
        "    print(f\"Realtime engine started. Watching: {input_path}\")\n",
        "\n",
        "    for raw_line in tailer:\n",
        "        if not running:\n",
        "            break\n",
        "        try:\n",
        "            parsed_list = parse_line_json(raw_line)\n",
        "        except Exception as e:\n",
        "            print('Parse error:', e)\n",
        "            continue\n",
        "        if not parsed_list:\n",
        "            continue\n",
        "        for flat in parsed_list:\n",
        "            try:\n",
        "                window.append(flat)\n",
        "            except Exception as e:\n",
        "                print('Append error (ignored):', e)\n",
        "                continue\n",
        "\n",
        "        now = time.time()\n",
        "        if now - last_analyze < max(0.01, REFRESH_SECONDS * 0.6):\n",
        "            continue\n",
        "        last_analyze = now\n",
        "\n",
        "        try:\n",
        "            merged_rows, signals, reversals, iv_crush = analyze_window(window)\n",
        "        except Exception as e:\n",
        "            print('Analysis error (skipping this cycle):', e)\n",
        "            continue\n",
        "\n",
        "        # write outputs\n",
        "        if WRITE_OUTPUT_JSON and not DRY_RUN:\n",
        "            try:\n",
        "                with open(OUT_AUTOTRADE, 'w') as fh:\n",
        "                    json.dump(signals, fh, indent=2, default=str)\n",
        "                with open(OUT_LATEST, 'w') as fh:\n",
        "                    json.dump({'timestamp': str(pd.Timestamp.now()), 'merged': merged_rows, 'reversals': reversals, 'iv_crush': iv_crush}, fh, indent=2, default=str)\n",
        "            except Exception as e:\n",
        "                print('Warning: failed to write outputs:', e)\n",
        "\n",
        "        # optional historical dump\n",
        "        if HISTORICAL_DUMP:\n",
        "            try:\n",
        "                with open(HISTORICAL_DUMP, 'a') as fh:\n",
        "                    fh.write(json.dumps({'ts': str(pd.Timestamp.now()), 'merged': merged_rows}) + '\\n')\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # print alerts\n",
        "        if signals:\n",
        "            for s in signals:\n",
        "                msg = f\"[SIGNAL] {s['time']} Strike {s['strike']} => {s['action']} (str={s['strength']:.2f}) reason={s['reason']}\"\n",
        "                print(msg)\n",
        "                if ALERT_LOG:\n",
        "                    with open(ALERT_LOG, 'a') as L:\n",
        "                        L.write(msg + '\\n')\n",
        "        if reversals:\n",
        "            for r in reversals:\n",
        "                msg = f\"[REVERSAL] Strike {r['strike']} drop {r['drop_pct']:.1f}% (peak {r['peak']} -> last {r['last']})\"\n",
        "                print(msg)\n",
        "                if ALERT_LOG:\n",
        "                    with open(ALERT_LOG, 'a') as L:\n",
        "                        L.write(msg + '\\n')\n",
        "        if iv_crush:\n",
        "            for e in iv_crush:\n",
        "                msg = f\"[IV_CRUSH] {e['time']} Strike {e['strike']} {e['side']} drop {e['drop_pct']:.1f}%\"\n",
        "                print(msg)\n",
        "                if ALERT_LOG:\n",
        "                    with open(ALERT_LOG, 'a') as L:\n",
        "                        L.write(msg + '\\n')\n",
        "\n",
        "    print('Realtime engine stopped.')\n",
        "\n",
        "# ---------------------- CLI ---------------------- #\n",
        "\n",
        "def cli():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument('--file', '-f', dest='file', default=INPUT_FILE, help='Input file to tail')\n",
        "    p.add_argument('--from-start', action='store_true', help='Start reading from start of file instead of EOF')\n",
        "    p.add_argument('--no-write', action='store_true', help='Do not write output json files (dry run)')\n",
        "    p.add_argument('--alert-log', dest='alert_log', default=ALERT_LOG, help='Path to append alert log')\n",
        "    p.add_argument('--window', type=int, default=WINDOW_MINUTES, help='Sliding window minutes')\n",
        "    args = p.parse_args()\n",
        "\n",
        "    global WRITE_OUTPUT_JSON, START_FROM_END, ALERT_LOG, WINDOW_MINUTES, DRY_RUN\n",
        "    WRITE_OUTPUT_JSON = True\n",
        "    START_FROM_END = not args.from_start\n",
        "    ALERT_LOG = args.alert_log\n",
        "    WINDOW_MINUTES = int(args.window)\n",
        "    DRY_RUN = bool(args.no_write)\n",
        "\n",
        "    if not os.path.exists(args.file):\n",
        "        print('ERROR: input file not found:', args.file)\n",
        "        sys.exit(1)\n",
        "\n",
        "    run_realtime(args.file, start_from_end=START_FROM_END)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    cli()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "u6G2l9TmKwdB",
        "outputId": "06f69565-0110-480f-d0ab-818758c9b27e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-4020145307.py, line 124)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-4020145307.py\"\u001b[0;36m, line \u001b[0;32m124\u001b[0m\n\u001b[0;31m    def tail_file(path: str, start_from_end: bool = True):(path: str, start_from_end: bool = True):\u001b[0m\n\u001b[0m                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "realtime_ce_pe_engine_improved.py\n",
        "\n",
        "Improved realtime tailing and CE/PE analysis engine based on the user's original script.\n",
        "Key improvements:\n",
        " - Robust JSON parsing (handles object-per-line, JSON lists, and stringified 'Current').\n",
        " - Safer tail implementation (handles rotation/truncation) and immediate processing of appended data.\n",
        " - Configurable via top-of-file constants; small CLI wrapper available.\n",
        " - Reduced pandas overhead by building DataFrame carefully and avoiding excessive apply() usage.\n",
        " - Better type coercion and error handling so single malformed snapshot won't stop the engine.\n",
        " - Optional historical dump and optional dry-run (no file writes) flags.\n",
        " - Clearer logging, and optional alert logfile support.\n",
        "\n",
        "Behavior: tail input file, keep a sliding time window of snapshots, compute per-strike summaries,\n",
        "produce AUTO_TRADE_SIGNALS.json and LATEST_MERGED.json (unless WRITE_OUTPUT_JSON=False),\n",
        "print alerts for signals/reversals/iv-crush to stdout and ALERT_LOG if enabled.\n",
        "\n",
        "Note: this file is self-contained and intended to run on a machine that has pandas & numpy installed.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import signal\n",
        "import argparse\n",
        "from collections import defaultdict, Counter, deque\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Optional, List, Dict, Any\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------- CONFIG ---------------------- #\n",
        "INPUT_FILE = \"14112025_BANK_PNL.txt\"      # default path to live appended snapshots (text)\n",
        "REFRESH_SECONDS = 1.0                     # poll interval (seconds)\n",
        "WINDOW_MINUTES = 15                       # sliding window size for analysis\n",
        "START_FROM_END = False  # process existing data first, then tail new changes                     # if True, ignore historical lines and start tailing from EOF\n",
        "WRITE_OUTPUT_JSON = True                  # write AUTO_TRADE_SIGNALS.json and LATEST_MERGED.json\n",
        "OUT_AUTOTRADE = \"AUTO_TRADE_SIGNALS.json\"\n",
        "OUT_LATEST = \"LATEST_MERGED.json\"\n",
        "ALERT_LOG = None                          # path to an alert logfile or None to only print to stdout\n",
        "IV_CRUSH_DROP = 15.0                      # percent IV drop threshold\n",
        "REVERSAL_DROP_PCT = 12.0                  # reversal detection threshold (drop from peak)\n",
        "REVERSAL_WINDOW_MIN = 5                   # look-back window for reversal check (minutes)\n",
        "MAX_SNAPSHOTS_STORE = 10000               # maximum snapshots to keep in memory (safety)\n",
        "HISTORICAL_DUMP = None                    # optional: path to write a periodic historical dump (or None)\n",
        "DRY_RUN = False                           # if True, won't write output files (useful for testing)\n",
        "\n",
        "running = True\n",
        "\n",
        "# ---------------------- SIGNAL HANDLING ---------------------- #\n",
        "def graceful_exit(signum, frame):\n",
        "    global running\n",
        "    running = False\n",
        "    print(\"\\nReceived exit signal. Shutting down...\")\n",
        "\n",
        "signal.signal(signal.SIGINT, graceful_exit)\n",
        "signal.signal(signal.SIGTERM, graceful_exit)\n",
        "\n",
        "# ---------------------- JSON PARSING ---------------------- #\n",
        "\n",
        "def parse_line_json(line: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Parse a line that may contain:\n",
        "      - a single JSON object\n",
        "      - a JSON-encoded list of objects\n",
        "      - a text line that contains a JSON object somewhere inside\n",
        "    Returns a list of flattened dicts (possibly empty).\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    if not line:\n",
        "        return out\n",
        "    s = line.strip()\n",
        "    if not s:\n",
        "        return out\n",
        "\n",
        "    # Try direct load (object or list)\n",
        "    try:\n",
        "        parsed = json.loads(s)\n",
        "    except Exception:\n",
        "        # try to find first '{' and parse from there\n",
        "        try:\n",
        "            i = s.index('{')\n",
        "            parsed = json.loads(s[i:])\n",
        "        except Exception:\n",
        "            return out\n",
        "\n",
        "    # normalize to list\n",
        "    items = parsed if isinstance(parsed, list) else [parsed]\n",
        "\n",
        "    for obj in items:\n",
        "        flat = {}\n",
        "        if not isinstance(obj, dict):\n",
        "            continue\n",
        "        # flatten top-level except 'Current' (handled below)\n",
        "        for k, v in obj.items():\n",
        "            if k != \"Current\":\n",
        "                flat[k] = v\n",
        "        # handle Current that may be a string or dict and may contain Previous/Current/Next blocks\n",
        "        curr_raw = obj.get(\"Current\")\n",
        "        if isinstance(curr_raw, str):\n",
        "            try:\n",
        "                curr = json.loads(curr_raw)\n",
        "            except Exception:\n",
        "                curr = None\n",
        "        elif isinstance(curr_raw, dict):\n",
        "            curr = curr_raw\n",
        "        else:\n",
        "            curr = None\n",
        "\n",
        "        if isinstance(curr, dict):\n",
        "            for section in (\"Previous\", \"Current\", \"Next\"):\n",
        "                block = curr.get(section)\n",
        "                if isinstance(block, dict):\n",
        "                    for key, val in block.items():\n",
        "                        flat[f\"{section}_{key}\"] = val\n",
        "        out.append(flat)\n",
        "    return out\n",
        "\n",
        "# ---------------------- TAIL SUPPORT ---------------------- #\n",
        "\n",
        "def tail_file(path: str, start_from_end: bool = True):\n",
        "    \"\"\"\n",
        "    Generator yielding new lines appended to the file.\n",
        "    Handles file truncation/rotation by checking file size/inode changes.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Input file not found: {path}\")\n",
        "\n",
        "    with open(path, 'r', encoding='utf-8', errors='ignore') as fh:\n",
        "        if start_from_end:\n",
        "            fh.seek(0, os.SEEK_END)\n",
        "        else:\n",
        "            fh.seek(0)\n",
        "        last_inode = os.fstat(fh.fileno()).st_ino if hasattr(os, 'fstat') else None\n",
        "        while running:\n",
        "            where = fh.tell()\n",
        "            line = fh.readline()\n",
        "            if not line:\n",
        "                # detect truncation/rotation\n",
        "                try:\n",
        "                    st = os.stat(path)\n",
        "                    if last_inode is not None and getattr(st, 'st_ino', None) != last_inode:\n",
        "                        # file rotated - reopen\n",
        "                        fh = open(path, 'r', encoding='utf-8', errors='ignore')\n",
        "                        last_inode = os.fstat(fh.fileno()).st_ino if hasattr(os, 'fstat') else None\n",
        "                        continue\n",
        "                    if st.st_size < where:\n",
        "                        # truncated\n",
        "                        fh.seek(0)\n",
        "                except FileNotFoundError:\n",
        "                    # file removed — wait for it to reappear\n",
        "                    time.sleep(max(0.1, REFRESH_SECONDS))\n",
        "                    continue\n",
        "                time.sleep(max(0.01, REFRESH_SECONDS))\n",
        "                fh.seek(where)\n",
        "                continue\n",
        "            yield line\n",
        "\n",
        "# ---------------------- IN-MEMORY STORE ---------------------- #\n",
        "class SnapshotWindow:\n",
        "    def __init__(self, window_minutes: int = WINDOW_MINUTES, max_snapshots: int = MAX_SNAPSHOTS_STORE):\n",
        "        self.window_minutes = int(window_minutes)\n",
        "        self.max_snapshots = int(max_snapshots)\n",
        "        self.store = deque()  # (timestamp (pd.Timestamp), flat_dict)\n",
        "\n",
        "    def append(self, flat: Dict[str, Any]):\n",
        "        # determine timestamp robustly\n",
        "        t = flat.get('LTT') or flat.get('ltt') or flat.get('time') or flat.get('ts') or None\n",
        "        if isinstance(t, str):\n",
        "            try:\n",
        "                t = pd.to_datetime(t)\n",
        "            except Exception:\n",
        "                t = pd.Timestamp.now()\n",
        "        elif isinstance(t, (pd.Timestamp, datetime)):\n",
        "            t = pd.Timestamp(t)\n",
        "        elif t is None:\n",
        "            t = pd.Timestamp.now()\n",
        "        else:\n",
        "            try:\n",
        "                t = pd.to_datetime(t)\n",
        "            except Exception:\n",
        "                t = pd.Timestamp.now()\n",
        "\n",
        "        self.store.append((t, flat))\n",
        "        # enforce max length\n",
        "        while len(self.store) > self.max_snapshots:\n",
        "            self.store.popleft()\n",
        "        # prune by time window\n",
        "        self.prune_old()\n",
        "\n",
        "    def prune_old(self):\n",
        "        if not self.store:\n",
        "            return\n",
        "        cutoff = pd.Timestamp.now() - pd.Timedelta(minutes=self.window_minutes)\n",
        "        while self.store and self.store[0][0] < cutoff:\n",
        "            self.store.popleft()\n",
        "\n",
        "    def to_dataframe(self) -> pd.DataFrame:\n",
        "        if not self.store:\n",
        "            return pd.DataFrame()\n",
        "        rows = []\n",
        "        for t, flat in self.store:\n",
        "            r = dict(flat)\n",
        "            r['LTT'] = t\n",
        "            rows.append(r)\n",
        "        # Create dataframe once\n",
        "        df = pd.DataFrame(rows)\n",
        "        return df\n",
        "\n",
        "# ---------------------- ANALYSIS HELPERS ---------------------- #\n",
        "\n",
        "def pick_column(df: pd.DataFrame, ltp: str, prem: str) -> Optional[str]:\n",
        "    return ltp if ltp in df.columns else (prem if prem in df.columns else None)\n",
        "\n",
        "\n",
        "def build_series_from_df(df: pd.DataFrame, prev_call_col, curr_call_col, next_call_col,\n",
        "                         prev_put_col, curr_put_col, next_put_col,\n",
        "                         prev_str=\"Previous_Strikeprice\", curr_str=\"Current_Strikeprice\", next_str=\"Next_Strikeprice\"):\n",
        "    call_series = defaultdict(list)\n",
        "    put_series = defaultdict(list)\n",
        "    if df.empty:\n",
        "        return call_series, put_series\n",
        "\n",
        "    # iterate rows once\n",
        "    for _, row in df.iterrows():\n",
        "        t = row.get('LTT')\n",
        "        for sc, pc in ((prev_str, prev_call_col), (curr_str, curr_call_col), (next_str, next_call_col)):\n",
        "            if sc in row and pc and pc in row:\n",
        "                scv = row.get(sc)\n",
        "                pcv = row.get(pc)\n",
        "                if pd.isna(scv) or pd.isna(pcv):\n",
        "                    continue\n",
        "                try:\n",
        "                    s = int(scv)\n",
        "                    p = float(pcv)\n",
        "                except Exception:\n",
        "                    continue\n",
        "                call_series[s].append((t, p))\n",
        "        for sc, pp in ((prev_str, prev_put_col), (curr_str, curr_put_col), (next_str, next_put_col)):\n",
        "            if sc in row and pp and pp in row:\n",
        "                scv = row.get(sc)\n",
        "                ppv = row.get(pp)\n",
        "                if pd.isna(scv) or pd.isna(ppv):\n",
        "                    continue\n",
        "                try:\n",
        "                    s = int(scv)\n",
        "                    p = float(ppv)\n",
        "                except Exception:\n",
        "                    continue\n",
        "                put_series[s].append((t, p))\n",
        "    return call_series, put_series\n",
        "\n",
        "\n",
        "def summarize_series(pairs):\n",
        "    if not pairs:\n",
        "        return None\n",
        "    sr = sorted(pairs, key=lambda x: x[0])\n",
        "    prices = [p for _, p in sr]\n",
        "    first = prices[0]\n",
        "    last = prices[-1]\n",
        "    peak = max(prices)\n",
        "    trough = min(prices)\n",
        "    abs_chg = last - first\n",
        "    pct_chg = (abs_chg / first * 100) if first != 0 else np.nan\n",
        "    return {\n",
        "        'first': first, 'last': last, 'peak': peak, 'trough': trough,\n",
        "        'abs_change': abs_chg, 'pct_change': pct_chg, 'n_obs': len(prices),\n",
        "        'series_sorted': sr\n",
        "    }\n",
        "\n",
        "\n",
        "def forecast_from_pct(last, pct):\n",
        "    if pd.isna(pct):\n",
        "        pct = 0.0\n",
        "    if pct >= 25:\n",
        "        return (last+15, last+35), (last+25, last+60)\n",
        "    if pct >= 8:\n",
        "        return (last+6, last+18), (last+12, last+30)\n",
        "    if pct > 0:\n",
        "        return (last+2, last+8), (last+5, last+15)\n",
        "    return (last-5, last+2), (last-8, last+5)\n",
        "\n",
        "\n",
        "def aggregate_tags_and_mf(df: pd.DataFrame, strikes: List[int]):\n",
        "    out = {int(s): {'tags': Counter(), 'call_mf': 0.0, 'put_mf': 0.0} for s in strikes}\n",
        "    tag_cols = [\"Previous_StrategyTag\", \"Current_StrategyTag\", \"Next_StrategyTag\"]\n",
        "    call_mflow_cols = [\"Previous_CallMoneyFlow\", \"Current_CallMoneyFlow\", \"Next_CallMoneyFlow\",\n",
        "                       \"Previous_TotalcallMoneyFlow\", \"Current_TotalcallMoneyFlow\", \"Next_TotalcallMoneyFlow\"]\n",
        "    put_mflow_cols  = [\"Previous_PutMoneyFlow\", \"Current_PutMoneyFlow\", \"Next_PutMoneyFlow\",\n",
        "                       \"Previous_TotalputMoneyFlow\", \"Current_TotalputMoneyFlow\", \"Next_TotalputMoneyFlow\"]\n",
        "\n",
        "    if df.empty:\n",
        "        return out\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        for sc in (\"Previous_Strikeprice\", \"Current_Strikeprice\", \"Next_Strikeprice\"):\n",
        "            if sc not in row:\n",
        "                continue\n",
        "            scv = row.get(sc)\n",
        "            if pd.isna(scv):\n",
        "                continue\n",
        "            try:\n",
        "                s = int(scv)\n",
        "            except Exception:\n",
        "                continue\n",
        "            if s not in out:\n",
        "                continue\n",
        "            for tc in tag_cols:\n",
        "                if tc in row:\n",
        "                    v = row.get(tc)\n",
        "                    if isinstance(v, str) and v.strip():\n",
        "                        tokens = [t.strip() for t in v.replace('|', ';').split(';') if t.strip()]\n",
        "                        for tkn in tokens:\n",
        "                            out[s]['tags'][tkn] += 1\n",
        "            for cm in call_mflow_cols:\n",
        "                if cm in row:\n",
        "                    val = row.get(cm)\n",
        "                    if not pd.isna(val):\n",
        "                        try:\n",
        "                            out[s]['call_mf'] += float(val)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "            for pm in put_mflow_cols:\n",
        "                if pm in row:\n",
        "                    val = row.get(pm)\n",
        "                    if not pd.isna(val):\n",
        "                        try:\n",
        "                            out[s]['put_mf'] += float(val)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "    return out\n",
        "\n",
        "\n",
        "def decide_action_from_row(row):\n",
        "    call_pct = row.get('pct_change_call') if pd.notna(row.get('pct_change_call')) else 0\n",
        "    put_pct  = row.get('pct_change_put')  if pd.notna(row.get('pct_change_put'))  else 0\n",
        "    tags = (row.get('tags') or '').lower()\n",
        "    bull_boost = ('call buying' in tags) or ('oi_support_call' in tags) or ('bull' in tags)\n",
        "    bear_boost = ('put buying' in tags) or ('call writing' in tags) or ('bear' in tags)\n",
        "\n",
        "    if (put_pct > 8) or (put_pct > 5 and bear_boost):\n",
        "        return 'BUY_PUT'\n",
        "    if (call_pct > 8) or (call_pct > 5 and bull_boost):\n",
        "        return 'BUY_CALL'\n",
        "    return 'HOLD'\n",
        "\n",
        "# ---------------------- CORE ANALYSIS ---------------------- #\n",
        "\n",
        "def analyze_window(window: SnapshotWindow):\n",
        "    df = window.to_dataframe()\n",
        "    # pick columns with fallback\n",
        "    prev_call_col = pick_column(df, 'Previous_Call_ltp', 'Previous_Call_Premium')\n",
        "    curr_call_col = pick_column(df, 'Current_Call_ltp', 'Current_Call_Premium')\n",
        "    next_call_col = pick_column(df, 'Next_Call_ltp', 'Next_Call_Premium')\n",
        "\n",
        "    prev_put_col = pick_column(df, 'Previous_Put_ltp', 'Previous_Put_Premium')\n",
        "    curr_put_col = pick_column(df, 'Current_Put_ltp', 'Current_Put_Premium')\n",
        "    next_put_col = pick_column(df, 'Next_Put_ltp', 'Next_Put_Premium')\n",
        "\n",
        "    call_series, put_series = build_series_from_df(df, prev_call_col, curr_call_col, next_call_col,\n",
        "                                                   prev_put_col, curr_put_col, next_put_col)\n",
        "\n",
        "    all_strikes = sorted(set(list(call_series.keys()) + list(put_series.keys())))\n",
        "\n",
        "    merged_rows = []\n",
        "    for s in all_strikes:\n",
        "        cs = summarize_series(call_series.get(s, []))\n",
        "        ps = summarize_series(put_series.get(s, []))\n",
        "        n_call = cs['n_obs'] if cs else 0\n",
        "        n_put  = ps['n_obs'] if ps else 0\n",
        "        # prefer side with more observations\n",
        "        if n_call >= n_put and cs:\n",
        "            first_p = cs['first']; last_p = cs['last']; peak = cs['peak']; trough = cs['trough']\n",
        "            abs_chg = cs['abs_change']; pct = cs['pct_change']\n",
        "            f5, f10 = forecast_from_pct(last_p, pct)\n",
        "            row = {\n",
        "                'strike': int(s),\n",
        "                'n_obs': n_call + n_put,\n",
        "                'n_obs_call': n_call, 'n_obs_put': n_put,\n",
        "                'first_premium': first_p, 'last_premium': last_p, 'peak_premium': peak, 'trough_premium': trough,\n",
        "                'abs_change': abs_chg, 'pct_change': pct,\n",
        "                '5min_low': f5[0], '5min_high': f5[1], '10min_low': f10[0], '10min_high': f10[1],\n",
        "                'first_call_ltp': cs['first'], 'last_call_ltp': cs['last'], 'peak_call_ltp': cs['peak'], 'trough_call_ltp': cs['trough'],\n",
        "                'abs_change_call': cs['abs_change'], 'pct_change_call': cs['pct_change'],\n",
        "                'first_put_ltp': ps['first'] if ps else np.nan, 'last_put_ltp': ps['last'] if ps else np.nan,\n",
        "                'pct_change_put': ps['pct_change'] if ps else np.nan\n",
        "            }\n",
        "        elif ps:\n",
        "            first_p = ps['first']; last_p = ps['last']; peak = ps['peak']; trough = ps['trough']\n",
        "            abs_chg = ps['abs_change']; pct = ps['pct_change']\n",
        "            f5, f10 = forecast_from_pct(last_p, pct)\n",
        "            row = {\n",
        "                'strike': int(s),\n",
        "                'n_obs': n_call + n_put,\n",
        "                'n_obs_call': n_call, 'n_obs_put': n_put,\n",
        "                'first_premium': first_p, 'last_premium': last_p, 'peak_premium': peak, 'trough_premium': trough,\n",
        "                'abs_change': abs_chg, 'pct_change': pct,\n",
        "                '5min_low': f5[0], '5min_high': f5[1], '10min_low': f10[0], '10min_high': f10[1],\n",
        "                'first_put_ltp': ps['first'], 'last_put_ltp': ps['last'], 'peak_put_ltp': ps['peak'], 'trough_put_ltp': ps['trough'],\n",
        "                'abs_change_put': ps['abs_change'], 'pct_change_put': ps['pct_change'],\n",
        "                'first_call_ltp': cs['first'] if cs else np.nan, 'last_call_ltp': cs['last'] if cs else np.nan,\n",
        "                'pct_change_call': cs['pct_change'] if cs else np.nan\n",
        "            }\n",
        "        else:\n",
        "            continue\n",
        "        merged_rows.append(row)\n",
        "\n",
        "    if not merged_rows:\n",
        "        return [], [], [], []\n",
        "\n",
        "    merged_df = pd.DataFrame(merged_rows)\n",
        "    strike_info = aggregate_tags_and_mf(df, merged_df['strike'].tolist())\n",
        "    # attach tags and moneyflow\n",
        "    merged_df['tags'] = merged_df['strike'].apply(lambda s: ';'.join([f\"{k}:{v}\" for k, v in strike_info[int(s)]['tags'].items()]) if strike_info[int(s)]['tags'] else '')\n",
        "    merged_df['call_moneyflow'] = merged_df['strike'].apply(lambda s: strike_info[int(s)]['call_mf'])\n",
        "    merged_df['put_moneyflow']  = merged_df['strike'].apply(lambda s: strike_info[int(s)]['put_mf'])\n",
        "\n",
        "    # build reasons\n",
        "    def build_reasons(row):\n",
        "        reasons = []\n",
        "        tags_text = str(row.get('tags', ''))\n",
        "        if any(k in tags_text.lower() for k in ['rsi', 'macd']):\n",
        "            reasons.append('RSI/MACD momentum')\n",
        "        if 'vwap' in tags_text.lower():\n",
        "            reasons.append('VWAP divergence')\n",
        "        if 'oi' in tags_text.lower():\n",
        "            reasons.append('OI support/resistance')\n",
        "        if row.get('call_moneyflow', 0) > 0:\n",
        "            reasons.append('Call net buying')\n",
        "        if row.get('put_moneyflow', 0) > 0:\n",
        "            reasons.append('Put net buying')\n",
        "        pct = row.get('pct_change', 0) or 0\n",
        "        try:\n",
        "            if pct > 10:\n",
        "                reasons.append('Strong premium move')\n",
        "            elif pct > 3:\n",
        "                reasons.append('Moderate premium move')\n",
        "        except Exception:\n",
        "            pass\n",
        "        return '; '.join(dict.fromkeys(reasons)) if reasons else 'No strong signals'\n",
        "\n",
        "    merged_df['reasons'] = merged_df.apply(build_reasons, axis=1)\n",
        "    merged_df['recommended_action'] = merged_df.apply(decide_action_from_row, axis=1)\n",
        "\n",
        "    # quick high-conviction stats (approximation)\n",
        "    hc_col = 'Current_IsHighConvictionSignal'\n",
        "    totals, successes, rates = [], [], []\n",
        "    for _, r in merged_df.iterrows():\n",
        "        s = int(r['strike'])\n",
        "        total = success = 0\n",
        "        if hc_col in df.columns:\n",
        "            cond = (df.get('Current_Strikeprice') == s) & (df.get(hc_col) == True)\n",
        "            hc_rows = df[cond]\n",
        "            total = int(hc_rows.shape[0])\n",
        "            for _, hr in hc_rows.iterrows():\n",
        "                t0 = hr.get('LTT')\n",
        "                if pd.isna(t0):\n",
        "                    continue\n",
        "                action = r['recommended_action']\n",
        "                base_col = curr_call_col if (action == 'BUY_CALL' and curr_call_col in df.columns) else (curr_put_col if (action == 'BUY_PUT' and curr_put_col in df.columns) else (curr_call_col if curr_call_col in df.columns else None))\n",
        "                if base_col is None:\n",
        "                    continue\n",
        "                p0 = hr.get(base_col)\n",
        "                if pd.isna(p0):\n",
        "                    continue\n",
        "                window = df[(df['LTT'] >= t0) & (df['LTT'] <= (t0 + pd.Timedelta(minutes=3)))]\n",
        "                if window.empty:\n",
        "                    continue\n",
        "                try:\n",
        "                    if window[base_col].max() > p0:\n",
        "                        success += 1\n",
        "                except Exception:\n",
        "                    pass\n",
        "        rate = (success / total) if total > 0 else None\n",
        "        totals.append(total); successes.append(success); rates.append(rate)\n",
        "    merged_df['highconv_total'] = totals\n",
        "    merged_df['highconv_success'] = successes\n",
        "    merged_df['highconv_hit_rate'] = rates\n",
        "\n",
        "    merged_df['Current_Strikeprice'] = merged_df['strike']\n",
        "\n",
        "    # assemble signals\n",
        "    signals = []\n",
        "    for _, r in merged_df.iterrows():\n",
        "        if r.get('recommended_action') in ('BUY_CALL', 'BUY_PUT'):\n",
        "            strength = float(r.get('pct_change_call', 0) or r.get('pct_change_put', 0) or 0)\n",
        "            signals.append({\n",
        "                'strike': int(r['strike']),\n",
        "                'action': r.get('recommended_action'),\n",
        "                'strength': strength,\n",
        "                'reason': r.get('reasons', ''),\n",
        "                'tags': r.get('tags', ''),\n",
        "                'last_premium': float(r.get('last_premium') or 0),\n",
        "                'time': str(pd.Timestamp.now())\n",
        "            })\n",
        "\n",
        "    # detect reversals\n",
        "    reversals = []\n",
        "    for s in all_strikes:\n",
        "        series = call_series.get(s) if call_series.get(s) else put_series.get(s)\n",
        "        if not series:\n",
        "            continue\n",
        "        sr = sorted(series, key=lambda x: x[0])\n",
        "        t_last = sr[-1][0]\n",
        "        cutoff = t_last - pd.Timedelta(minutes=REVERSAL_WINDOW_MIN)\n",
        "        window = [p for t, p in sr if t >= cutoff]\n",
        "        if not window:\n",
        "            continue\n",
        "        peak = max(window); lastp = window[-1]\n",
        "        drop = (peak - lastp) / peak * 100 if peak > 0 else 0\n",
        "        if drop >= REVERSAL_DROP_PCT:\n",
        "            reversals.append({'strike': s, 'peak': peak, 'last': lastp, 'drop_pct': drop})\n",
        "\n",
        "    # detect IV crush\n",
        "    iv_crush = []\n",
        "    if not df.empty:\n",
        "        for _, row in df.iterrows():\n",
        "            for prev_iv, curr_iv, side in [(\"Previous_Call_IV\", \"Current_Call_IV\", \"CALL\"), (\"Previous_Put_IV\", \"Current_Put_IV\", \"PUT\")]:\n",
        "                if prev_iv in row and curr_iv in row and pd.notna(row[prev_iv]) and pd.notna(row[curr_iv]) and row[prev_iv] > 0:\n",
        "                    drop = (row[prev_iv] - row[curr_iv]) / row[prev_iv] * 100\n",
        "                    if drop >= IV_CRUSH_DROP:\n",
        "                        iv_crush.append({'time': str(row.get('LTT')), 'strike': row.get('Current_Strikeprice'), 'side': side, 'drop_pct': drop})\n",
        "\n",
        "    return merged_df.to_dict(orient='records'), signals, reversals, iv_crush\n",
        "\n",
        "# ---------------------- MAIN LOOP ---------------------- #\n",
        "\n",
        "def run_realtime(input_path: str, start_from_end: bool = START_FROM_END):\n",
        "    window = SnapshotWindow(window_minutes=WINDOW_MINUTES)\n",
        "    tailer = tail_file(input_path, start_from_end)\n",
        "\n",
        "    last_analyze = 0.0\n",
        "    print(f\"Realtime engine started. Watching: {input_path}\")\n",
        "\n",
        "    for raw_line in tailer:\n",
        "        if not running:\n",
        "            break\n",
        "        try:\n",
        "            parsed_list = parse_line_json(raw_line)\n",
        "        except Exception as e:\n",
        "            print('Parse error:', e)\n",
        "            continue\n",
        "        if not parsed_list:\n",
        "            continue\n",
        "        for flat in parsed_list:\n",
        "            try:\n",
        "                window.append(flat)\n",
        "            except Exception as e:\n",
        "                print('Append error (ignored):', e)\n",
        "                continue\n",
        "\n",
        "        now = time.time()\n",
        "        if now - last_analyze < max(0.01, REFRESH_SECONDS * 0.6):\n",
        "            continue\n",
        "        last_analyze = now\n",
        "\n",
        "        try:\n",
        "            merged_rows, signals, reversals, iv_crush = analyze_window(window)\n",
        "        except Exception as e:\n",
        "            print('Analysis error (skipping this cycle):', e)\n",
        "            continue\n",
        "\n",
        "        # write outputs\n",
        "        if WRITE_OUTPUT_JSON and not DRY_RUN:\n",
        "            try:\n",
        "                with open(OUT_AUTOTRADE, 'w') as fh:\n",
        "                    json.dump(signals, fh, indent=2, default=str)\n",
        "                with open(OUT_LATEST, 'w') as fh:\n",
        "                    json.dump({'timestamp': str(pd.Timestamp.now()), 'merged': merged_rows, 'reversals': reversals, 'iv_crush': iv_crush}, fh, indent=2, default=str)\n",
        "            except Exception as e:\n",
        "                print('Warning: failed to write outputs:', e)\n",
        "\n",
        "        # optional historical dump\n",
        "        if HISTORICAL_DUMP:\n",
        "            try:\n",
        "                with open(HISTORICAL_DUMP, 'a') as fh:\n",
        "                    fh.write(json.dumps({'ts': str(pd.Timestamp.now()), 'merged': merged_rows}) + '\\n')\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # print alerts\n",
        "        if signals:\n",
        "            for s in signals:\n",
        "                msg = f\"[SIGNAL] {s['time']} Strike {s['strike']} => {s['action']} (str={s['strength']:.2f}) reason={s['reason']}\"\n",
        "                print(msg)\n",
        "                if ALERT_LOG:\n",
        "                    with open(ALERT_LOG, 'a') as L:\n",
        "                        L.write(msg + '\\n')\n",
        "        if reversals:\n",
        "            for r in reversals:\n",
        "                msg = f\"[REVERSAL] Strike {r['strike']} drop {r['drop_pct']:.1f}% (peak {r['peak']} -> last {r['last']})\"\n",
        "                print(msg)\n",
        "                if ALERT_LOG:\n",
        "                    with open(ALERT_LOG, 'a') as L:\n",
        "                        L.write(msg + '\\n')\n",
        "        if iv_crush:\n",
        "            for e in iv_crush:\n",
        "                msg = f\"[IV_CRUSH] {e['time']} Strike {e['strike']} {e['side']} drop {e['drop_pct']:.1f}%\"\n",
        "                print(msg)\n",
        "                if ALERT_LOG:\n",
        "                    with open(ALERT_LOG, 'a') as L:\n",
        "                        L.write(msg + '\\n')\n",
        "\n",
        "    print('Realtime engine stopped.')\n",
        "\n",
        "# ---------------------- CLI ---------------------- #\n",
        "\n",
        "def cli():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument('--file', '-f', dest='file', default=INPUT_FILE, help='Input file to tail')\n",
        "    p.add_argument('--from-start', action='store_true', help='Start reading from start of file instead of EOF')\n",
        "    p.add_argument('--no-write', action='store_true', help='Do not write output json files (dry run)')\n",
        "    p.add_argument('--alert-log', dest='alert_log', default=ALERT_LOG, help='Path to append alert log')\n",
        "    p.add_argument('--window', type=int, default=WINDOW_MINUTES, help='Sliding window minutes')\n",
        "    args = p.parse_args()\n",
        "\n",
        "    global WRITE_OUTPUT_JSON, START_FROM_END, ALERT_LOG, WINDOW_MINUTES, DRY_RUN\n",
        "    WRITE_OUTPUT_JSON = True\n",
        "    START_FROM_END = not args.from_start\n",
        "    ALERT_LOG = args.alert_log\n",
        "    WINDOW_MINUTES = int(args.window)\n",
        "    DRY_RUN = bool(args.no_write)\n",
        "\n",
        "    if not os.path.exists(args.file):\n",
        "        print('ERROR: input file not found:', args.file)\n",
        "        sys.exit(1)\n",
        "\n",
        "    run_realtime(args.file, start_from_end=START_FROM_END)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    cli()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "pqHozWqSK6co",
        "outputId": "f03bd61e-6643-450b-ed4d-e883eabda39b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "name 'ALERT_LOG' is used prior to global declaration (ipython-input-4211273433.py, line 620)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-4211273433.py\"\u001b[0;36m, line \u001b[0;32m620\u001b[0m\n\u001b[0;31m    global WRITE_OUTPUT_JSON, START_FROM_END, ALERT_LOG, WINDOW_MINUTES, DRY_RUN\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m name 'ALERT_LOG' is used prior to global declaration\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "realtime_ce_pe_engine_improved.py\n",
        "\n",
        "Improved realtime tailing and CE/PE analysis engine based on the user's original script.\n",
        "Key improvements:\n",
        " - Robust JSON parsing (handles object-per-line, JSON lists, and stringified 'Current').\n",
        " - Safer tail implementation (handles rotation/truncation) and immediate processing of appended data.\n",
        " - Configurable via top-of-file constants; small CLI wrapper available.\n",
        " - Reduced pandas overhead by building DataFrame carefully and avoiding excessive apply() usage.\n",
        " - Better type coercion and error handling so single malformed snapshot won't stop the engine.\n",
        " - Optional historical dump and optional dry-run (no file writes) flags.\n",
        " - Clearer logging, and optional alert logfile support.\n",
        "\n",
        "Behavior: tail input file, keep a sliding time window of snapshots, compute per-strike summaries,\n",
        "produce AUTO_TRADE_SIGNALS.json and LATEST_MERGED.json (unless WRITE_OUTPUT_JSON=False),\n",
        "print alerts for signals/reversals/iv-crush to stdout and ALERT_LOG if enabled.\n",
        "\n",
        "Note: this file is self-contained and intended to run on a machine that has pandas & numpy installed.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import signal\n",
        "import argparse\n",
        "from collections import defaultdict, Counter, deque\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Optional, List, Dict, Any\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------- CONFIG ---------------------- #\n",
        "INPUT_FILE = \"14112025_BANK_PNL.txt\"      # default path to live appended snapshots (text)\n",
        "REFRESH_SECONDS = 1.0                     # poll interval (seconds)\n",
        "WINDOW_MINUTES = 15                       # sliding window size for analysis\n",
        "START_FROM_END = False  # process existing data first, then tail new changes                     # if True, ignore historical lines and start tailing from EOF\n",
        "WRITE_OUTPUT_JSON = True                  # write AUTO_TRADE_SIGNALS.json and LATEST_MERGED.json\n",
        "OUT_AUTOTRADE = \"AUTO_TRADE_SIGNALS.json\"\n",
        "OUT_LATEST = \"LATEST_MERGED.json\"\n",
        "ALERT_LOG = None                          # path to an alert logfile or None to only print to stdout\n",
        "IV_CRUSH_DROP = 15.0                      # percent IV drop threshold\n",
        "REVERSAL_DROP_PCT = 12.0                  # reversal detection threshold (drop from peak)\n",
        "REVERSAL_WINDOW_MIN = 5                   # look-back window for reversal check (minutes)\n",
        "MAX_SNAPSHOTS_STORE = 10000               # maximum snapshots to keep in memory (safety)\n",
        "HISTORICAL_DUMP = None                    # optional: path to write a periodic historical dump (or None)\n",
        "DRY_RUN = False                           # if True, won't write output files (useful for testing)\n",
        "\n",
        "running = True\n",
        "\n",
        "# ---------------------- SIGNAL HANDLING ---------------------- #\n",
        "def graceful_exit(signum, frame):\n",
        "    global running\n",
        "    running = False\n",
        "    print(\"\\nReceived exit signal. Shutting down...\")\n",
        "\n",
        "signal.signal(signal.SIGINT, graceful_exit)\n",
        "signal.signal(signal.SIGTERM, graceful_exit)\n",
        "\n",
        "# ---------------------- JSON PARSING ---------------------- #\n",
        "\n",
        "def parse_line_json(line: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Parse a line that may contain:\n",
        "      - a single JSON object\n",
        "      - a JSON-encoded list of objects\n",
        "      - a text line that contains a JSON object somewhere inside\n",
        "    Returns a list of flattened dicts (possibly empty).\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    if not line:\n",
        "        return out\n",
        "    s = line.strip()\n",
        "    if not s:\n",
        "        return out\n",
        "\n",
        "    # Try direct load (object or list)\n",
        "    try:\n",
        "        parsed = json.loads(s)\n",
        "    except Exception:\n",
        "        # try to find first '{' and parse from there\n",
        "        try:\n",
        "            i = s.index('{')\n",
        "            parsed = json.loads(s[i:])\n",
        "        except Exception:\n",
        "            return out\n",
        "\n",
        "    # normalize to list\n",
        "    items = parsed if isinstance(parsed, list) else [parsed]\n",
        "\n",
        "    for obj in items:\n",
        "        flat = {}\n",
        "        if not isinstance(obj, dict):\n",
        "            continue\n",
        "        # flatten top-level except 'Current' (handled below)\n",
        "        for k, v in obj.items():\n",
        "            if k != \"Current\":\n",
        "                flat[k] = v\n",
        "        # handle Current that may be a string or dict and may contain Previous/Current/Next blocks\n",
        "        curr_raw = obj.get(\"Current\")\n",
        "        if isinstance(curr_raw, str):\n",
        "            try:\n",
        "                curr = json.loads(curr_raw)\n",
        "            except Exception:\n",
        "                curr = None\n",
        "        elif isinstance(curr_raw, dict):\n",
        "            curr = curr_raw\n",
        "        else:\n",
        "            curr = None\n",
        "\n",
        "        if isinstance(curr, dict):\n",
        "            for section in (\"Previous\", \"Current\", \"Next\"):\n",
        "                block = curr.get(section)\n",
        "                if isinstance(block, dict):\n",
        "                    for key, val in block.items():\n",
        "                        flat[f\"{section}_{key}\"] = val\n",
        "        out.append(flat)\n",
        "    return out\n",
        "\n",
        "# ---------------------- TAIL SUPPORT ---------------------- #\n",
        "\n",
        "def tail_file(path: str, start_from_end: bool = True):\n",
        "    \"\"\"\n",
        "    Generator yielding new lines appended to the file.\n",
        "    Handles file truncation/rotation by checking file size/inode changes.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Input file not found: {path}\")\n",
        "\n",
        "    with open(path, 'r', encoding='utf-8', errors='ignore') as fh:\n",
        "        if start_from_end:\n",
        "            fh.seek(0, os.SEEK_END)\n",
        "        else:\n",
        "            fh.seek(0)\n",
        "        last_inode = os.fstat(fh.fileno()).st_ino if hasattr(os, 'fstat') else None\n",
        "        while running:\n",
        "            where = fh.tell()\n",
        "            line = fh.readline()\n",
        "            if not line:\n",
        "                # detect truncation/rotation\n",
        "                try:\n",
        "                    st = os.stat(path)\n",
        "                    if last_inode is not None and getattr(st, 'st_ino', None) != last_inode:\n",
        "                        # file rotated - reopen\n",
        "                        fh = open(path, 'r', encoding='utf-8', errors='ignore')\n",
        "                        last_inode = os.fstat(fh.fileno()).st_ino if hasattr(os, 'fstat') else None\n",
        "                        continue\n",
        "                    if st.st_size < where:\n",
        "                        # truncated\n",
        "                        fh.seek(0)\n",
        "                except FileNotFoundError:\n",
        "                    # file removed — wait for it to reappear\n",
        "                    time.sleep(max(0.1, REFRESH_SECONDS))\n",
        "                    continue\n",
        "                time.sleep(max(0.01, REFRESH_SECONDS))\n",
        "                fh.seek(where)\n",
        "                continue\n",
        "            yield line\n",
        "\n",
        "# ---------------------- IN-MEMORY STORE ---------------------- #\n",
        "class SnapshotWindow:\n",
        "    def __init__(self, window_minutes: int = WINDOW_MINUTES, max_snapshots: int = MAX_SNAPSHOTS_STORE):\n",
        "        self.window_minutes = int(window_minutes)\n",
        "        self.max_snapshots = int(max_snapshots)\n",
        "        self.store = deque()  # (timestamp (pd.Timestamp), flat_dict)\n",
        "\n",
        "    def append(self, flat: Dict[str, Any]):\n",
        "        # determine timestamp robustly\n",
        "        t = flat.get('LTT') or flat.get('ltt') or flat.get('time') or flat.get('ts') or None\n",
        "        if isinstance(t, str):\n",
        "            try:\n",
        "                t = pd.to_datetime(t)\n",
        "            except Exception:\n",
        "                t = pd.Timestamp.now()\n",
        "        elif isinstance(t, (pd.Timestamp, datetime)):\n",
        "            t = pd.Timestamp(t)\n",
        "        elif t is None:\n",
        "            t = pd.Timestamp.now()\n",
        "        else:\n",
        "            try:\n",
        "                t = pd.to_datetime(t)\n",
        "            except Exception:\n",
        "                t = pd.Timestamp.now()\n",
        "\n",
        "        self.store.append((t, flat))\n",
        "        # enforce max length\n",
        "        while len(self.store) > self.max_snapshots:\n",
        "            self.store.popleft()\n",
        "        # prune by time window\n",
        "        self.prune_old()\n",
        "\n",
        "    def prune_old(self):\n",
        "        if not self.store:\n",
        "            return\n",
        "        cutoff = pd.Timestamp.now() - pd.Timedelta(minutes=self.window_minutes)\n",
        "        while self.store and self.store[0][0] < cutoff:\n",
        "            self.store.popleft()\n",
        "\n",
        "    def to_dataframe(self) -> pd.DataFrame:\n",
        "        if not self.store:\n",
        "            return pd.DataFrame()\n",
        "        rows = []\n",
        "        for t, flat in self.store:\n",
        "            r = dict(flat)\n",
        "            r['LTT'] = t\n",
        "            rows.append(r)\n",
        "        # Create dataframe once\n",
        "        df = pd.DataFrame(rows)\n",
        "        return df\n",
        "\n",
        "# ---------------------- ANALYSIS HELPERS ---------------------- #\n",
        "\n",
        "def pick_column(df: pd.DataFrame, ltp: str, prem: str) -> Optional[str]:\n",
        "    return ltp if ltp in df.columns else (prem if prem in df.columns else None)\n",
        "\n",
        "\n",
        "def build_series_from_df(df: pd.DataFrame, prev_call_col, curr_call_col, next_call_col,\n",
        "                         prev_put_col, curr_put_col, next_put_col,\n",
        "                         prev_str=\"Previous_Strikeprice\", curr_str=\"Current_Strikeprice\", next_str=\"Next_Strikeprice\"):\n",
        "    call_series = defaultdict(list)\n",
        "    put_series = defaultdict(list)\n",
        "    if df.empty:\n",
        "        return call_series, put_series\n",
        "\n",
        "    # iterate rows once\n",
        "    for _, row in df.iterrows():\n",
        "        t = row.get('LTT')\n",
        "        for sc, pc in ((prev_str, prev_call_col), (curr_str, curr_call_col), (next_str, next_call_col)):\n",
        "            if sc in row and pc and pc in row:\n",
        "                scv = row.get(sc)\n",
        "                pcv = row.get(pc)\n",
        "                if pd.isna(scv) or pd.isna(pcv):\n",
        "                    continue\n",
        "                try:\n",
        "                    s = int(scv)\n",
        "                    p = float(pcv)\n",
        "                except Exception:\n",
        "                    continue\n",
        "                call_series[s].append((t, p))\n",
        "        for sc, pp in ((prev_str, prev_put_col), (curr_str, curr_put_col), (next_str, next_put_col)):\n",
        "            if sc in row and pp and pp in row:\n",
        "                scv = row.get(sc)\n",
        "                ppv = row.get(pp)\n",
        "                if pd.isna(scv) or pd.isna(ppv):\n",
        "                    continue\n",
        "                try:\n",
        "                    s = int(scv)\n",
        "                    p = float(ppv)\n",
        "                except Exception:\n",
        "                    continue\n",
        "                put_series[s].append((t, p))\n",
        "    return call_series, put_series\n",
        "\n",
        "\n",
        "def summarize_series(pairs):\n",
        "    if not pairs:\n",
        "        return None\n",
        "    sr = sorted(pairs, key=lambda x: x[0])\n",
        "    prices = [p for _, p in sr]\n",
        "    first = prices[0]\n",
        "    last = prices[-1]\n",
        "    peak = max(prices)\n",
        "    trough = min(prices)\n",
        "    abs_chg = last - first\n",
        "    pct_chg = (abs_chg / first * 100) if first != 0 else np.nan\n",
        "    return {\n",
        "        'first': first, 'last': last, 'peak': peak, 'trough': trough,\n",
        "        'abs_change': abs_chg, 'pct_change': pct_chg, 'n_obs': len(prices),\n",
        "        'series_sorted': sr\n",
        "    }\n",
        "\n",
        "\n",
        "def forecast_from_pct(last, pct):\n",
        "    if pd.isna(pct):\n",
        "        pct = 0.0\n",
        "    if pct >= 25:\n",
        "        return (last+15, last+35), (last+25, last+60)\n",
        "    if pct >= 8:\n",
        "        return (last+6, last+18), (last+12, last+30)\n",
        "    if pct > 0:\n",
        "        return (last+2, last+8), (last+5, last+15)\n",
        "    return (last-5, last+2), (last-8, last+5)\n",
        "\n",
        "\n",
        "def aggregate_tags_and_mf(df: pd.DataFrame, strikes: List[int]):\n",
        "    out = {int(s): {'tags': Counter(), 'call_mf': 0.0, 'put_mf': 0.0} for s in strikes}\n",
        "    tag_cols = [\"Previous_StrategyTag\", \"Current_StrategyTag\", \"Next_StrategyTag\"]\n",
        "    call_mflow_cols = [\"Previous_CallMoneyFlow\", \"Current_CallMoneyFlow\", \"Next_CallMoneyFlow\",\n",
        "                       \"Previous_TotalcallMoneyFlow\", \"Current_TotalcallMoneyFlow\", \"Next_TotalcallMoneyFlow\"]\n",
        "    put_mflow_cols  = [\"Previous_PutMoneyFlow\", \"Current_PutMoneyFlow\", \"Next_PutMoneyFlow\",\n",
        "                       \"Previous_TotalputMoneyFlow\", \"Current_TotalputMoneyFlow\", \"Next_TotalputMoneyFlow\"]\n",
        "\n",
        "    if df.empty:\n",
        "        return out\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        for sc in (\"Previous_Strikeprice\", \"Current_Strikeprice\", \"Next_Strikeprice\"):\n",
        "            if sc not in row:\n",
        "                continue\n",
        "            scv = row.get(sc)\n",
        "            if pd.isna(scv):\n",
        "                continue\n",
        "            try:\n",
        "                s = int(scv)\n",
        "            except Exception:\n",
        "                continue\n",
        "            if s not in out:\n",
        "                continue\n",
        "            for tc in tag_cols:\n",
        "                if tc in row:\n",
        "                    v = row.get(tc)\n",
        "                    if isinstance(v, str) and v.strip():\n",
        "                        tokens = [t.strip() for t in v.replace('|', ';').split(';') if t.strip()]\n",
        "                        for tkn in tokens:\n",
        "                            out[s]['tags'][tkn] += 1\n",
        "            for cm in call_mflow_cols:\n",
        "                if cm in row:\n",
        "                    val = row.get(cm)\n",
        "                    if not pd.isna(val):\n",
        "                        try:\n",
        "                            out[s]['call_mf'] += float(val)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "            for pm in put_mflow_cols:\n",
        "                if pm in row:\n",
        "                    val = row.get(pm)\n",
        "                    if not pd.isna(val):\n",
        "                        try:\n",
        "                            out[s]['put_mf'] += float(val)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "    return out\n",
        "\n",
        "\n",
        "def decide_action_from_row(row):\n",
        "    call_pct = row.get('pct_change_call') if pd.notna(row.get('pct_change_call')) else 0\n",
        "    put_pct  = row.get('pct_change_put')  if pd.notna(row.get('pct_change_put'))  else 0\n",
        "    tags = (row.get('tags') or '').lower()\n",
        "    bull_boost = ('call buying' in tags) or ('oi_support_call' in tags) or ('bull' in tags)\n",
        "    bear_boost = ('put buying' in tags) or ('call writing' in tags) or ('bear' in tags)\n",
        "\n",
        "    if (put_pct > 8) or (put_pct > 5 and bear_boost):\n",
        "        return 'BUY_PUT'\n",
        "    if (call_pct > 8) or (call_pct > 5 and bull_boost):\n",
        "        return 'BUY_CALL'\n",
        "    return 'HOLD'\n",
        "\n",
        "# ---------------------- CORE ANALYSIS ---------------------- #\n",
        "\n",
        "def analyze_window(window: SnapshotWindow):\n",
        "    df = window.to_dataframe()\n",
        "    # pick columns with fallback\n",
        "    prev_call_col = pick_column(df, 'Previous_Call_ltp', 'Previous_Call_Premium')\n",
        "    curr_call_col = pick_column(df, 'Current_Call_ltp', 'Current_Call_Premium')\n",
        "    next_call_col = pick_column(df, 'Next_Call_ltp', 'Next_Call_Premium')\n",
        "\n",
        "    prev_put_col = pick_column(df, 'Previous_Put_ltp', 'Previous_Put_Premium')\n",
        "    curr_put_col = pick_column(df, 'Current_Put_ltp', 'Current_Put_Premium')\n",
        "    next_put_col = pick_column(df, 'Next_Put_ltp', 'Next_Put_Premium')\n",
        "\n",
        "    call_series, put_series = build_series_from_df(df, prev_call_col, curr_call_col, next_call_col,\n",
        "                                                   prev_put_col, curr_put_col, next_put_col)\n",
        "\n",
        "    all_strikes = sorted(set(list(call_series.keys()) + list(put_series.keys())))\n",
        "\n",
        "    merged_rows = []\n",
        "    for s in all_strikes:\n",
        "        cs = summarize_series(call_series.get(s, []))\n",
        "        ps = summarize_series(put_series.get(s, []))\n",
        "        n_call = cs['n_obs'] if cs else 0\n",
        "        n_put  = ps['n_obs'] if ps else 0\n",
        "        # prefer side with more observations\n",
        "        if n_call >= n_put and cs:\n",
        "            first_p = cs['first']; last_p = cs['last']; peak = cs['peak']; trough = cs['trough']\n",
        "            abs_chg = cs['abs_change']; pct = cs['pct_change']\n",
        "            f5, f10 = forecast_from_pct(last_p, pct)\n",
        "            row = {\n",
        "                'strike': int(s),\n",
        "                'n_obs': n_call + n_put,\n",
        "                'n_obs_call': n_call, 'n_obs_put': n_put,\n",
        "                'first_premium': first_p, 'last_premium': last_p, 'peak_premium': peak, 'trough_premium': trough,\n",
        "                'abs_change': abs_chg, 'pct_change': pct,\n",
        "                '5min_low': f5[0], '5min_high': f5[1], '10min_low': f10[0], '10min_high': f10[1],\n",
        "                'first_call_ltp': cs['first'], 'last_call_ltp': cs['last'], 'peak_call_ltp': cs['peak'], 'trough_call_ltp': cs['trough'],\n",
        "                'abs_change_call': cs['abs_change'], 'pct_change_call': cs['pct_change'],\n",
        "                'first_put_ltp': ps['first'] if ps else np.nan, 'last_put_ltp': ps['last'] if ps else np.nan,\n",
        "                'pct_change_put': ps['pct_change'] if ps else np.nan\n",
        "            }\n",
        "        elif ps:\n",
        "            first_p = ps['first']; last_p = ps['last']; peak = ps['peak']; trough = ps['trough']\n",
        "            abs_chg = ps['abs_change']; pct = ps['pct_change']\n",
        "            f5, f10 = forecast_from_pct(last_p, pct)\n",
        "            row = {\n",
        "                'strike': int(s),\n",
        "                'n_obs': n_call + n_put,\n",
        "                'n_obs_call': n_call, 'n_obs_put': n_put,\n",
        "                'first_premium': first_p, 'last_premium': last_p, 'peak_premium': peak, 'trough_premium': trough,\n",
        "                'abs_change': abs_chg, 'pct_change': pct,\n",
        "                '5min_low': f5[0], '5min_high': f5[1], '10min_low': f10[0], '10min_high': f10[1],\n",
        "                'first_put_ltp': ps['first'], 'last_put_ltp': ps['last'], 'peak_put_ltp': ps['peak'], 'trough_put_ltp': ps['trough'],\n",
        "                'abs_change_put': ps['abs_change'], 'pct_change_put': ps['pct_change'],\n",
        "                'first_call_ltp': cs['first'] if cs else np.nan, 'last_call_ltp': cs['last'] if cs else np.nan,\n",
        "                'pct_change_call': cs['pct_change'] if cs else np.nan\n",
        "            }\n",
        "        else:\n",
        "            continue\n",
        "        merged_rows.append(row)\n",
        "\n",
        "    if not merged_rows:\n",
        "        return [], [], [], []\n",
        "\n",
        "    merged_df = pd.DataFrame(merged_rows)\n",
        "    strike_info = aggregate_tags_and_mf(df, merged_df['strike'].tolist())\n",
        "    # attach tags and moneyflow\n",
        "    merged_df['tags'] = merged_df['strike'].apply(lambda s: ';'.join([f\"{k}:{v}\" for k, v in strike_info[int(s)]['tags'].items()]) if strike_info[int(s)]['tags'] else '')\n",
        "    merged_df['call_moneyflow'] = merged_df['strike'].apply(lambda s: strike_info[int(s)]['call_mf'])\n",
        "    merged_df['put_moneyflow']  = merged_df['strike'].apply(lambda s: strike_info[int(s)]['put_mf'])\n",
        "\n",
        "    # build reasons\n",
        "    def build_reasons(row):\n",
        "        reasons = []\n",
        "        tags_text = str(row.get('tags', ''))\n",
        "        if any(k in tags_text.lower() for k in ['rsi', 'macd']):\n",
        "            reasons.append('RSI/MACD momentum')\n",
        "        if 'vwap' in tags_text.lower():\n",
        "            reasons.append('VWAP divergence')\n",
        "        if 'oi' in tags_text.lower():\n",
        "            reasons.append('OI support/resistance')\n",
        "        if row.get('call_moneyflow', 0) > 0:\n",
        "            reasons.append('Call net buying')\n",
        "        if row.get('put_moneyflow', 0) > 0:\n",
        "            reasons.append('Put net buying')\n",
        "        pct = row.get('pct_change', 0) or 0\n",
        "        try:\n",
        "            if pct > 10:\n",
        "                reasons.append('Strong premium move')\n",
        "            elif pct > 3:\n",
        "                reasons.append('Moderate premium move')\n",
        "        except Exception:\n",
        "            pass\n",
        "        return '; '.join(dict.fromkeys(reasons)) if reasons else 'No strong signals'\n",
        "\n",
        "    merged_df['reasons'] = merged_df.apply(build_reasons, axis=1)\n",
        "    merged_df['recommended_action'] = merged_df.apply(decide_action_from_row, axis=1)\n",
        "\n",
        "    # quick high-conviction stats (approximation)\n",
        "    hc_col = 'Current_IsHighConvictionSignal'\n",
        "    totals, successes, rates = [], [], []\n",
        "    for _, r in merged_df.iterrows():\n",
        "        s = int(r['strike'])\n",
        "        total = success = 0\n",
        "        if hc_col in df.columns:\n",
        "            cond = (df.get('Current_Strikeprice') == s) & (df.get(hc_col) == True)\n",
        "            hc_rows = df[cond]\n",
        "            total = int(hc_rows.shape[0])\n",
        "            for _, hr in hc_rows.iterrows():\n",
        "                t0 = hr.get('LTT')\n",
        "                if pd.isna(t0):\n",
        "                    continue\n",
        "                action = r['recommended_action']\n",
        "                base_col = curr_call_col if (action == 'BUY_CALL' and curr_call_col in df.columns) else (curr_put_col if (action == 'BUY_PUT' and curr_put_col in df.columns) else (curr_call_col if curr_call_col in df.columns else None))\n",
        "                if base_col is None:\n",
        "                    continue\n",
        "                p0 = hr.get(base_col)\n",
        "                if pd.isna(p0):\n",
        "                    continue\n",
        "                window = df[(df['LTT'] >= t0) & (df['LTT'] <= (t0 + pd.Timedelta(minutes=3)))]\n",
        "                if window.empty:\n",
        "                    continue\n",
        "                try:\n",
        "                    if window[base_col].max() > p0:\n",
        "                        success += 1\n",
        "                except Exception:\n",
        "                    pass\n",
        "        rate = (success / total) if total > 0 else None\n",
        "        totals.append(total); successes.append(success); rates.append(rate)\n",
        "    merged_df['highconv_total'] = totals\n",
        "    merged_df['highconv_success'] = successes\n",
        "    merged_df['highconv_hit_rate'] = rates\n",
        "\n",
        "    merged_df['Current_Strikeprice'] = merged_df['strike']\n",
        "\n",
        "    # assemble signals\n",
        "    signals = []\n",
        "    for _, r in merged_df.iterrows():\n",
        "        if r.get('recommended_action') in ('BUY_CALL', 'BUY_PUT'):\n",
        "            strength = float(r.get('pct_change_call', 0) or r.get('pct_change_put', 0) or 0)\n",
        "            signals.append({\n",
        "                'strike': int(r['strike']),\n",
        "                'action': r.get('recommended_action'),\n",
        "                'strength': strength,\n",
        "                'reason': r.get('reasons', ''),\n",
        "                'tags': r.get('tags', ''),\n",
        "                'last_premium': float(r.get('last_premium') or 0),\n",
        "                'time': str(pd.Timestamp.now())\n",
        "            })\n",
        "\n",
        "    # detect reversals\n",
        "    reversals = []\n",
        "    for s in all_strikes:\n",
        "        series = call_series.get(s) if call_series.get(s) else put_series.get(s)\n",
        "        if not series:\n",
        "            continue\n",
        "        sr = sorted(series, key=lambda x: x[0])\n",
        "        t_last = sr[-1][0]\n",
        "        cutoff = t_last - pd.Timedelta(minutes=REVERSAL_WINDOW_MIN)\n",
        "        window = [p for t, p in sr if t >= cutoff]\n",
        "        if not window:\n",
        "            continue\n",
        "        peak = max(window); lastp = window[-1]\n",
        "        drop = (peak - lastp) / peak * 100 if peak > 0 else 0\n",
        "        if drop >= REVERSAL_DROP_PCT:\n",
        "            reversals.append({'strike': s, 'peak': peak, 'last': lastp, 'drop_pct': drop})\n",
        "\n",
        "    # detect IV crush\n",
        "    iv_crush = []\n",
        "    if not df.empty:\n",
        "        for _, row in df.iterrows():\n",
        "            for prev_iv, curr_iv, side in [(\"Previous_Call_IV\", \"Current_Call_IV\", \"CALL\"), (\"Previous_Put_IV\", \"Current_Put_IV\", \"PUT\")]:\n",
        "                if prev_iv in row and curr_iv in row and pd.notna(row[prev_iv]) and pd.notna(row[curr_iv]) and row[prev_iv] > 0:\n",
        "                    drop = (row[prev_iv] - row[curr_iv]) / row[prev_iv] * 100\n",
        "                    if drop >= IV_CRUSH_DROP:\n",
        "                        iv_crush.append({'time': str(row.get('LTT')), 'strike': row.get('Current_Strikeprice'), 'side': side, 'drop_pct': drop})\n",
        "\n",
        "    return merged_df.to_dict(orient='records'), signals, reversals, iv_crush\n",
        "\n",
        "# ---------------------- MAIN LOOP ---------------------- #\n",
        "\n",
        "def run_realtime(input_path: str, start_from_end: bool = START_FROM_END):\n",
        "    window = SnapshotWindow(window_minutes=WINDOW_MINUTES)\n",
        "    tailer = tail_file(input_path, start_from_end)\n",
        "\n",
        "    last_analyze = 0.0\n",
        "    print(f\"Realtime engine started. Watching: {input_path}\")\n",
        "\n",
        "    for raw_line in tailer:\n",
        "        if not running:\n",
        "            break\n",
        "        try:\n",
        "            parsed_list = parse_line_json(raw_line)\n",
        "        except Exception as e:\n",
        "            print('Parse error:', e)\n",
        "            continue\n",
        "        if not parsed_list:\n",
        "            continue\n",
        "        for flat in parsed_list:\n",
        "            try:\n",
        "                window.append(flat)\n",
        "            except Exception as e:\n",
        "                print('Append error (ignored):', e)\n",
        "                continue\n",
        "\n",
        "        now = time.time()\n",
        "        if now - last_analyze < max(0.01, REFRESH_SECONDS * 0.6):\n",
        "            continue\n",
        "        last_analyze = now\n",
        "\n",
        "        try:\n",
        "            merged_rows, signals, reversals, iv_crush = analyze_window(window)\n",
        "        except Exception as e:\n",
        "            print('Analysis error (skipping this cycle):', e)\n",
        "            continue\n",
        "\n",
        "        # write outputs\n",
        "        if WRITE_OUTPUT_JSON and not DRY_RUN:\n",
        "            try:\n",
        "                with open(OUT_AUTOTRADE, 'w') as fh:\n",
        "                    json.dump(signals, fh, indent=2, default=str)\n",
        "                with open(OUT_LATEST, 'w') as fh:\n",
        "                    json.dump({'timestamp': str(pd.Timestamp.now()), 'merged': merged_rows, 'reversals': reversals, 'iv_crush': iv_crush}, fh, indent=2, default=str)\n",
        "            except Exception as e:\n",
        "                print('Warning: failed to write outputs:', e)\n",
        "\n",
        "        # optional historical dump\n",
        "        if HISTORICAL_DUMP:\n",
        "            try:\n",
        "                with open(HISTORICAL_DUMP, 'a') as fh:\n",
        "                    fh.write(json.dumps({'ts': str(pd.Timestamp.now()), 'merged': merged_rows}) + '\\n')\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # print alerts\n",
        "        if signals:\n",
        "            for s in signals:\n",
        "                msg = f\"[SIGNAL] {s['time']} Strike {s['strike']} => {s['action']} (str={s['strength']:.2f}) reason={s['reason']}\"\n",
        "                print(msg)\n",
        "                if ALERT_LOG:\n",
        "                    with open(ALERT_LOG, 'a') as L:\n",
        "                        L.write(msg + '\\n')\n",
        "        if reversals:\n",
        "            for r in reversals:\n",
        "                msg = f\"[REVERSAL] Strike {r['strike']} drop {r['drop_pct']:.1f}% (peak {r['peak']} -> last {r['last']})\"\n",
        "                print(msg)\n",
        "                if ALERT_LOG:\n",
        "                    with open(ALERT_LOG, 'a') as L:\n",
        "                        L.write(msg + '\\n')\n",
        "        if iv_crush:\n",
        "            for e in iv_crush:\n",
        "                msg = f\"[IV_CRUSH] {e['time']} Strike {e['strike']} {e['side']} drop {e['drop_pct']:.1f}%\"\n",
        "                print(msg)\n",
        "                if ALERT_LOG:\n",
        "                    with open(ALERT_LOG, 'a') as L:\n",
        "                        L.write(msg + '\\n')\n",
        "\n",
        "    print('Realtime engine stopped.')\n",
        "\n",
        "# ---------------------- CLI ---------------------- #\n",
        "\n",
        "def cli():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument('--file', '-f', dest='file', default=INPUT_FILE, help='Input file to tail')\n",
        "    p.add_argument('--from-start', action='store_true', help='Start reading from start of file instead of EOF')\n",
        "    p.add_argument('--no-write', action='store_true', help='Do not write output json files (dry run)')\n",
        "    p.add_argument('--alert-log', dest='alert_log', default=ALERT_LOG, help='Path to append alert log')\n",
        "    p.add_argument('--window', type=int, default=WINDOW_MINUTES, help='Sliding window minutes')\n",
        "    args = p.parse_args()\n",
        "\n",
        "    global WRITE_OUTPUT_JSON, START_FROM_END, DRY_RUN  # WINDOW_MINUTES set via globals() below\n",
        "    WRITE_OUTPUT_JSON = True\n",
        "    START_FROM_END = not args.from_start\n",
        "    # set ALERT_LOG separately to avoid global conflict\n",
        "    globals()['ALERT_LOG'] = args.alert_log\n",
        "    globals()['WINDOW_MINUTES'] = int(args.window)\n",
        "    DRY_RUN = bool(args.no_write)\n",
        "\n",
        "    if not os.path.exists(args.file):\n",
        "        print('ERROR: input file not found:', args.file)\n",
        "        sys.exit(1)\n",
        "\n",
        "    run_realtime(args.file, start_from_end=START_FROM_END)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    cli()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhjBI-fSLB7o",
        "outputId": "3230a339-7079-4d23-9936-d61da5e878c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Realtime engine started. Watching: /root/.local/share/jupyter/runtime/kernel-3b3ea389-36a4-4fd9-bc51-b08e84c883db.json\n",
            "\n",
            "Received exit signal. Shutting down...\n",
            "Realtime engine stopped.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vBRLbWjl7U9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# LOAD DATA\n",
        "# ---------------------------------------------------------\n",
        "path = \"last50.json\"      # <-- change if needed\n",
        "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "df = pd.json_normalize(data)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# HELPERS\n",
        "# ---------------------------------------------------------\n",
        "def to_bool(x):\n",
        "    if pd.isna(x):\n",
        "        return False\n",
        "    if isinstance(x, bool):\n",
        "        return x\n",
        "    s = str(x).strip().lower()\n",
        "    return s in (\"true\", \"1\", \"yes\", \"y\", \"t\")\n",
        "\n",
        "\n",
        "def normalize(series):\n",
        "    if series.max() == series.min():\n",
        "        return series * 0\n",
        "    return (series - series.min()) / (series.max() - series.min())\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# NORMALIZE BOOLEANS\n",
        "# ---------------------------------------------------------\n",
        "bool_cols = [c for c in df.columns if 'Is' in c or 'HighConviction' in c]\n",
        "for c in bool_cols:\n",
        "    df[c] = df[c].apply(to_bool)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# NORMALIZE NUMERICS\n",
        "# ---------------------------------------------------------\n",
        "numeric_candidates = [\n",
        "    'Current_CallScore','Current_PutScore','Next_CallScore','Next_PutScore',\n",
        "    'SpotPrice','AvgPrice','Current_CallMoneyFlow','Current_PutMoneyFlow',\n",
        "    'Next_CallMoneyFlow','Next_PutMoneyFlow','CallTTQ','PutTTQ',\n",
        "    'Next_CallTTQ','Next_PutTTQ'\n",
        "]\n",
        "\n",
        "for c in numeric_candidates:\n",
        "    if c in df.columns:\n",
        "        df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0.0)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# CALCULATE recalc_strength\n",
        "# ---------------------------------------------------------\n",
        "df[\"recalc_strength\"] = (\n",
        "    df[\"Current_IsHighConvictionSignal\"].astype(int) * 10\n",
        "    + df[\"Current_CallScore\"]\n",
        "    + df[\"Current_PutScore\"]\n",
        ")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# CALCULATE alt_strength\n",
        "# ---------------------------------------------------------\n",
        "def alt_strength(row):\n",
        "    cs = row.get(\"Current_CallScore\", 0)\n",
        "    ps = row.get(\"Current_PutScore\", 0)\n",
        "    side = str(row.get(\"Current_BuySide\") or \"\").upper()\n",
        "\n",
        "    base = cs + ps\n",
        "    if side == \"CALL\":\n",
        "        base += 0.5 * cs\n",
        "    elif side == \"PUT\":\n",
        "        base += 0.5 * ps\n",
        "\n",
        "    if row.get(\"Current_IsHighConvictionSignal\"):\n",
        "        base += 10\n",
        "\n",
        "    return base\n",
        "\n",
        "df[\"alt_strength\"] = df.apply(alt_strength, axis=1)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# AGREEMENT COUNT (count of CURRENT boolean signals)\n",
        "# ---------------------------------------------------------\n",
        "current_bool_cols = [\n",
        "    c for c in df.columns\n",
        "    if c.startswith(\"Current_\") and df[c].dtype == bool\n",
        "]\n",
        "\n",
        "df[\"agreement_count\"] = df[current_bool_cols].sum(axis=1)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# FORWARD VALIDATION (damn-sure signals)\n",
        "# ---------------------------------------------------------\n",
        "df[\"forward_confirm\"] = False\n",
        "df[\"forward_confirm_call\"] = False\n",
        "df[\"forward_confirm_put\"] = False\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    side = str(row.get(\"Current_BuySide\") or \"\").upper()\n",
        "    cond = True\n",
        "\n",
        "    if side == \"CALL\":\n",
        "        cond &= row.get(\"Next_CallScore\", 0) >= row.get(\"Current_CallScore\", 0)\n",
        "        cond &= row.get(\"Next_IsHighConvictionSignal\", False)\n",
        "        cond &= row.get(\"Next_CallMoneyFlow\", 0) >= row.get(\"Current_CallMoneyFlow\", 0)\n",
        "\n",
        "        if cond:\n",
        "            df.at[i, \"forward_confirm\"] = True\n",
        "            df.at[i, \"forward_confirm_call\"] = True\n",
        "\n",
        "    elif side == \"PUT\":\n",
        "        cond &= row.get(\"Next_PutScore\", 0) >= row.get(\"Current_PutScore\", 0)\n",
        "        cond &= row.get(\"Next_IsHighConvictionSignal\", False)\n",
        "        cond &= row.get(\"Next_PutMoneyFlow\", 0) >= row.get(\"Current_PutMoneyFlow\", 0)\n",
        "\n",
        "        if cond:\n",
        "            df.at[i, \"forward_confirm\"] = True\n",
        "            df.at[i, \"forward_confirm_put\"] = True\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# CORRELATION SUPPORT (numeric features supporting strength)\n",
        "# ---------------------------------------------------------\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "numeric_cols = [c for c in numeric_cols if c not in (\"recalc_strength\", \"alt_strength\")]\n",
        "\n",
        "corrs = {}\n",
        "for c in numeric_cols:\n",
        "    try:\n",
        "        corr_val = df[\"recalc_strength\"].corr(df[c])\n",
        "        corrs[c] = corr_val if not pd.isna(corr_val) else 0\n",
        "    except:\n",
        "        corrs[c] = 0\n",
        "\n",
        "# use only moderately correlated columns\n",
        "corr_support_cols = [c for c, v in corrs.items() if abs(v) >= 0.25]\n",
        "\n",
        "df[\"corr_support_raw\"] = 0.0\n",
        "for c in corr_support_cols:\n",
        "    col = df[c]\n",
        "    if col.max() != col.min():\n",
        "        norm = (col - col.min()) / (col.max() - col.min())\n",
        "    else:\n",
        "        norm = col * 0\n",
        "    df[\"corr_support_raw\"] += np.sign(corrs[c]) * norm * abs(corrs[c])\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# FINAL SCORE (WEIGHTED)\n",
        "# ---------------------------------------------------------\n",
        "df[\"norm_recalc\"] = normalize(df[\"recalc_strength\"])\n",
        "df[\"norm_alt\"] = normalize(df[\"alt_strength\"])\n",
        "df[\"norm_agreement\"] = normalize(df[\"agreement_count\"])\n",
        "df[\"norm_corr\"] = normalize(df[\"corr_support_raw\"])\n",
        "df[\"norm_forward\"] = df[\"forward_confirm\"].astype(int)\n",
        "\n",
        "weights = {\n",
        "    \"recalc\": 0.20,\n",
        "    \"alt\": 0.15,\n",
        "    \"agreement\": 0.15,\n",
        "    \"corr\": 0.15,\n",
        "    \"forward\": 0.35,\n",
        "}\n",
        "\n",
        "df[\"final_score\"] = (\n",
        "    df[\"norm_recalc\"] * weights[\"recalc\"]\n",
        "    + df[\"norm_alt\"] * weights[\"alt\"]\n",
        "    + df[\"norm_agreement\"] * weights[\"agreement\"]\n",
        "    + df[\"norm_corr\"] * weights[\"corr\"]\n",
        "    + df[\"norm_forward\"] * weights[\"forward\"]\n",
        ")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# EXTRACT FINAL DAMN–SURE SIGNALS\n",
        "# ---------------------------------------------------------\n",
        "confirmed = df[df[\"forward_confirm\"] == True].copy()\n",
        "confirmed = confirmed.sort_values(\"final_score\", ascending=False)\n",
        "\n",
        "cols_to_show = [\n",
        "    \"LTT\", \"SpotPrice\", \"Current_BuySide\",\n",
        "    \"Current_IsHighConvictionSignal\",\n",
        "    \"Current_CallScore\", \"Current_PutScore\",\n",
        "    \"agreement_count\", \"forward_confirm\",\n",
        "    \"forward_confirm_call\", \"forward_confirm_put\",\n",
        "    \"recalc_strength\", \"alt_strength\",\n",
        "    \"final_score\"\n",
        "]\n",
        "\n",
        "print(\"\\n\\n===== FORWARD-VALIDATED (DAMN-SURE) SIGNALS =====\\n\")\n",
        "print(confirmed[cols_to_show].to_string(index=False))\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# EXPORT CSV\n",
        "# ---------------------------------------------------------\n",
        "confirmed.to_csv(\"confirmed_damn_sure_signals.csv\", index=False)\n",
        "print(\"\\nSaved: confirmed_damn_sure_signals.csv\\n\")\n"
      ],
      "metadata": {
        "id": "KD24roK0L7qt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd255530-05c9-4738-c68d-4b1b7bea1b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "===== FORWARD-VALIDATED (DAMN-SURE) SIGNALS =====\n",
            "\n",
            "             LTT  SpotPrice Current_BuySide  Current_IsHighConvictionSignal  Current_CallScore  Current_PutScore  agreement_count  forward_confirm  forward_confirm_call  forward_confirm_put  recalc_strength  alt_strength  final_score\n",
            "14-11-2025 14:59   58198.70             PUT                            True                  4                 7                2             True                 False                 True               21          24.5     0.872779\n",
            "14-11-2025 14:59   58191.10             PUT                           False                  4                 7                1             True                 False                 True               11          14.5     0.547162\n",
            "14-11-2025 15:00   58282.95             PUT                           False                  4                 6                1             True                 False                 True               10          13.0     0.462823\n",
            "14-11-2025 15:00   58283.15             PUT                           False                  4                 6                1             True                 False                 True               10          13.0     0.458786\n",
            "14-11-2025 15:00   58280.00             PUT                           False                  4                 6                1             True                 False                 True               10          13.0     0.455376\n",
            "\n",
            "Saved: confirmed_damn_sure_signals.csv\n",
            "\n"
          ]
        }
      ]
    }
  ]
}